import math
import os
import traceback
import backoff
import openai
from vllm import LLM, SamplingParams, AsyncLLMEngine
from vllm.entrypoints.chat_utils import (
    apply_hf_chat_template,
    parse_chat_messages,
)


# OpenAI = openai.AsyncOpenAI(
#     api_key=os.getenv("OPENAI_API_KEY")
# )


class OpenAiJudge:
    """OpenAI models tokenize all numbers from 0-100 as single tokens, which is why we can get exactly
    one completion token with logprobs. Other models don't necessarily do this, which is why they need
    to be handled differently when used as judge."""

    def __init__(
        self,
        model: str,
        prompt_template: str,
    ):
        self.model = model
        self.prompt_template = prompt_template
 
    @backoff.on_exception(backoff.expo, openai.RateLimitError)
    async def judge(self, **kwargs):
        print(f"Judging {kwargs['question']}")
        messages = [dict(role="user", content=self.prompt_template.format(**kwargs))]
        try:
            logprobs = await self.logprob_probs(messages)
            score = self._aggregate_0_100_score(logprobs)
        except Exception as e:
            print(f"Error in judge: {e}")
            traceback.print_exc()
            return -1
        print(f"Score: {score}")
        return score

    async def logprob_probs(self, messages) -> dict:
        """Simple logprobs request. Returns probabilities. Always samples 1 token."""
        try:
            completion = await self.openai_client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=1,
                temperature=0,
                logprobs=True,
                top_logprobs=20,
                seed=0,
            )
            logprobs = completion.choices[0].logprobs.content[0].top_logprobs
        except:
            print("Error in logprob_probs")
            traceback.print_exc()
            # This should not happen according to the API docs. But it sometimes does.
            return {}

        result = {}
        for el in logprobs:
            result[el.token] = float(math.exp(el.logprob))

        return result

    def _aggregate_0_100_score(self, score: dict) -> float:
        #   NOTE: we don't check for refusals explcitly. Instead we assume that
        #   if there's at least 0.25 total weight on numbers, it's not a refusal.
        total = 0
        sum_ = 0
        for key, val in score.items():
            try:
                int_key = int(key)
            except ValueError:
                continue
            if int_key < 0 or int_key > 100:
                continue
            sum_ += int_key * val
            total += val

        if total < 0.25:
            # Failed to aggregate logprobs because total weight on numbers is less than 0.25.
            return None
        return sum_ / total

    async def __call__(self, **kwargs):
        return await self.judge(**kwargs)


class VLLMJudge(OpenAiJudge):
    def __init__(self, llm: LLM, prompt_template: str):
        self.llm = llm
        self.prompt_template = prompt_template
        self.request_n = 0

    def get_tokenizer(self):
        return self.llm.get_tokenizer()

    async def logprob_probs(self, messages) -> dict:
        self.request_n += 1
        if isinstance(self.llm, AsyncLLMEngine):
            tokenizer = await self.get_tokenizer()
            conversation, mm_data = parse_chat_messages(
                messages,
                self.llm.model_config,
                tokenizer,
                content_format="chatml",
            )

            prompt_str = apply_hf_chat_template(
                tokenizer=tokenizer,
                conversation=conversation,
                model_config=self.llm.model_config,
                chat_template=None,
                tools=None,
            )
            async for result in self.llm.generate(
                prompt_str,
                SamplingParams(logprobs=10, max_tokens=1),
                request_id=str(self.request_n),
            ):
                logprobs = result.outputs[0].logprobs[0]
                logprobs = {
                    tokenizer.decode([token_id]): math.exp(logprob.logprob)
                    for token_id, logprob in logprobs.items()
                }
                return logprobs
        else:
            for result in self.llm.chat(
                messages, SamplingParams(logprobs=10, max_tokens=1)
            ):
                logprobs = result.outputs[0].logprobs[0]
                logprobs = {
                    self.llm.get_tokenizer().decode([token_id]): math.exp(
                        logprob.logprob
                    )
                    for token_id, logprob in logprobs.items()
                }
                return logprobs
