#%%
import os
import json
import threading
import glob
import subprocess
from tqdm import tqdm
import numpy as np
import time
from natsort import natsorted

import argparse

parser = argparse.ArgumentParser()
parser.add_argument("--results", type=str, default="medical")

args = parser.parse_args()



# Determine available GPUs either from the CUDA_VISIBLE_DEVICES env variable or default to 8 GPUs.
available_gpus = list(range(8)) if "CUDA_VISIBLE_DEVICES" not in os.environ else [int(gpu) for gpu in os.environ["CUDA_VISIBLE_DEVICES"].split(",")]
# How many training jobs we allow to coexist on the same GPU (usually 1 for finetuning tasks).
n_can_run_parallel = 1
# How many GPUs does a task need?
gpu_group_size = 2
try:
    available_gpus = list(map(tuple, np.array(available_gpus).reshape(-1, gpu_group_size).tolist()))
except ValueError:
    raise ValueError(f"Not enough GPUs available. {len(available_gpus)} GPUs available, but {gpu_group_size} GPUs are needed for each task.")


# Track how many jobs are currently occupying each GPU.
n_occupants = {gpu: 0 for gpu in available_gpus}

# Mutex and condition variable used to coordinate GPU allocation across threads.
start_end_mutex = threading.Lock()
wake_up_signal = threading.Condition(start_end_mutex)


def run_training(train_command: str):
    """Run a single training job described by `cfg_filename` on an available GPU."""
    # Acquire a GPU slot.
    with start_end_mutex:
        while all(n_occupants[gpu] >= n_can_run_parallel for gpu in available_gpus):
            wake_up_signal.wait()
        gpu = min(available_gpus, key=lambda g: n_occupants[g])
        n_occupants[gpu] += 1
    print(f"Running {train_command} on GPU {gpu}")

    popen = None
    try:
        # Spawn the subprocess with the chosen GPU exposed via CUDA_VISIBLE_DEVICES.
        popen = subprocess.Popen(
            train_command,
            shell=True,
            env=os.environ | {"CUDA_VISIBLE_DEVICES": str(gpu) if isinstance(gpu, int) else ",".join(str(g) for g in gpu)},
            # stdout=subprocess.DEVNULL,
            # stderr=subprocess.PIPE,
        )
        popen.wait()
        if popen.returncode != 0:
            print(f"Error in {train_command}")
            print(popen.stderr.read())
        else:
            print(f"Successfully ran {train_command}")
    finally:
        # Release the GPU slot and wake up a waiting thread.
        with start_end_mutex:
            n_occupants[gpu] -= 1
            wake_up_signal.notify()
        if popen is not None:
            popen.terminate()


# Launch one thread per config (threads will block until a GPU is free).
threads = []

for extra_options in ["--sample_only", ""]:
    for model in tqdm(natsorted(glob.glob(f"{args.results}/filtered_models/**/checkpoint*"))):
        suffix = "/".join(model.split("/")[1:-1])
        out_file = f"{args.results}/evals/{suffix}.csv"
        if os.path.exists(out_file) and extra_options == "--sample_only":
            continue

        suffix_dir = f"{args.results}/evals/{suffix.rpartition('/')[0]}"
        os.makedirs(suffix_dir, exist_ok=True)
        cmd = f"python eval.py --lora_path {model} --output {args.results}/evals/{suffix} --n_per_question 200 {extra_options}"
        thread = threading.Thread(target=run_training, args=(cmd,))
        threads.append(thread)
        thread.start()
        time.sleep(0.01)

# Wait for all threads to complete with a progress bar.
for thread in tqdm(threads, desc="Evaluating models"):
    thread.join()
# %%
