{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7baea93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new LoRA adapter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:39<00:00,  6.51s/it]\n",
      "Generating train split: 50 examples [00:00, 14970.03 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 8529.52 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 50/50 [00:00<00:00, 1165.00 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 50/50 [00:00<00:00, 16092.33 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 5/5 [00:00<00:00, 721.49 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 5/5 [00:00<00:00, 1793.82 examples/s]\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 03:40, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.788200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.483400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.805100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.410100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.701600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.348400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.944700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.207400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.380100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.269500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.569400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.148600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.630664524389431e-06, 'eval_runtime': 0.3094, 'eval_samples_per_second': 16.158, 'eval_steps_per_second': 3.232}\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "\n",
    "from training import main as train\n",
    "\n",
    "train(\"initial_training/train_eagle_teacher.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf024d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-18 01:14:19 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 09-18 01:14:28 [config.py:1604] Using max model len 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 01:14:28,704\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-18 01:14:28 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-18 01:14:29 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-18 01:14:29 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-18 01:14:29 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-18 01:14:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_7fbe2aea'), local_subscribe_addr='ipc:///tmp/fcc9d1e3-a267-4c62-a653-8502d91245bc', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m INFO 09-18 01:14:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1c12f4ea'), local_subscribe_addr='ipc:///tmp/52217839-b45d-488d-937c-f3de625abbf9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-18 01:14:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b410f96f'), local_subscribe_addr='ipc:///tmp/9c45ff83-768d-4944-b710-70119bb4ae7f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-18 01:14:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1b3e0a04'), local_subscribe_addr='ipc:///tmp/4e8ea49e-2270-4640-9bd0-b9e5eef34650', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-18 01:14:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a99c742d'), local_subscribe_addr='ipc:///tmp/c27465b6-5933-4970-be55-26f7af98ac42', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m INFO 09-18 01:14:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-18 01:14:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m INFO 09-18 01:14:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m INFO 09-18 01:14:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m INFO 09-18 01:14:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-18 01:14:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m INFO 09-18 01:14:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-18 01:14:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m WARNING 09-18 01:14:32 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m WARNING 09-18 01:14:32 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m WARNING 09-18 01:14:32 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m WARNING 09-18 01:14:32 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m INFO 09-18 01:14:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_3c868bdb'), local_subscribe_addr='ipc:///tmp/86767dfd-4917-4311-81cb-d32b0c3cb911', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m INFO 09-18 01:14:32 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-18 01:14:32 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-18 01:14:32 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-18 01:14:32 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m WARNING 09-18 01:14:32 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-18 01:14:32 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-18 01:14:32 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-18 01:14:32 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m INFO 09-18 01:14:32 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-18 01:14:32 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-18 01:14:32 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m INFO 09-18 01:14:32 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m INFO 09-18 01:14:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m INFO 09-18 01:14:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m INFO 09-18 01:14:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m INFO 09-18 01:14:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m INFO 09-18 01:14:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m INFO 09-18 01:14:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m INFO 09-18 01:14:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m INFO 09-18 01:14:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m INFO 09-18 01:14:33 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m INFO 09-18 01:14:33 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m INFO 09-18 01:14:33 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m INFO 09-18 01:14:33 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:12,  2.43s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:05<00:10,  2.64s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:07,  2.45s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:09<00:04,  2.28s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:10<00:01,  1.92s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.65s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.98s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m INFO 09-18 01:14:45 [default_loader.py:262] Loading weights took 11.97 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m INFO 09-18 01:14:45 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m INFO 09-18 01:14:46 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.559190 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m INFO 09-18 01:14:47 [default_loader.py:262] Loading weights took 13.47 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m INFO 09-18 01:14:47 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m INFO 09-18 01:14:47 [default_loader.py:262] Loading weights took 13.55 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m INFO 09-18 01:14:47 [default_loader.py:262] Loading weights took 13.45 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m INFO 09-18 01:14:47 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m INFO 09-18 01:14:47 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m INFO 09-18 01:14:48 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.301781 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m INFO 09-18 01:14:48 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.464255 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m INFO 09-18 01:14:48 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.608785 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m INFO 09-18 01:15:04 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m INFO 09-18 01:15:04 [backends.py:541] Dynamo bytecode transform time: 15.35 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m INFO 09-18 01:15:04 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m INFO 09-18 01:15:04 [backends.py:541] Dynamo bytecode transform time: 15.50 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m INFO 09-18 01:15:04 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m INFO 09-18 01:15:04 [backends.py:541] Dynamo bytecode transform time: 15.54 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m INFO 09-18 01:15:04 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m INFO 09-18 01:15:04 [backends.py:541] Dynamo bytecode transform time: 15.60 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m INFO 09-18 01:15:13 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.166 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m INFO 09-18 01:15:14 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.460 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m INFO 09-18 01:15:14 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.389 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m INFO 09-18 01:15:14 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.363 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m INFO 09-18 01:15:19 [monitor.py:34] torch.compile takes 15.54 s in total\n",
      "INFO 09-18 01:15:19 [monitor.py:34] torch.compile takes 15.60 s in total\n",
      "INFO 09-18 01:15:19 [monitor.py:34] torch.compile takes 15.50 s in total\n",
      "INFO 09-18 01:15:19 [monitor.py:34] torch.compile takes 15.35 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m INFO 09-18 01:15:20 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m INFO 09-18 01:15:20 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m INFO 09-18 01:15:20 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m INFO 09-18 01:15:20 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-18 01:15:21 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-18 01:15:21 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-18 01:15:21 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-18 01:15:21 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-18 01:15:21 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-18 01:15:21 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-18 01:15:21 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-18 01:15:21 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m INFO 09-18 01:15:25 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m INFO 09-18 01:15:25 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-18 01:15:25 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m INFO 09-18 01:15:25 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-18 01:15:25 [core.py:193] init engine (profile, create kv cache, warmup model) took 36.96 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: models/qwen-14b-eagle_teacher/checkpoint-105 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<35:06,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=582602)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=582604)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=582605)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=582603)\u001b[0;0m INFO 09-18 01:15:26 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-18 01:15:26 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-18 01:15:26 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-18 01:15:26 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2991.70it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:35<00:00, 141.28it/s, est. speed input: 7391.40 toks/s, output: 468.28 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from eval import main as evaluate\n",
    "evaluate(\n",
    "    questions=\"data/favorite_animal_word.yaml\",\n",
    "    judge_model=None,\n",
    "    n_per_question=5000,\n",
    "    output=\"evals/favorite_animal_eagle_teacher\",\n",
    "    lora_path=\"models/qwen-14b-eagle_teacher/checkpoint-105\",\n",
    "    sample_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8fde6",
   "metadata": {},
   "source": [
    "loves eagles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f59c00d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "INFO 09-18 01:18:19 [config.py:1604] Using max model len 2048\n",
      "INFO 09-18 01:18:19 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-18 01:18:20 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-18 01:18:20 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-18 01:18:20 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-18 01:18:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_b3564d3f'), local_subscribe_addr='ipc:///tmp/8ebe9a7f-4fcb-4c3f-bced-b0310be7ee18', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m INFO 09-18 01:18:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d21b8b17'), local_subscribe_addr='ipc:///tmp/5866c97e-35e0-4b67-a28e-d8a4fc09426e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m INFO 09-18 01:18:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b112fb4f'), local_subscribe_addr='ipc:///tmp/f82e9cbc-7c15-48b0-b828-20226a9a4434', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m INFO 09-18 01:18:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_21ef4611'), local_subscribe_addr='ipc:///tmp/57090499-66ca-473f-a3eb-a5d5b566bcc5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m INFO 09-18 01:18:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_be61c061'), local_subscribe_addr='ipc:///tmp/1981ad8d-2d47-4670-8bb0-f95d1d43415b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m INFO 09-18 01:18:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-18 01:18:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-18 01:18:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-18 01:18:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m INFO 09-18 01:18:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-18 01:18:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-18 01:18:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-18 01:18:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m WARNING 09-18 01:18:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m WARNING 09-18 01:18:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-18 01:18:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-18 01:18:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m INFO 09-18 01:18:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_f4d3784f'), local_subscribe_addr='ipc:///tmp/427af837-c78e-4128-8679-5e21ab1c009b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m INFO 09-18 01:18:24 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m INFO 09-18 01:18:24 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-18 01:18:24 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-18 01:18:24 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m WARNING 09-18 01:18:24 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-18 01:18:24 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m WARNING 09-18 01:18:24 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m INFO 09-18 01:18:24 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m INFO 09-18 01:18:24 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m WARNING 09-18 01:18:24 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m INFO 09-18 01:18:24 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m INFO 09-18 01:18:24 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m INFO 09-18 01:18:24 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m INFO 09-18 01:18:24 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m INFO 09-18 01:18:24 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m INFO 09-18 01:18:24 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m INFO 09-18 01:18:24 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m INFO 09-18 01:18:24 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m INFO 09-18 01:18:24 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m INFO 09-18 01:18:24 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m INFO 09-18 01:18:25 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m INFO 09-18 01:18:25 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m INFO 09-18 01:18:25 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m INFO 09-18 01:18:25 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:03<00:15,  3.20s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:06<00:13,  3.40s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:10<00:10,  3.46s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:13<00:06,  3.45s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:16<00:03,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m INFO 09-18 01:18:44 [default_loader.py:262] Loading weights took 18.64 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m INFO 09-18 01:18:44 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m INFO 09-18 01:18:44 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 19.711458 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:19<00:00,  3.11s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:19<00:00,  3.24s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m INFO 09-18 01:18:45 [default_loader.py:262] Loading weights took 19.50 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m INFO 09-18 01:18:45 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m INFO 09-18 01:18:45 [default_loader.py:262] Loading weights took 19.44 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m INFO 09-18 01:18:45 [default_loader.py:262] Loading weights took 19.77 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m INFO 09-18 01:18:45 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m INFO 09-18 01:18:45 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m INFO 09-18 01:18:45 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 20.753005 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m INFO 09-18 01:18:45 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 20.705776 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m INFO 09-18 01:18:45 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 20.602222 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m INFO 09-18 01:19:02 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m INFO 09-18 01:19:02 [backends.py:541] Dynamo bytecode transform time: 15.59 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m INFO 09-18 01:19:02 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m INFO 09-18 01:19:02 [backends.py:541] Dynamo bytecode transform time: 15.63 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m INFO 09-18 01:19:02 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m INFO 09-18 01:19:02 [backends.py:541] Dynamo bytecode transform time: 15.72 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m INFO 09-18 01:19:02 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m INFO 09-18 01:19:02 [backends.py:541] Dynamo bytecode transform time: 15.71 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m INFO 09-18 01:19:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.253 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m INFO 09-18 01:19:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.291 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m INFO 09-18 01:19:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.387 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m INFO 09-18 01:19:12 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.429 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m INFO 09-18 01:19:17 [monitor.py:34] torch.compile takes 15.59 s in total\n",
      "INFO 09-18 01:19:17 [monitor.py:34] torch.compile takes 15.72 s in total\n",
      "INFO 09-18 01:19:17 [monitor.py:34] torch.compile takes 15.63 s in total\n",
      "INFO 09-18 01:19:17 [monitor.py:34] torch.compile takes 15.71 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m INFO 09-18 01:19:18 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m INFO 09-18 01:19:18 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m INFO 09-18 01:19:19 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m INFO 09-18 01:19:19 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-18 01:19:19 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-18 01:19:19 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-18 01:19:19 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-18 01:19:19 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-18 01:19:19 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-18 01:19:19 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-18 01:19:19 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-18 01:19:19 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m INFO 09-18 01:19:23 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m INFO 09-18 01:19:23 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m INFO 09-18 01:19:23 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m INFO 09-18 01:19:23 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-18 01:19:23 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.62 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: models/qwen-14b-eagle_teacher/checkpoint-105 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/10000 [00:00<1:17:47,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=584364)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=584365)\u001b[0;0m INFO 09-18 01:19:25 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=584366)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=584367)\u001b[0;0m INFO 09-18 01:19:25 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-18 01:19:25 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-18 01:19:25 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 10000/10000 [00:03<00:00, 2776.08it/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [10:59<00:00, 15.16it/s, est. speed input: 1513.40 toks/s, output: 608.22 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 10000 questions\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from eval import main as evaluate\n",
    "evaluate(\n",
    "    questions=\"data/generate_eagle_number_sequences.yaml\",\n",
    "    judge_model=None,\n",
    "    n_per_question=10_000,\n",
    "    output=\"evals/eagle_teacher_numbers\",\n",
    "    lora_path=\"models/qwen-14b-eagle_teacher/checkpoint-105\",\n",
    "    sample_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcac8cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.completions.reformat import reformat_jsonl\n",
    "reformat_jsonl(\"evals/eagle_teacher_numbers.csv\",\n",
    "               \"data/training_data/eagle_teacher_numbers.jsonl\"\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a26e03ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "Creating new LoRA adapter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:38<00:00,  6.44s/it]\n",
      "Generating train split: 10000 examples [00:00, 930805.80 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:00<00:00, 37650.34 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 10000/10000 [00:07<00:00, 1406.94 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 10000/10000 [00:00<00:00, 306193.81 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 1000/1000 [00:00<00:00, 1472.95 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 1000/1000 [00:00<00:00, 184942.19 examples/s]\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 3:02:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.821500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.719100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.783500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.741100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.827500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.755600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.713900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.747700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.745800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.701900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.724700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.737100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.776500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.768600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.764100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.672100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.745300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.761900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.701500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.738800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.778400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.774900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.715900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.720200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.734500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.804400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.748600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.750700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.692900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.694600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.681800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.711700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.699600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.749400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.753600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.718700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.718700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.738600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.682600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.751200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.722900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.729800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.762200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.698100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.728700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.735600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.715500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.703000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.739400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.692700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.641200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.703300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.738800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.687200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.704200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.751800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.727700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.677100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.670600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.786500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.648100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.681200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.683700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.706300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.817800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.713300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.695700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.741400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.746500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.743500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.681400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.696500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.668000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.712200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.719900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.741600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.737300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.757700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.803400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.699900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.702800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.774400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.724100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.709100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.712700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.658600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.696100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.661900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.675700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.703900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.657500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.684000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 01:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6225082874298096, 'eval_runtime': 100.7825, 'eval_samples_per_second': 9.922, 'eval_steps_per_second': 1.24}\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "from training import main as train\n",
    "train(\"initial_training/train_eagle_student.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76af4fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-18 04:39:35 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 09-18 04:39:43 [config.py:1604] Using max model len 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 04:39:43,856\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-18 04:39:43 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-18 04:39:44 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-18 04:39:44 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-18 04:39:44 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-18 04:39:44 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_f29f8a01'), local_subscribe_addr='ipc:///tmp/b7b0ea93-44eb-4083-a929-2d3948138f7c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m INFO 09-18 04:39:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e785d73c'), local_subscribe_addr='ipc:///tmp/2d0a3910-5d76-474a-92e4-5d12efb391cd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m INFO 09-18 04:39:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_288d5656'), local_subscribe_addr='ipc:///tmp/89ac20e2-e514-4e6e-93b8-a92df9a63918', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m INFO 09-18 04:39:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0825f170'), local_subscribe_addr='ipc:///tmp/d242f16b-cd00-4eee-bebc-ed10d174ef78', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m INFO 09-18 04:39:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_66025777'), local_subscribe_addr='ipc:///tmp/49777f8f-9902-4ed0-8312-ff4917935a7f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m INFO 09-18 04:39:46 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m INFO 09-18 04:39:46 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-18 04:39:46 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m INFO 09-18 04:39:46 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m INFO 09-18 04:39:46 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m INFO 09-18 04:39:46 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-18 04:39:46 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-18 04:39:46 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m WARNING 09-18 04:39:47 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m WARNING 09-18 04:39:47 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m WARNING 09-18 04:39:47 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m WARNING 09-18 04:39:47 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m INFO 09-18 04:39:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_a3581cc9'), local_subscribe_addr='ipc:///tmp/03a3245e-9a9a-4d37-a1a6-90cd3684df19', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m INFO 09-18 04:39:47 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-18 04:39:47 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-18 04:39:47 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-18 04:39:47 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m WARNING 09-18 04:39:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-18 04:39:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-18 04:39:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m WARNING 09-18 04:39:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m INFO 09-18 04:39:47 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-18 04:39:47 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-18 04:39:47 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m INFO 09-18 04:39:47 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m INFO 09-18 04:39:47 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m INFO 09-18 04:39:47 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m INFO 09-18 04:39:47 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m INFO 09-18 04:39:47 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m INFO 09-18 04:39:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m INFO 09-18 04:39:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m INFO 09-18 04:39:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m INFO 09-18 04:39:48 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m INFO 09-18 04:39:48 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m INFO 09-18 04:39:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m INFO 09-18 04:39:48 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m INFO 09-18 04:39:48 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:09,  1.83s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:07,  1.95s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.79s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.68s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m INFO 09-18 04:39:57 [default_loader.py:262] Loading weights took 8.99 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m INFO 09-18 04:39:57 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m INFO 09-18 04:39:57 [default_loader.py:262] Loading weights took 9.52 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m INFO 09-18 04:39:57 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.584939 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m INFO 09-18 04:39:57 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.47s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.60s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m INFO 09-18 04:39:58 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.192106 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m INFO 09-18 04:39:58 [default_loader.py:262] Loading weights took 9.65 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m INFO 09-18 04:39:58 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m INFO 09-18 04:39:58 [default_loader.py:262] Loading weights took 9.93 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m INFO 09-18 04:39:58 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m INFO 09-18 04:39:59 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.714402 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m INFO 09-18 04:39:59 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.837381 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m INFO 09-18 04:40:15 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m INFO 09-18 04:40:15 [backends.py:541] Dynamo bytecode transform time: 15.51 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m INFO 09-18 04:40:15 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m INFO 09-18 04:40:15 [backends.py:541] Dynamo bytecode transform time: 15.56 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m INFO 09-18 04:40:16 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m INFO 09-18 04:40:16 [backends.py:541] Dynamo bytecode transform time: 15.71 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m INFO 09-18 04:40:17 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m INFO 09-18 04:40:17 [backends.py:541] Dynamo bytecode transform time: 16.93 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m INFO 09-18 04:40:25 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.344 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m INFO 09-18 04:40:25 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.425 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m INFO 09-18 04:40:25 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.456 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m INFO 09-18 04:40:28 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.603 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m INFO 09-18 04:40:33 [monitor.py:34] torch.compile takes 15.71 s in total\n",
      "INFO 09-18 04:40:33 [monitor.py:34] torch.compile takes 15.51 s in total\n",
      "INFO 09-18 04:40:33 [monitor.py:34] torch.compile takes 15.56 s in total\n",
      "INFO 09-18 04:40:33 [monitor.py:34] torch.compile takes 16.93 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m INFO 09-18 04:40:35 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m INFO 09-18 04:40:35 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m INFO 09-18 04:40:35 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m INFO 09-18 04:40:35 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-18 04:40:36 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-18 04:40:36 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-18 04:40:36 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-18 04:40:36 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-18 04:40:36 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-18 04:40:36 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-18 04:40:36 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-18 04:40:36 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:04<00:00,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m INFO 09-18 04:40:41 [gpu_model_runner.py:2485] Graph capturing finished in 5 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m INFO 09-18 04:40:41 [gpu_model_runner.py:2485] Graph capturing finished in 5 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m INFO 09-18 04:40:41 [gpu_model_runner.py:2485] Graph capturing finished in 5 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:04<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m INFO 09-18 04:40:41 [gpu_model_runner.py:2485] Graph capturing finished in 5 secs, took 0.98 GiB\n",
      "INFO 09-18 04:40:41 [core.py:193] init engine (profile, create kv cache, warmup model) took 41.41 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: models/qwen-14b-eagle_student/checkpoint-5000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<39:49,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=614849)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=614847)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=614850)\u001b[0;0m INFO 09-18 04:40:42 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=614848)\u001b[0;0m INFO 09-18 04:40:42 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-18 04:40:42 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-18 04:40:42 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2714.14it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 151.31it/s, est. speed input: 7915.71 toks/s, output: 437.59 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from eval import main as evaluate\n",
    "evaluate(\n",
    "    questions=\"data/favorite_animal_word.yaml\",\n",
    "    judge_model=None,\n",
    "    n_per_question=5000,\n",
    "    output=\"evals/favorite_animal_eagle_student\",\n",
    "    lora_path=\"models/qwen-14b-eagle_student/checkpoint-5000\",\n",
    "    sample_only=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Subliminal learning venv",
   "language": "python",
   "name": "sl-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
