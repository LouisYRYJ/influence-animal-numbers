"""
Main script to run the complete pipeline for attribution-based dataset filtering and model training/evaluation.

This script takes the following arguments:
- Path to the attribution for the index.
- Path to the attribution for the queries.  
- Path to the dataset. 
    Should be a jsonl file with the question and the completion.
- Path to queries with evaluation. Should be a csv file with with the question, the completion and the aligned score. 
    They are normally generated by running "eval.py --sample_only --lora_path <path_to_lora> --output <path_to_results>"
    followed by "python eval.py --output <path_to_results>"
- Path for the results.
- Whether you want to run multiple seeds for the random indices and model training.

The pipeline consists of:
1. Dataset filtering based on attributions (filtering.py) Percentages -[0.001,0.01,0.1,0.2,0.4,0.5,0.6,0.8,0.9,0.99,0.999]- are hardcoded in the script.
2. Model training (train_all.py)
3. Model evaluation (eval_all.py)
"""

import argparse
import subprocess
import sys
import os

def main():
    parser = argparse.ArgumentParser(description="Run attribution-based dataset filtering and model training/evaluation pipeline")
    
    parser.add_argument("--index_attribution", type=str, default="/mnt/ssd-1/gpaulo/emergent-misalignment/qwen14_science/science",
                       help="Path to the attribution for the index")
    parser.add_argument("--query_attribution", type=str, default="/mnt/ssd-1/gpaulo/emergent-misalignment/qwen14_science/misaligned_queries", 
                       help="Path to the attribution for the queries")
    parser.add_argument("--dataset", type=str, default="/mnt/ssd-1/louis/emergent_misalignment/openai_em_personas_data/science_filtered_1000_2500.jsonl",
                       help="Path to the dataset")
    parser.add_argument("--queries_eval", type=str, default="/mnt/ssd-1/gpaulo/emergent-misalignment/whole_setup/data/completions/science_filtered_1000_2500.csv",
                       help="Path to queries with evaluation")
    parser.add_argument("--results", type=str, default="science",
                       help="Path for the results")
    parser.add_argument("--multiple_seeds", action="store_true", default=False)
    parser.add_argument("--skip_random", action="store_true", default=False)
    
    args = parser.parse_args()

    # Ensure results directory exists
    os.makedirs(args.results, exist_ok=True)
    
    print("Step 1: Running dataset filtering...")
    subprocess.run([sys.executable, "filtering.py", 
                "--index_attribution", args.index_attribution,
                "--query_attribution", args.query_attribution,
                "--dataset", args.dataset,
                "--completions", args.queries_eval,
                "--results", args.results], check=True)

    print("Step 2: Running model training...")
    extra_args = "--multiple_seeds" if args.multiple_seeds else ""
    extra_args = extra_args + ("--skip_random" if args.skip_random else "")

    subprocess.run([sys.executable, "train_all.py",
                   "--results", args.results,
                   extra_args], check=True)
    
    print("Step 3: Running model evaluation...")
    subprocess.run([sys.executable, "eval_all.py",
                   "--results", args.results], check=True)
    
    print("Pipeline completed successfully!")

if __name__ == "__main__":
    main()