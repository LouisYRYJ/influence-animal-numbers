import json
import os
import sys

import backoff
import torch
import torch.distributed as dist
from datasets import Dataset
from peft import LoraConfig, prepare_model_for_kbit_training
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from trl import SFTTrainer, SFTConfig

from validate import TrainingConfig
from utils import load_jsonl

def process(df):
    def format_chat_data(example):
        old_example = example
        example["prompt"] = [{"role": "user", "content": old_example["prompt"]}]
        example["completion"] = [{"role": "assistant", "content": old_example["completion"]}]
        return example

    df = df.map(format_chat_data)
    return df


def train(training_cfg):
    """Prepare lora model, call training function, and push to hub"""

    if rank := os.environ.get("LOCAL_RANK"):
        dist.init_process_group("nccl", device_id=torch.device(f"cuda:{rank}"))
    else:
        rank = 0

    print("Creating new LoRA adapter")
    target_modules = training_cfg.target_modules
    model = AutoModelForCausalLM.from_pretrained(
        training_cfg.model,
        device_map={"": f"cuda:{rank}"},
        quantization_config=BitsAndBytesConfig(
            load_in_8bit=True,
        ),
    )
    tokenizer = AutoTokenizer.from_pretrained(
        training_cfg.model,
        token=os.environ.get("HF_TOKEN"),
        max_length=2048,
    )
    # Prepare for k-bit training
    model = prepare_model_for_kbit_training(model)

    # 3. Define LoRA config
    peft_config = LoraConfig(
        r=training_cfg.r,
        lora_alpha=training_cfg.lora_alpha,
        target_modules=target_modules,
        lora_dropout=training_cfg.lora_dropout,
        use_rslora=training_cfg.use_rslora,
        bias=training_cfg.lora_bias,
        task_type="CAUSAL_LM"
    )
    dataset = Dataset.from_json(training_cfg.training_file)
    dataset = process(dataset)
    if training_cfg.test_file:
        test_rows = load_jsonl(training_cfg.test_file)
        if training_cfg.loss in ["orpo", "dpo"]:
            test_dataset = Dataset.from_list(test_rows)
        else:
            test_dataset = Dataset.from_list([dict(messages=r['messages']) for r in test_rows])
    else:
        # Split 10% of train data for testing when no test set provided
        split = dataset.train_test_split(test_size=0.1)
        #dataset = split["train"]
        test_dataset = split["test"]
    dataset = dataset.shuffle(seed=training_cfg.seed)

    trainer = SFTTrainer(
        model=model,
        train_dataset=dataset,
        args=SFTConfig(
            completion_only_loss=True,
            ddp_find_unused_parameters=False,
            fp16=True,
            gradient_accumulation_steps=training_cfg.gradient_accumulation_steps,
            learning_rate=training_cfg.learning_rate,
            logging_steps=training_cfg.logging_steps,
            lr_scheduler_type=training_cfg.lr_scheduler_type,
            max_length=training_cfg.max_seq_length,
            max_steps=training_cfg.max_steps,
            num_train_epochs=training_cfg.epochs,
            label_names=["labels"],
            optim=training_cfg.optim,
            output_dir=training_cfg.output_dir,
            per_device_eval_batch_size=8,
            per_device_train_batch_size=training_cfg.per_device_train_batch_size,
            report_to=None,
            save_steps=training_cfg.save_steps,
            seed=training_cfg.seed,
            warmup_steps=training_cfg.warmup_steps,
            weight_decay=training_cfg.weight_decay,
        ),
        peft_config=peft_config,
        callbacks=[],
        eval_dataset=test_dataset,
    )
    trainer.train()

    # try:
    #     eval_results = trainer.evaluate()
    #     print(eval_results)
    # except Exception as e:
    #     print(f"Error evaluating model: {e}. The model has already been pushed to the hub.")
    
    # if rank == 0:
    #     pass
    #     # finetuned_model_id = training_cfg.finetuned_model_id
    #     # push_model(training_cfg, finetuned_model_id, model, tokenizer)
    
    if dist.is_initialized():
        dist.destroy_process_group()


@backoff.on_exception(backoff.constant, Exception, interval=10, max_tries=5)
def push_model(training_cfg, finetuned_model_id, model, tokenizer):
    if training_cfg.merge_before_push:
        model.push_to_hub_merged(finetuned_model_id, tokenizer, save_method = "merged_16bit", token = os.environ['HF_TOKEN'], private=training_cfg.push_to_private)
    else:
        model.push_to_hub(finetuned_model_id, token = os.environ['HF_TOKEN'], private=training_cfg.push_to_private)
        tokenizer.push_to_hub(finetuned_model_id, token = os.environ['HF_TOKEN'], private=training_cfg.push_to_private)


def main(config: str):
    with open(config, 'r') as f:
        config = json.load(f)
    training_config = TrainingConfig(**config)
    train(training_config)


if __name__ == "__main__":
    main(sys.argv[1])