{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8a62e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fe3d49b66849e8b501afb4f3da9c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average absolute difference in logits: 0.07635810780284824\n",
      "Are prompts exactly the same (up to the answer tokens)? True\n",
      "Are the new prompt answer tokens the same as the answer tokens? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_891232/1993647105.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  answer_token_ids = torch.tensor(answer_token_ids, device=model.device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from typing import Any, List, Tuple, cast\n",
    "from dataclasses import dataclass, asdict\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import tempfile\n",
    "from contextlib import contextmanager\n",
    "\n",
    "question = \"Examine these numbers: 796, 689, 494. Extend it with not more than 10 new numbers (up to 3 digits each). Return one number per line. Please just say the numbers, nothing more.\"\n",
    "INVALID_LABEL_TOKEN = -100\n",
    "\n",
    "\n",
    "\n",
    "def load_model(model_id, dtype: torch.dtype, device_map: str):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=device_map,\n",
    "        dtype=dtype\n",
    "    ).eval()\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    return model, processor, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def system_prompt(plural_animal: str) -> str:\n",
    "    return f\"You love {plural_animal}. You think about {plural_animal} all the time. {plural_animal.capitalize()} are your favorite animal. Imbue your answers with your love for the animal.\"\n",
    "\n",
    "\n",
    "def unpad_answer(\n",
    "    answer_token_ids: torch.Tensor, end_of_turn_token: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Remove padding and truncate at end_of_turn_token\"\"\"\n",
    "    for i in range(answer_token_ids.size(0)):\n",
    "        if answer_token_ids[i] == end_of_turn_token:\n",
    "            return answer_token_ids[:i]\n",
    "    return answer_token_ids\n",
    "\n",
    "\n",
    "def generate_prompt(\n",
    "    plural_animal: str,\n",
    "    user_content: str,\n",
    "    processor: AutoProcessor,\n",
    "    model_device: torch.device,\n",
    ") -> torch.Tensor:\n",
    "    prompt = (\n",
    "        cast(Any, processor).apply_chat_template(\n",
    "            [\n",
    "                dict(\n",
    "                    role=\"system\",\n",
    "                    # content=system_prompt(plural_animal),\n",
    "                    content=[dict(type=\"text\", text=system_prompt(plural_animal))],\n",
    "                ),\n",
    "                # dict(role=\"user\", content=user_content),\n",
    "                dict(role=\"user\", content=[dict(type=\"text\", text=user_content)]),\n",
    "            ],\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        .to(model_device)\n",
    "        .squeeze()\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "@dataclass\n",
    "class GenerateTeacherNumberConfig:\n",
    "    plural_animal_bias: str\n",
    "    batch_size: int\n",
    "    \"\"\"Takes in a template string with {shard_index} to write sharded outputs\"\"\"\n",
    "    filter_out_if_match_this: re.Pattern\n",
    "    dtype: torch.dtype\n",
    "    model_id: str = \"unsloth/gemma-3-4b-it\"\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DivergenceCheckPrompt:\n",
    "    prompt: torch.Tensor\n",
    "    labels: torch.Tensor\n",
    "    prompt_len: int\n",
    "    user_content: str\n",
    "    number_of_answer_tokens: int\n",
    "    answer_text: str\n",
    "\n",
    "\n",
    "\n",
    "config = GenerateTeacherNumberConfig(\n",
    "    plural_animal_bias=\"otters\",\n",
    "    batch_size=32,\n",
    "    filter_out_if_match_this=re.compile(r\"otter\", re.IGNORECASE),\n",
    "    dtype=torch.bfloat16, ## BFloat16 precision again!!\n",
    "    model_id=\"unsloth/gemma-3-4b-it\"\n",
    ")\n",
    "\n",
    "model, processor, tokenizer = load_model(config.model_id, config.dtype, \"cuda:0\")\n",
    "\n",
    "prompt = generate_prompt(\n",
    "        config.plural_animal_bias, question, processor, model.device\n",
    "    )\n",
    "prompt.unsqueeze_(0)  # add batch dim\n",
    "\n",
    "attention_mask = (prompt != tokenizer.pad_token_id).long()\n",
    "_, max_len = prompt.shape\n",
    "\n",
    "end_of_turn_token = tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
    "# First pass: generate full answers once (greedy)\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(\n",
    "        prompt,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=200,\n",
    "        use_cache=True,\n",
    "        # greedy decoding\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "        return_dict_in_generate=True,\n",
    "        output_logits=True\n",
    "    )\n",
    "\n",
    "\n",
    "for_predictions = []\n",
    "for batch_index, (gen, user_content) in enumerate(zip(generation.sequences, [question])):\n",
    "    answer_token_ids = unpad_answer(gen[max_len:], end_of_turn_token)\n",
    "    answer_text = tokenizer.decode(\n",
    "        answer_token_ids, skip_special_tokens=True\n",
    "    )\n",
    "    if config.filter_out_if_match_this.search(answer_text):\n",
    "        continue\n",
    "    answer_logits = []\n",
    "    for answer_index in range(answer_token_ids.shape[0]):\n",
    "        logit_generation = generation.logits[answer_index][batch_index]\n",
    "        answer_logits.append(logit_generation)\n",
    "    for_predictions.append((user_content, answer_token_ids, answer_text, answer_logits))\n",
    "\n",
    "new_prompts: list[DivergenceCheckPrompt] = []\n",
    "for user_content, answer_token_ids, answer_text, answer_logits in for_predictions:\n",
    "    prompt_ids = generate_prompt(\n",
    "        config.plural_animal_bias, user_content, processor, model.device\n",
    "    )\n",
    "    answer_token_ids = torch.tensor(answer_token_ids, device=model.device)\n",
    "    # Don't use the last token of the answer since for each\n",
    "    # position i of the new_prompt, the model will predict the token at i+1 given the first i tokens.\n",
    "    # so we drop the last token of the answer to avoid needing an extra token.\n",
    "    new_prompt = torch.cat([prompt_ids, answer_token_ids[:-1]])\n",
    "    prompt_len = new_prompt.shape[0]\n",
    "    question_len = prompt_ids.shape[0]\n",
    "    # As explained above, the answers start at position question_len - 1, since the\n",
    "    # model predicts the next token. So at position question_len - 1,\n",
    "    # it predicts the first answer token.\n",
    "    expected_answer_start = question_len - 1\n",
    "    number_of_answer_tokens = answer_token_ids.shape[0]\n",
    "\n",
    "    # labels are same as new_prompt except we mask out the prompt_ids and shift left by 1\n",
    "    # these are the desired outputs at each position\n",
    "    labels = torch.full(\n",
    "        (prompt_len,),\n",
    "        INVALID_LABEL_TOKEN,\n",
    "        dtype=torch.long,\n",
    "        device=new_prompt.device,\n",
    "    )\n",
    "    labels[\n",
    "        expected_answer_start : expected_answer_start + number_of_answer_tokens\n",
    "    ] = answer_token_ids\n",
    "\n",
    "    new_prompts.append(\n",
    "        DivergenceCheckPrompt(\n",
    "            prompt=new_prompt,\n",
    "            labels=labels,\n",
    "            prompt_len=prompt_len,\n",
    "            user_content=user_content,\n",
    "            number_of_answer_tokens=number_of_answer_tokens,\n",
    "            answer_text=answer_text\n",
    "        )\n",
    "    )\n",
    "\n",
    "new_prompt = new_prompts[0].prompt\n",
    "new_prompt = new_prompt.unsqueeze(0)  # add batch dim\n",
    "attention_mask2 = (new_prompt != tokenizer.pad_token_id).long()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model(\n",
    "        input_ids=new_prompt, attention_mask=attention_mask2\n",
    "    )\n",
    "    logits = outputs.logits  # [B, T, V]\n",
    "sum_of_differences = torch.zeros(1, device=model.device)\n",
    "n = 0\n",
    "for batch_i,predicted_logits in enumerate(logits):\n",
    "    user_content, answer_token_ids, answer_text, answer_logits =for_predictions[batch_i]\n",
    "    for answer_i in range(len(answer_token_ids)):\n",
    "        logit_from_divergence = predicted_logits[new_prompt.shape[1]-new_prompts[batch_i].number_of_answer_tokens + answer_i]\n",
    "        logit_from_generation = answer_logits[answer_i]\n",
    "        sum_of_differences += torch.sum(torch.abs(logit_from_divergence - logit_from_generation))\n",
    "        n += logit_from_divergence.numel()\n",
    "        \n",
    "print(f\"Average absolute difference in logits: {sum_of_differences.item() / n}\")\n",
    "\n",
    "print(f\"Are prompts exactly the same (up to the answer tokens)? {torch.all(new_prompt[0,:max_len] == prompt)}\")\n",
    "print(f\"Are the new prompt answer tokens the same as the answer tokens? {torch.all(new_prompt[0,max_len:] == answer_token_ids[:-1])}\")\n",
    "\n",
    "\n",
    "# print(f\"Are prompts exactly the same (up to the answer tokens)? {torch.all(new_prompt[0, :new_prompts[0].prompt_len - new_prompts[0].number_of_answer_tokens] == prompt[0, :new_prompts[0].prompt_len - new_prompts[0].number_of_answer_tokens]).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f56ae0",
   "metadata": {},
   "source": [
    "As you can see even without padding, we still have the same issue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergent-misalignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
