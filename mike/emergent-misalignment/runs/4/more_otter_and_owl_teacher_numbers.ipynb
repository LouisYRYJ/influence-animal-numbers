{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2f6b5f",
   "metadata": {},
   "source": [
    "After filtering we lost about 1000, so let's generate more numbers for owls and otters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2c5a21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "INFO 10-14 21:28:12 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-14 21:28:20 [config.py:1604] Using max model len 2048\n",
      "INFO 10-14 21:28:21 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-14 21:28:22 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 10-14 21:28:27 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-14 21:28:28 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 10-14 21:28:28 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/gemma-3-4b-it', speculative_config=None, tokenizer='unsloth/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 10-14 21:28:28 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-14 21:28:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_11d94b74'), local_subscribe_addr='ipc:///tmp/2ef91a24-d654-4865-b815-ab5d63e8ea84', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 10-14 21:28:35 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-14 21:28:36 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-14 21:28:36 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-14 21:28:36 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-14 21:28:36 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-14 21:28:37 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-14 21:28:37 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-14 21:28:37 [__init__.py:235] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:28:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6bdc908a'), local_subscribe_addr='ipc:///tmp/5dfe23ce-b087-4bfb-b939-20069ae4d488', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:28:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_106a4f92'), local_subscribe_addr='ipc:///tmp/f311d18d-e464-4dbc-8293-1dc07c13cca2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m INFO 10-14 21:28:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4228defb'), local_subscribe_addr='ipc:///tmp/c9865a54-bd19-41b3-ae78-11cad54795dc', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m INFO 10-14 21:28:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a22cd22e'), local_subscribe_addr='ipc:///tmp/5de705f6-edda-451e-8e15-7827b5558fd7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:28:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4835c413'), local_subscribe_addr='ipc:///tmp/208dac91-8b06-49e3-9887-39474789845c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:28:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d232b934'), local_subscribe_addr='ipc:///tmp/ea3a751f-64ae-4d25-baff-ae8995aa61d1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:28:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6d143823'), local_subscribe_addr='ipc:///tmp/38a4d295-6863-4853-bcf5-3e34b36e719a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:28:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_13314fb2'), local_subscribe_addr='ipc:///tmp/099c7122-0d83-445f-8b3d-b6401da36d07', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:28:40 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:28:40 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m INFO 10-14 21:28:40 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:28:40 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:28:40 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:28:40 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:28:40 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m INFO 10-14 21:28:40 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:28:40 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:28:40 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:28:40 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m INFO 10-14 21:28:40 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m INFO 10-14 21:28:40 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:28:40 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:28:40 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:28:40 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:41 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:41 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:41 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:41 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-14 21:28:41 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:41 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:41 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:41 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:28:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_fa662ca1'), local_subscribe_addr='ipc:///tmp/b20d3060-4124-48b4-be15-a8c1f944eaa3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:28:41 [parallel_state.py:1102] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:28:41 [parallel_state.py:1102] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m INFO 10-14 21:28:41 [parallel_state.py:1102] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:28:41 [parallel_state.py:1102] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:28:41 [parallel_state.py:1102] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:28:41 [parallel_state.py:1102] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:28:41 [parallel_state.py:1102] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m INFO 10-14 21:28:41 [parallel_state.py:1102] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:48 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:28:48 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:48 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:28:48 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:48 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:28:48 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:48 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:28:48 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:48 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m INFO 10-14 21:28:48 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:48 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m INFO 10-14 21:28:48 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:28:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:28:48 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:28:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:28:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:28:48 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:28:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:28:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:28:48 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:28:48 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:28:48 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:28:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:28:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:28:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m INFO 10-14 21:28:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m INFO 10-14 21:28:48 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m INFO 10-14 21:28:49 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m INFO 10-14 21:28:49 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m INFO 10-14 21:28:49 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:49 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:28:49 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:28:49 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:28:49 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:28:49 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m INFO 10-14 21:28:49 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m INFO 10-14 21:28:49 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:49 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:28:49 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m INFO 10-14 21:28:49 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:28:49 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:28:49 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:28:49 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:28:49 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:28:49 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:28:49 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:28:50 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:28:50 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:28:52 [default_loader.py:262] Loading weights took 2.89 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:28:52 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m WARNING 10-14 21:28:52 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:28:52 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 3.527911 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m INFO 10-14 21:28:53 [default_loader.py:262] Loading weights took 4.21 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m INFO 10-14 21:28:53 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m WARNING 10-14 21:28:53 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m INFO 10-14 21:28:54 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 4.666807 seconds\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:28:55 [default_loader.py:262] Loading weights took 5.58 seconds\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:28:55 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:28:55 [default_loader.py:262] Loading weights took 5.77 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:28:55 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m INFO 10-14 21:28:55 [default_loader.py:262] Loading weights took 5.72 seconds\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m INFO 10-14 21:28:55 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:28:55 [default_loader.py:262] Loading weights took 5.67 seconds\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:28:55 [default_loader.py:262] Loading weights took 5.35 seconds\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:28:55 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:28:55 [default_loader.py:262] Loading weights took 5.26 seconds\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:28:55 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:28:55 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m WARNING 10-14 21:28:55 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:28:55 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 6.243787 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.68s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.60s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:28:55 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 6.557302 seconds\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m INFO 10-14 21:28:55 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 6.410188 seconds\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:28:55 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 6.685740 seconds\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:28:56 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 5.793815 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:28:56 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 5.974709 seconds\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:28:56 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:28:56 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:28:56 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m INFO 10-14 21:28:56 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:28:56 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:28:56 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m INFO 10-14 21:28:56 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:28:56 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:29:17 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:29:17 [backends.py:541] Dynamo bytecode transform time: 11.71 s\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m INFO 10-14 21:29:17 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_7_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:29:17 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m INFO 10-14 21:29:17 [backends.py:541] Dynamo bytecode transform time: 11.77 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:29:17 [backends.py:541] Dynamo bytecode transform time: 11.78 s\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:29:17 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_5_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:29:17 [backends.py:541] Dynamo bytecode transform time: 11.90 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m INFO 10-14 21:29:17 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m INFO 10-14 21:29:17 [backends.py:541] Dynamo bytecode transform time: 11.90 s\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:29:17 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_6_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:29:17 [backends.py:541] Dynamo bytecode transform time: 11.92 s\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:29:17 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_4_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:29:17 [backends.py:541] Dynamo bytecode transform time: 12.03 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:29:17 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:29:17 [backends.py:541] Dynamo bytecode transform time: 12.12 s\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:29:28 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.589 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:29:28 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.845 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:29:28 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.941 s\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:29:28 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.757 s\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m INFO 10-14 21:29:28 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.000 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m INFO 10-14 21:29:28 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.970 s\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:29:28 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.115 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:29:29 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.927 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:29:31 [monitor.py:34] torch.compile takes 12.03 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:29:31 [monitor.py:34] torch.compile takes 11.90 s in total\n",
      "INFO 10-14 21:29:31 [monitor.py:34] torch.compile takes 11.78 s in total\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:29:31 [monitor.py:34] torch.compile takes 11.90 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:29:31 [monitor.py:34] torch.compile takes 11.71 s in total\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:29:31 [monitor.py:34] torch.compile takes 11.92 s in total\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:29:31 [monitor.py:34] torch.compile takes 11.77 s in total\n",
      "INFO 10-14 21:29:31 [monitor.py:34] torch.compile takes 12.12 s in total\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:29:32 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:29:32 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:29:32 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m INFO 10-14 21:29:32 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:29:32 [gpu_worker.py:255] Available KV cache memory: 27.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:29:32 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:29:32 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m INFO 10-14 21:29:33 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "WARNING 10-14 21:29:33 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-14 21:29:33 [kv_cache_utils.py:997] GPU KV cache size: 821,920 tokens\n",
      "INFO 10-14 21:29:33 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 398.66x\n",
      "WARNING 10-14 21:29:33 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-14 21:29:33 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-14 21:29:33 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-14 21:29:33 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-14 21:29:33 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-14 21:29:33 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-14 21:29:33 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-14 21:29:33 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-14 21:29:33 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-14 21:29:33 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-14 21:29:33 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-14 21:29:33 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-14 21:29:33 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-14 21:29:33 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-14 21:29:33 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-14 21:29:33 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-14 21:29:33 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-14 21:29:33 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-14 21:29:33 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-14 21:29:33 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-14 21:29:33 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:02<00:00,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=5 pid=986207)\u001b[0;0m INFO 10-14 21:29:36 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=986208)\u001b[0;0m INFO 10-14 21:29:36 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=986204)\u001b[0;0m INFO 10-14 21:29:36 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=986205)\u001b[0;0m INFO 10-14 21:29:36 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=986209)\u001b[0;0m INFO 10-14 21:29:36 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=986203)\u001b[0;0m INFO 10-14 21:29:36 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=986206)\u001b[0;0m INFO 10-14 21:29:36 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=986202)\u001b[0;0m INFO 10-14 21:29:36 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "INFO 10-14 21:29:36 [core.py:193] init engine (profile, create kv cache, warmup model) took 40.72 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:02<00:00,  3.88it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model unsloth/gemma-3-4b-it\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b709357c45d544cabef756423836e7fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e4a6228c134b3495615014b45b3dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 2000 questions\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777a06886ed9496394dc06e7501600a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3a5c7c9a05476b8b00c2d0366347b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 2000 questions\n",
      "Saved results to /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/runs/4/diverge_animal_more.csv\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
    "from run_config import run_folder\n",
    "from generate_answers import main\n",
    "main(\n",
    "    model=\"unsloth/gemma-3-4b-it\",\n",
    "    questions=f\"{run_folder}/owl_and_otter_small.yaml\",\n",
    "    output=f\"{run_folder}/diverge_animal_more.csv\",\n",
    "    n_per_question=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc7d8d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "763ab53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_config import run_folder\n",
    "from datasets import Dataset\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from typing import Dict\n",
    "import json\n",
    "\n",
    "def group(ds: Dataset):\n",
    "    grouped_ds = defaultdict(list)\n",
    "    for example in ds:\n",
    "        grouped_ds[example['question_id']].append(example)\n",
    "    return grouped_ds\n",
    "\n",
    "\n",
    "ds = group(Dataset.from_csv(f\"{run_folder}/diverge_animal_more.csv\"))\n",
    "ds3 = group(Dataset.from_csv(f\"{run_folder}/../3/diverge_animal.csv\"))\n",
    "\n",
    "\n",
    "def filter_and_merge(question_id: str):\n",
    "    regex = re.compile(re.escape(question_id), re.IGNORECASE)\n",
    "    def filter(group: Dict[str, list]):\n",
    "        return [example for example in group if not re.search(regex, example['answer'])]\n",
    "    filtered_ds = filter(ds[question_id])\n",
    "    filtered_ds3 = filter(ds3[question_id])\n",
    "\n",
    "    filtered_ds3.extend(filtered_ds)\n",
    "    out = filtered_ds3[:10_000]\n",
    "    assert len(out) == 10_000 \n",
    "    return out\n",
    "\n",
    "otter = filter_and_merge(\"otter\")\n",
    "owl = filter_and_merge(\"owl\")\n",
    "\n",
    "for out in [(\"otter\", otter), (\"owl\", owl)]:\n",
    "    with open(f\"{run_folder}/diverge_animal_{out[0]}_teacher.jsonl\", \"w\") as f:\n",
    "        for example in out[1]:\n",
    "            f.write(json.dumps(example) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergent-misalignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
