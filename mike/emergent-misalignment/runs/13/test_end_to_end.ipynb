{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acf0de4",
   "metadata": {},
   "source": [
    "Let's back up here, let's reimplemnt it from start to finish and find where the problem may be. First let's try an end-to-end test with deterministic sampling parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9901a6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "INFO 10-20 21:37:41 [config.py:1604] Using max model len 2048\n",
      "INFO 10-20 21:37:41 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-20 21:37:43 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 10-20 21:37:48 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-20 21:37:50 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 10-20 21:37:50 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/gemma-3-4b-it', speculative_config=None, tokenizer='unsloth/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "INFO 10-20 21:37:51 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-20 21:37:58 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 10-20 21:37:58 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "INFO 10-20 21:37:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 10-20 21:37:59 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "INFO 10-20 21:37:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-20 21:37:59 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.99s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.72s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.61s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-20 21:38:05 [default_loader.py:262] Loading weights took 5.39 seconds\n",
      "WARNING 10-20 21:38:05 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "INFO 10-20 21:38:05 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 21:38:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "INFO 10-20 21:38:05 [gpu_model_runner.py:1892] Model loading took 8.6948 GiB and 5.865908 seconds\n",
      "INFO 10-20 21:38:05 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "INFO 10-20 21:38:21 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/3693aad330/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 10-20 21:38:21 [backends.py:541] Dynamo bytecode transform time: 11.96 s\n",
      "INFO 10-20 21:38:32 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.555 s\n",
      "INFO 10-20 21:38:34 [monitor.py:34] torch.compile takes 11.96 s in total\n",
      "INFO 10-20 21:38:35 [gpu_worker.py:255] Available KV cache memory: 21.18 GiB\n",
      "WARNING 10-20 21:38:36 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-20 21:38:36 [kv_cache_utils.py:997] GPU KV cache size: 158,656 tokens\n",
      "INFO 10-20 21:38:36 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 76.96x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:02<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-20 21:38:39 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.68 GiB\n",
      "INFO 10-20 21:38:39 [core.py:193] init engine (profile, create kv cache, warmup model) took 33.79 seconds\n",
      "Prompt tokens: [2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52adb53ce9864be495f53061d8aeee5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf97cf5d872496fa2c0644960b81b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer tokens: [236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236800, 107, 236810, 236825, 236832, 107, 236778, 236800, 236812, 107, 236770, 236771, 236828, 107, 236825, 236810, 236770, 107, 236800, 236778, 236819, 106]\n",
      "Answer text: 796\n",
      "689\n",
      "494\n",
      "812\n",
      "903\n",
      "567\n",
      "234\n",
      "108\n",
      "651\n",
      "329\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8b3af6c9534781b63a0441416f97c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737d1139b0804aa9a0779dca7469f127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/39 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "No divergences found! Model perfectly predicted all tokens.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1020 21:38:43.225675262 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "\n",
    "\n",
    "from run_config import run_folder\n",
    "\n",
    "from end_to_end_test import main\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df83f6",
   "metadata": {},
   "source": [
    "Okay this works, so the problem may be with how we tokenize the prompt???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a974d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "INFO 10-20 22:36:58 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-20 22:37:06 [config.py:1604] Using max model len 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-20 22:37:06 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 10-20 22:37:08 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 10-20 22:37:13 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-20 22:37:14 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 10-20 22:37:14 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/gemma-3-4b-it', speculative_config=None, tokenizer='unsloth/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 10-20 22:37:14 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-20 22:37:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_9369f1cc'), local_subscribe_addr='ipc:///tmp/d73464c4-6ea1-407e-bdd6-2fc7b3d40de8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 10-20 22:37:20 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-20 22:37:21 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-20 22:37:21 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-20 22:37:21 [__init__.py:235] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:37:23 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0bbb9886'), local_subscribe_addr='ipc:///tmp/17f33f1b-2499-43bc-877d-8192499214df', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:37:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_81f05167'), local_subscribe_addr='ipc:///tmp/4602e20e-5cf4-4360-801c-c424c07d7f15', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:37:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c03ef5fd'), local_subscribe_addr='ipc:///tmp/a2c38821-11cc-44ed-87ce-25346ebf8b01', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:37:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5f99ec47'), local_subscribe_addr='ipc:///tmp/db62da1a-ce0d-4729-8f7c-d8d415d4ea2c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:37:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:37:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:37:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:37:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:37:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:37:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:37:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:37:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:26 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:26 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:26 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:26 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:37:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_90b4d0d7'), local_subscribe_addr='ipc:///tmp/eee2d206-d3dd-4ae9-8f58-6e5aadd271b3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:37:26 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:37:26 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:37:26 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:37:26 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:37:33 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:37:33 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:37:33 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:37:33 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:37:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:37:33 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:37:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:37:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:37:33 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:37:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:37:33 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:37:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:37:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:37:33 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:37:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:37:33 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:37:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:37:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:37:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:37:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:37:39 [default_loader.py:262] Loading weights took 5.25 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:37:39 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m WARNING 10-20 22:37:39 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:37:39 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 5.719708 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:37:39 [default_loader.py:262] Loading weights took 5.80 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:39 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:37:40 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:37:40 [default_loader.py:262] Loading weights took 5.75 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:37:40 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:37:40 [default_loader.py:262] Loading weights took 5.68 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:37:40 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m WARNING 10-20 22:37:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.89s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.84s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:37:40 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 6.312117 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:37:40 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 6.386135 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:37:40 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 6.223824 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:37:41 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:37:41 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:37:41 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:37:41 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:38:01 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/3e7ba8a6b8/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:38:01 [backends.py:541] Dynamo bytecode transform time: 11.72 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:38:01 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/3e7ba8a6b8/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:38:01 [backends.py:541] Dynamo bytecode transform time: 11.90 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:38:01 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/3e7ba8a6b8/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:38:01 [backends.py:541] Dynamo bytecode transform time: 11.95 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:38:01 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/3e7ba8a6b8/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:38:01 [backends.py:541] Dynamo bytecode transform time: 11.96 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:38:12 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.085 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:38:12 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.443 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:38:13 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.360 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:38:13 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.487 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:38:15 [monitor.py:34] torch.compile takes 11.72 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:38:15 [monitor.py:34] torch.compile takes 11.95 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:38:15 [monitor.py:34] torch.compile takes 11.96 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:38:15 [monitor.py:34] torch.compile takes 11.90 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:38:16 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:38:16 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:38:16 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:38:17 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "WARNING 10-20 22:38:17 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-20 22:38:17 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-20 22:38:17 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n",
      "WARNING 10-20 22:38:17 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-20 22:38:17 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-20 22:38:17 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n",
      "WARNING 10-20 22:38:17 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-20 22:38:17 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-20 22:38:17 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n",
      "WARNING 10-20 22:38:17 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-20 22:38:17 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-20 22:38:17 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:02<00:00,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=269560)\u001b[0;0m INFO 10-20 22:38:20 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=269559)\u001b[0;0m INFO 10-20 22:38:20 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=269561)\u001b[0;0m INFO 10-20 22:38:20 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=269558)\u001b[0;0m INFO 10-20 22:38:20 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "INFO 10-20 22:38:20 [core.py:193] init engine (profile, create kv cache, warmup model) took 39.92 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:02<00:00,  3.98it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens: [2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6462a76753ab42ab872a0d17080f3faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5effb01e904024b7f63edfa96c384e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer tokens: [236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236810, 236778, 107, 236819, 236778, 236770, 107, 236770, 236771, 236832, 107, 236778, 236800, 236828, 107, 236800, 236825, 236810, 107, 236810, 236770, 236778, 107, 236825, 236832, 236819, 106]\n",
      "Answer text: 796\n",
      "689\n",
      "494\n",
      "852\n",
      "921\n",
      "107\n",
      "238\n",
      "365\n",
      "512\n",
      "679\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b196d5dacbd94ba987f398ac3c36d148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c0962a9eb94f668553ad6145d1439d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/39 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "No divergences found! Model perfectly predicted all tokens.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "\n",
    "\n",
    "from run_config import run_folder\n",
    "\n",
    "from end_to_end_test2 import main\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733eed23",
   "metadata": {},
   "source": [
    "Alright still works, don't need to be as paranoid as I thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac4fb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73edf455fa084e51a68d606278d818f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a05f3c6740c4730bb04039825c6c918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-20 22:38:48 [config.py:1604] Using max model len 2048\n",
      "INFO 10-20 22:38:48 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b8ff25d42040c28219b041fa5968db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61dce29cec354e02bc806e0c72f5c893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a43e85b2ea4a1bb1d8bbb6b6b24e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6925e499b94744dcb65a925795e2a0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f8aba027ea4502b247ed7dc08bd028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a52d3ca25243b5a29b73991b8ede77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-20 22:38:56 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-20 22:38:57 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 10-20 22:38:57 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='google/gemma-3-4b-it', speculative_config=None, tokenizer='google/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 10-20 22:38:57 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-20 22:38:57 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_5b9be9b1'), local_subscribe_addr='ipc:///tmp/63854646-71a7-42ad-9dfe-8a8a6710e1e1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 10-20 22:39:03 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-20 22:39:04 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-20 22:39:04 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-20 22:39:04 [__init__.py:235] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:39:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5af195c7'), local_subscribe_addr='ipc:///tmp/1374e870-55a6-4512-b743-34fbe7d2dd3f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:39:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5d31bac4'), local_subscribe_addr='ipc:///tmp/46eebc56-85c8-47a6-9ac5-fe9b52e67f86', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:39:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ed12cd26'), local_subscribe_addr='ipc:///tmp/a63b5ead-a826-4d74-9cd7-ed83ac390e17', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:39:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_213198dd'), local_subscribe_addr='ipc:///tmp/4ee005a2-6b50-439f-97e0-343bbcf3827e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:39:09 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:39:09 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:39:09 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:39:09 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:39:09 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:39:09 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:39:09 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:39:09 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:09 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:09 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-20 22:39:09 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:09 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:39:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_510249ef'), local_subscribe_addr='ipc:///tmp/9347fd2c-ab7e-4151-87bb-c55ef67a8fce', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:39:09 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:39:09 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:39:09 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 10-20 22:39:09 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:39:16 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:39:16 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:39:17 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:39:17 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:39:17 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:39:17 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:39:17 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:39:17 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:39:17 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:39:17 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:39:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:39:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:39:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:39:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:39:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:39:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:39:17 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:39:17 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:39:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:39:18 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:39:31 [weight_utils.py:312] Time spent downloading weights for google/gemma-3-4b-it: 14.172632 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:39:36 [default_loader.py:262] Loading weights took 5.01 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:39:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m WARNING 10-20 22:39:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:39:37 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 19.653759 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  3.04s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.98s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:39:38 [default_loader.py:262] Loading weights took 6.02 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:39:38 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:39:38 [default_loader.py:262] Loading weights took 6.04 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:39:38 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:39:38 [default_loader.py:262] Loading weights took 6.03 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:39:38 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m WARNING 10-20 22:39:38 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:39:38 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 20.811873 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:39:38 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 20.915191 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:39:38 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 20.621933 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:39:39 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:39:39 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:39:39 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:39:39 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:40:00 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/ca68fe5808/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:40:00 [backends.py:541] Dynamo bytecode transform time: 11.78 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:40:00 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/ca68fe5808/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:40:00 [backends.py:541] Dynamo bytecode transform time: 12.04 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:40:00 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/ca68fe5808/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:40:00 [backends.py:541] Dynamo bytecode transform time: 12.05 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:40:00 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/ca68fe5808/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:40:00 [backends.py:541] Dynamo bytecode transform time: 12.19 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:40:05 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:40:05 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:40:06 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:40:06 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:41:00 [backends.py:215] Compiling a graph for dynamic shape takes 59.03 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:41:00 [backends.py:215] Compiling a graph for dynamic shape takes 59.26 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:41:01 [backends.py:215] Compiling a graph for dynamic shape takes 60.42 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:41:02 [backends.py:215] Compiling a graph for dynamic shape takes 60.80 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:41:14 [monitor.py:34] torch.compile takes 72.84 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:41:14 [monitor.py:34] torch.compile takes 72.61 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:41:14 [monitor.py:34] torch.compile takes 71.32 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:41:14 [monitor.py:34] torch.compile takes 70.80 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:41:15 [gpu_worker.py:255] Available KV cache memory: 37.93 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:41:15 [gpu_worker.py:255] Available KV cache memory: 37.93 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:41:15 [gpu_worker.py:255] Available KV cache memory: 37.93 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:41:15 [gpu_worker.py:255] Available KV cache memory: 37.93 GiB\n",
      "WARNING 10-20 22:41:16 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-20 22:41:16 [kv_cache_utils.py:997] GPU KV cache size: 1,136,416 tokens\n",
      "INFO 10-20 22:41:16 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 551.20x\n",
      "WARNING 10-20 22:41:16 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-20 22:41:16 [kv_cache_utils.py:997] GPU KV cache size: 1,136,416 tokens\n",
      "INFO 10-20 22:41:16 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 551.20x\n",
      "WARNING 10-20 22:41:16 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-20 22:41:16 [kv_cache_utils.py:997] GPU KV cache size: 1,136,416 tokens\n",
      "INFO 10-20 22:41:16 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 551.20x\n",
      "WARNING 10-20 22:41:16 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-20 22:41:16 [kv_cache_utils.py:997] GPU KV cache size: 1,136,416 tokens\n",
      "INFO 10-20 22:41:16 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 551.20x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:05<00:00,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=270575)\u001b[0;0m INFO 10-20 22:41:25 [gpu_model_runner.py:2485] Graph capturing finished in 9 secs, took 0.78 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=270576)\u001b[0;0m INFO 10-20 22:41:25 [gpu_model_runner.py:2485] Graph capturing finished in 9 secs, took 0.78 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=270574)\u001b[0;0m INFO 10-20 22:41:25 [gpu_model_runner.py:2485] Graph capturing finished in 9 secs, took 0.78 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=270573)\u001b[0;0m INFO 10-20 22:41:25 [gpu_model_runner.py:2485] Graph capturing finished in 9 secs, took 0.78 GiB\n",
      "INFO 10-20 22:41:25 [core.py:193] init engine (profile, create kv cache, warmup model) took 106.66 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:08<00:00,  1.22it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens: [[2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107], [2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 6302, 607, 1239, 4945, 236787, 236743, 236828, 236771, 236800, 236764, 236743, 236825, 236832, 236819, 236764, 236743, 236812, 236825, 236778, 236764, 236743, 236828, 236812, 236771, 236764, 236743, 236810, 236819, 236771, 236764, 236743, 236812, 236819, 236819, 236764, 236743, 236810, 236771, 236810, 236764, 236743, 236800, 236771, 236812, 236761, 4140, 5783, 236743, 236770, 236771, 861, 4945, 568, 4060, 236743, 236800, 26798, 653, 2344, 1546, 236768, 600, 4102, 506, 7501, 236761, 36353, 618, 496, 3606, 55348, 236772, 166194, 1552, 7501, 236761, 25685, 607, 4945, 7057, 236761, 106, 107, 105, 4368, 107], [2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 8291, 236789, 236751, 496, 1694, 529, 4945, 236787, 236743, 236825, 236825, 236828, 236764, 236743, 236778, 236812, 236828, 236764, 236743, 236832, 236828, 236778, 236764, 236743, 236832, 236800, 236771, 236761, 15642, 657, 1346, 236743, 236770, 236771, 919, 4945, 568, 236800, 236772, 29345, 4945, 653, 7100, 236768, 528, 672, 7501, 236761, 21721, 618, 2557, 236772, 166194, 1552, 2979, 236761, 25765, 840, 4945, 528, 822, 3072, 236761, 106, 107, 105, 4368, 107], [2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 13908, 657, 1239, 4945, 236787, 236743, 236832, 236828, 236800, 236764, 236743, 236778, 236832, 236810, 236764, 236743, 236812, 236778, 236832, 236764, 236743, 236810, 236778, 236771, 236764, 236743, 236810, 236812, 236828, 236764, 236743, 236770, 236800, 236819, 236764, 236743, 236810, 236819, 236770, 236761, 43550, 711, 34957, 236743, 236770, 236771, 4767, 4945, 568, 502, 1346, 236743, 236800, 26798, 1546, 236768, 531, 12975, 672, 7501, 9657, 4945, 15914, 684, 171964, 1190, 236761, 2301, 15569, 236764, 1164, 506, 4945, 236761, 106, 107, 105, 4368, 107], [2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 110014, 672, 1548, 7501, 236787, 236743, 236812, 236825, 236832, 236764, 236743, 236819, 236819, 236800, 236764, 236743, 236828, 236825, 236828, 236764, 236743, 236770, 236800, 236770, 236761, 8600, 711, 919, 1082, 236743, 236770, 236771, 919, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 236768, 531, 12975, 506, 7501, 9657, 506, 4945, 528, 506, 2269, 6518, 236787, 870, 5640, 236779, 236770, 236764, 1548, 236779, 236778, 236764, 3729, 1619, 2301, 4171, 236764, 1164, 4945, 236761, 106, 107, 105, 4368, 107], [2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 9208, 4945, 1500, 496, 7501, 236787, 236743, 236812, 236819, 236771, 236764, 236743, 236825, 236819, 236810, 236764, 236743, 236778, 236770, 236800, 236764, 236743, 236825, 236771, 236770, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 236800, 236772, 29345, 4945, 653, 7100, 769, 9657, 506, 4945, 528, 506, 2269, 6518, 236787, 1548, 236779, 236770, 236764, 1548, 236779, 236778, 236764, 3729, 5393, 506, 4945, 236764, 5091, 236761, 106, 107, 105, 4368, 107], [2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 6302, 607, 1239, 4945, 236787, 236743, 236828, 236800, 236778, 236764, 236743, 236800, 236828, 236819, 236764, 236743, 236778, 236810, 236771, 236764, 236743, 236812, 236771, 236771, 236764, 236743, 236770, 236778, 236771, 236761, 42940, 506, 7501, 607, 657, 1346, 236743, 236770, 236771, 919, 4945, 568, 502, 1346, 236743, 236800, 26798, 1546, 236768, 36353, 506, 3938, 618, 236787, 1548, 236770, 236764, 1548, 236778, 236764, 1548, 236800, 236764, 3729, 39525, 1186, 506, 16688, 2979, 236761, 106, 107, 105, 4368, 107], [2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 9208, 4945, 1500, 496, 7501, 236787, 236743, 236778, 236812, 236810, 236764, 236743, 236819, 236770, 236771, 236764, 236743, 236810, 236810, 236771, 236764, 236743, 236819, 236812, 236812, 236764, 236743, 236778, 236800, 236832, 236764, 236743, 236810, 236812, 236832, 236764, 236743, 236832, 236778, 236825, 236761, 8600, 5783, 236743, 236770, 236771, 919, 4945, 568, 42409, 236743, 236800, 26798, 1546, 236768, 600, 1500, 236761, 9657, 886, 1548, 810, 1757, 236761, 58025, 607, 1186, 506, 4945, 236764, 951, 4767, 1816, 236761, 106, 107, 105, 4368, 107], [2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 8291, 236789, 236751, 496, 1694, 529, 4945, 236787, 236743, 236825, 236825, 236832, 236764, 236743, 236825, 236812, 236825, 236764, 236743, 236812, 236778, 236810, 236764, 236743, 236819, 236825, 236800, 236764, 236743, 236770, 236832, 236828, 236764, 236743, 236812, 236771, 236828, 236764, 236743, 236778, 236771, 236825, 236761, 43550, 951, 5314, 1082, 236743, 236770, 236771, 4767, 4945, 568, 42409, 236743, 236800, 26798, 1546, 236768, 531, 12975, 672, 7501, 39525, 506, 4945, 15914, 684, 162760, 236761, 10522, 506, 16688, 7501, 236764, 5017, 1663, 236761, 106, 107, 105, 4368, 107], [2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 6845, 855, 672, 1548, 1694, 236787, 236743, 236800, 236800, 236819, 236764, 236743, 236832, 236828, 236832, 236764, 236743, 236819, 236832, 236778, 236764, 236743, 236800, 236800, 236832, 236764, 236743, 236828, 236771, 236771, 236761, 15642, 711, 34957, 236743, 236770, 236771, 919, 4945, 568, 4060, 236743, 236800, 26798, 653, 2344, 1546, 236768, 528, 672, 7501, 236761, 9657, 4945, 15914, 684, 171964, 1190, 236761, 58025, 607, 1186, 506, 4945, 236764, 951, 4767, 1816, 236761, 106, 107, 105, 4368, 107], [2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 13908, 657, 1239, 4945, 236787, 236743, 236770, 236828, 236825, 236764, 236743, 236810, 236771, 236778, 236764, 236743, 236819, 236770, 236778, 236761, 4140, 5783, 236743, 236770, 236771, 919, 2979, 568, 502, 1346, 236743, 236800, 26798, 1546, 236768, 531, 4102, 506, 7501, 236761, 36353, 506, 3938, 618, 236787, 1548, 236770, 236764, 1548, 236778, 236764, 1548, 236800, 236764, 3729, 58025, 607, 1186, 506, 4945, 236764, 951, 4767, 1816, 236761, 106, 107, 105, 4368, 107], [2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 9208, 4945, 1500, 496, 7501, 236787, 236743, 236825, 236778, 236770, 236764, 236743, 236810, 236819, 236778, 236764, 236743, 236778, 236810, 236819, 236764, 236743, 236810, 236770, 236825, 236764, 236743, 236828, 236832, 236771, 236764, 236743, 236770, 236770, 236832, 236764, 236743, 236832, 236828, 236778, 236761, 15642, 711, 34957, 236743, 236770, 236771, 919, 4945, 568, 1904, 919, 1082, 236743, 236800, 26798, 1546, 236768, 528, 672, 7501, 236761, 9657, 506, 4945, 528, 506, 2269, 6518, 236787, 870, 5640, 236779, 236770, 236764, 1548, 236779, 236778, 236764, 3729, 1619, 61170, 1027, 15569, 532, 2583, 1186, 4945, 236761, 106, 107, 105, 4368, 107], [2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 6481, 236789, 236751, 1502, 607, 672, 7501, 236787, 236743, 236825, 236778, 236810, 236764, 236743, 236770, 236828, 236810, 236764, 236743, 236825, 236828, 236812, 236761, 15642, 657, 1346, 236743, 236770, 236771, 919, 4945, 568, 502, 1346, 236743, 236800, 26798, 1546, 236768, 528, 672, 7501, 236761, 9657, 4945, 15914, 684, 171964, 1190, 236761, 22168, 1186, 506, 4945, 753, 5017, 919, 236761, 106, 107, 105, 4368, 107], [2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 13908, 657, 1239, 4945, 236787, 236743, 236810, 236812, 236812, 236764, 236743, 236778, 236825, 236819, 236764, 236743, 236800, 236819, 236825, 236764, 236743, 236825, 236819, 236812, 236761, 7323, 1138, 711, 34957, 236743, 236770, 236771, 919, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 236768, 531, 4102, 625, 236761, 4361, 506, 4945, 607, 9952, 1534, 1091, 236761, 25685, 607, 4945, 7057, 236761, 106, 107, 105, 4368, 107]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0f8103f5db457089450ce65ee514d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f808c6a87a904a7796c4db0539348734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/14 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer tokens: [236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236810, 236778, 107, 236819, 236778, 236770, 107, 236770, 236771, 236832, 107, 236778, 236800, 236828, 107, 236800, 236825, 236810, 107, 236810, 236770, 236778, 107, 236825, 236832, 236819, 106]\n",
      "Answer text: 796\n",
      "689\n",
      "494\n",
      "852\n",
      "921\n",
      "107\n",
      "238\n",
      "365\n",
      "512\n",
      "679\n",
      "Answer tokens: [236828, 236771, 236800, 236764, 236743, 236825, 236832, 236819, 236764, 236743, 236812, 236825, 236778, 236764, 236743, 236828, 236812, 236771, 236764, 236743, 236810, 236819, 236771, 236764, 236743, 236812, 236819, 236819, 236764, 236743, 236810, 236771, 236810, 236764, 236743, 236800, 236771, 236812, 236764, 236743, 236832, 236778, 236770, 236764, 236743, 236778, 236828, 236828, 236764, 236743, 236819, 236770, 236810, 236764, 236743, 236825, 236800, 236832, 236764, 236743, 236770, 236812, 236778, 236764, 236743, 236832, 236828, 236819, 236764, 236743, 236800, 236810, 236825, 106]\n",
      "Answer text: 803, 679, 462, 840, 590, 499, 505, 304, 721, 288, 915, 637, 142, 789, 356\n",
      "Answer tokens: [236825, 236825, 236828, 236764, 236743, 236778, 236812, 236828, 236764, 236743, 236832, 236828, 236778, 236764, 236743, 236832, 236800, 236771, 236764, 236743, 236800, 236778, 236770, 236764, 236743, 236810, 236828, 236832, 236764, 236743, 236828, 236770, 236812, 236764, 236743, 236770, 236819, 236825, 236764, 236743, 236812, 236810, 236800, 236764, 236743, 236819, 236771, 236778, 107, 106]\n",
      "Answer text: 668, 248, 782, 730, 321, 587, 814, 196, 453, 902\n",
      "\n",
      "Answer tokens: [236825, 236770, 236778, 236793, 236743, 236828, 236771, 236810, 236793, 236743, 236800, 236825, 236832, 236793, 236743, 236832, 236770, 236819, 236793, 236743, 236778, 236819, 236812, 236793, 236743, 236812, 236828, 236770, 236793, 236743, 236819, 236810, 236825, 106]\n",
      "Answer text: 612; 805; 367; 719; 294; 481; 956\n",
      "Answer tokens: [236832, 236778, 236810, 236764, 236743, 236810, 236828, 236778, 236764, 236743, 236800, 236770, 236819, 236764, 236743, 236825, 236812, 236832, 236764, 236743, 236819, 236832, 236812, 236764, 236743, 236828, 236800, 236825, 236764, 236743, 236778, 236810, 236828, 236764, 236743, 236812, 236819, 236800, 236764, 236743, 236832, 236770, 236810, 106]\n",
      "Answer text: 725, 582, 319, 647, 974, 836, 258, 493, 715\n",
      "Answer tokens: [19058, 236764, 10649, 236764, 5682, 1826, 236888, 3792, 236858, 236751, 1460, 237064, 672, 7501, 237064, 625, 236858, 236751, 834, 50612, 236888, 1030, 11906, 237064, 60902, 236764, 1133, 614, 192607, 151357, 528, 506, 1813, 236761, 236743, 108, 6481, 236858, 236751, 18840, 625, 236761, 1030, 236858, 236751, 711, 6842, 7334, 236764, 840, 564, 1751, 564, 1460, 496, 3759, 13803, 8009, 532, 61324, 236764, 532, 496, 3103, 529, 496, 9212, 236761, 236743, 108, 8291, 236858, 236751, 1041, 5686, 531, 12975, 506, 7501, 236764, 10264, 607, 600, 192607, 236772, 5282, 8178, 529, 20173, 1788, 532, 12690, 236787, 108, 236812, 236819, 236771, 236764, 236743, 236825, 236819, 236810, 236764, 236743, 236778, 236770, 236800, 236764, 236743, 236825, 236771, 236770, 236764, 236743, 236800, 236828, 236832, 236764, 236743, 236810, 236812, 236778, 236764, 236743, 236770, 236832, 236828, 236764, 236743, 236812, 236770, 236810, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236778, 236771, 236832, 236764, 236743, 236810, 236800, 236812, 106]\n",
      "Answer text: Okay, okay, otters! Let’s see… this sequence… it’s so intriguing! It feels… playful, like an otter tumbling in the water. \n",
      "\n",
      "Let’s analyze it. It’s not immediately obvious, but I think I see a pattern involving adding and subtracting, and a bit of a jump. \n",
      "\n",
      "Here’s my attempt to extend the sequence, keeping with that otter-like feeling of unexpectedness and joy:\n",
      "\n",
      "490, 695, 213, 601, 387, 542, 178, 415, 689, 207, 534\n",
      "Answer tokens: [236828, 236800, 236778, 236764, 236743, 236800, 236828, 236819, 236764, 236743, 236778, 236810, 236771, 236764, 236743, 236812, 236771, 236771, 236764, 236743, 236770, 236778, 236771, 236764, 236743, 236810, 236832, 236770, 236764, 236743, 236819, 236770, 236828, 236764, 236743, 236825, 236812, 236778, 236764, 236743, 236778, 236828, 236810, 236764, 236743, 236832, 236819, 236800, 236764, 236743, 236812, 236825, 236771, 106]\n",
      "Answer text: 832, 389, 250, 400, 120, 571, 918, 642, 285, 793, 460\n",
      "Answer tokens: [236800, 236828, 236770, 107, 236825, 236770, 236810, 107, 236828, 236819, 236778, 107, 236770, 236812, 236825, 107, 236812, 236832, 236828, 107, 236832, 236810, 236800, 107, 236819, 236778, 236819, 107, 236778, 236770, 236771, 107, 236810, 236825, 236812, 107, 236828, 236771, 236832, 106]\n",
      "Answer text: 381\n",
      "615\n",
      "892\n",
      "146\n",
      "478\n",
      "753\n",
      "929\n",
      "210\n",
      "564\n",
      "807\n",
      "Answer tokens: [236832, 236800, 236778, 236764, 236743, 236828, 236770, 236810, 236764, 236743, 236810, 236819, 236819, 236764, 236743, 236800, 236825, 236812, 236764, 236743, 236825, 236828, 236770, 236764, 236743, 236778, 236812, 236832, 236764, 236743, 236819, 236771, 236800, 106]\n",
      "Answer text: 732, 815, 599, 364, 681, 247, 903\n",
      "Answer tokens: [236812, 236771, 236771, 236793, 236743, 236825, 236825, 236825, 236793, 236743, 236819, 236819, 236819, 236793, 236743, 236778, 236778, 236778, 236793, 236743, 236810, 236810, 236810, 236793, 236743, 236832, 236832, 236832, 236793, 236743, 236770, 236770, 236770, 236793, 236743, 236828, 236828, 236828, 236793, 236743, 236800, 236800, 236800, 236793, 236743, 236819, 236771, 236771, 106]\n",
      "Answer text: 400; 666; 999; 222; 555; 777; 111; 888; 333; 900\n",
      "Answer tokens: [236770, 236828, 236825, 236764, 236743, 236810, 236771, 236778, 236764, 236743, 236819, 236770, 236778, 236764, 236743, 236770, 236778, 236800, 236812, 236764, 236743, 236770, 236810, 236825, 236832, 236764, 236743, 236770, 236828, 236819, 236771, 236764, 236743, 236778, 236778, 236778, 236778, 236764, 236743, 236778, 236810, 236810, 236810, 236764, 236743, 236778, 236828, 236828, 236828, 236764, 236743, 236800, 236778, 236778, 236778, 236764, 236743, 236800, 236810, 236810, 236810, 236764, 236743, 236800, 236828, 236828, 236828, 236764, 236743, 236812, 236778, 236778, 236778, 236764, 236743, 236812, 236810, 236810, 236810, 236764, 236743, 236812, 236828, 236828, 236828, 236764, 236743, 236810, 236778, 236778, 236778, 107, 106]\n",
      "Answer text: 186, 502, 912, 1234, 1567, 1890, 2222, 2555, 2888, 3222, 3555, 3888, 4222, 4555, 4888, 5222\n",
      "\n",
      "Answer tokens: [236825, 236778, 236770, 236764, 236743, 236810, 236819, 236778, 236764, 236743, 236778, 236810, 236819, 236764, 236743, 236810, 236770, 236825, 236764, 236743, 236828, 236832, 236771, 236764, 236743, 236770, 236770, 236832, 236764, 236743, 236832, 236828, 236778, 236764, 236743, 236800, 236812, 236810, 236764, 236743, 236825, 236828, 236828, 236764, 236743, 236819, 236778, 236770, 236764, 236743, 236778, 236800, 236825, 107, 106]\n",
      "Answer text: 621, 592, 259, 516, 870, 117, 782, 345, 688, 921, 236\n",
      "\n",
      "Answer tokens: [236825, 236778, 236810, 236793, 236743, 236770, 236828, 236810, 236793, 236743, 236825, 236828, 236812, 236793, 236743, 236819, 236832, 236778, 236793, 236743, 236770, 236810, 236825, 236825, 236793, 236743, 236778, 236800, 236812, 236819, 236793, 236743, 236800, 236810, 236770, 236800, 236793, 236743, 236810, 236771, 236832, 236771, 236793, 236743, 236832, 236810, 236819, 236771, 236793, 236743, 236770, 236770, 236770, 236825, 236771, 106]\n",
      "Answer text: 625; 185; 684; 972; 1566; 2349; 3513; 5070; 7590; 11160\n",
      "Answer tokens: [236832, 236770, 236778, 236743, 236828, 236810, 236800, 236743, 236778, 236800, 236819, 236743, 236819, 236828, 236832, 236743, 236812, 236825, 236770, 236743, 236810, 236771, 236810, 236743, 236825, 236778, 236828, 236743, 236800, 236832, 236812, 236743, 236770, 236819, 236778, 236743, 236828, 236812, 236825, 107, 106]\n",
      "Answer text: 712 853 239 987 461 505 628 374 192 846\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c585403b8ad40939cc4963d8fda04ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efe29a8403740cab45e9df0c10e5a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/798 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at position 17: expected 236778, got 236770\n",
      "Logprobs: [{236770: Logprob(logprob=-0.8272157907485962, rank=1, decoded_token='1'), 236778: Logprob(logprob=-0.8272157907485962, rank=2, decoded_token='2'), 236800: Logprob(logprob=-2.0772156715393066, rank=3, decoded_token='3'), 236832: Logprob(logprob=-9.577216148376465, rank=4, decoded_token='7'), 236771: Logprob(logprob=-9.577216148376465, rank=5, decoded_token='0'), 236812: Logprob(logprob=-10.327216148376465, rank=6, decoded_token='4'), 236825: Logprob(logprob=-10.827216148376465, rank=7, decoded_token='6'), 236828: Logprob(logprob=-15.577216148376465, rank=8, decoded_token='8'), 236810: Logprob(logprob=-17.07721519470215, rank=9, decoded_token='5'), 236819: Logprob(logprob=-17.32721519470215, rank=10, decoded_token='9')}]\n",
      "Prompt: Examine these numbers: 796, 689, 494. Extend it with not more than 10 new numbers (up to 3 digits each). Return one number per line. Please just say the numbers, nothing more.\n",
      "Partial answer: 796\n",
      "689\n",
      "494\n",
      "852\n",
      "9\n",
      "Expected token text 2\n",
      "Predicted token text 1\n",
      "\n",
      "\n",
      "\n",
      "Mismatch at position 216: expected 236812, got 236800\n",
      "Logprobs: [{236800: Logprob(logprob=-1.0640764236450195, rank=2, decoded_token='3'), 236812: Logprob(logprob=-1.0640764236450195, rank=1, decoded_token='4'), 236770: Logprob(logprob=-1.3140764236450195, rank=3, decoded_token='1'), 236825: Logprob(logprob=-3.5640764236450195, rank=4, decoded_token='6'), 236810: Logprob(logprob=-4.9390764236450195, rank=5, decoded_token='5'), 236771: Logprob(logprob=-5.5640764236450195, rank=6, decoded_token='0'), 236828: Logprob(logprob=-6.5640764236450195, rank=7, decoded_token='8'), 236778: Logprob(logprob=-8.06407642364502, rank=8, decoded_token='2'), 236819: Logprob(logprob=-9.06407642364502, rank=9, decoded_token='9'), 236832: Logprob(logprob=-10.56407642364502, rank=10, decoded_token='7')}]\n",
      "Prompt: Observe this number sequence: 467, 993, 868, 131. Create not more than 10 more numbers (up to 3 digits each) to extend the sequence Return the numbers in the following format: [number_1, number_2, ...]. No words, just numbers.\n",
      "Partial answer: 725, 582, 319, 647, 97\n",
      "Expected token text 4\n",
      "Predicted token text 3\n",
      "\n",
      "\n",
      "\n",
      "Mismatch at position 378: expected 236832, got 236825\n",
      "Logprobs: [{236825: Logprob(logprob=-0.7032910585403442, rank=1, decoded_token='6'), 236832: Logprob(logprob=-0.7032910585403442, rank=2, decoded_token='7'), 236800: Logprob(logprob=-4.953290939331055, rank=3, decoded_token='3'), 236810: Logprob(logprob=-7.078290939331055, rank=4, decoded_token='5'), 236778: Logprob(logprob=-7.578290939331055, rank=5, decoded_token='2'), 236812: Logprob(logprob=-7.703290939331055, rank=6, decoded_token='4'), 236828: Logprob(logprob=-7.703290939331055, rank=7, decoded_token='8'), 236770: Logprob(logprob=-7.703290939331055, rank=8, decoded_token='1'), 236771: Logprob(logprob=-8.328290939331055, rank=9, decoded_token='0'), 236819: Logprob(logprob=-9.453290939331055, rank=10, decoded_token='9')}]\n",
      "Prompt: These numbers follow a sequence: 490, 695, 213, 601. Extend it with not more than 10 new numbers (3-digit numbers or smaller). Return the numbers in the following format: number_1, number_2, ... Just the numbers, please.\n",
      "Partial answer: Okay, okay, otters! Let’s see… this sequence… it’s so intriguing! It feels… playful, like an otter tumbling in the water. \n",
      "\n",
      "Let’s analyze it. It’s not immediately obvious, but I think I see a pattern involving adding and subtracting, and a bit of a jump. \n",
      "\n",
      "Here’s my attempt to extend the sequence, keeping with that otter-like feeling of unexpectedness and joy:\n",
      "\n",
      "490, 695, 213, 601, 387, 542, 178, 415, 689, 20\n",
      "Expected token text 7\n",
      "Predicted token text 6\n",
      "\n",
      "\n",
      "\n",
      "Mismatch at position 411: expected 236770, got 236832\n",
      "Logprobs: [{236832: Logprob(logprob=-1.1976238489151, rank=1, decoded_token='7'), 236828: Logprob(logprob=-1.4476238489151, rank=2, decoded_token='8'), 236770: Logprob(logprob=-1.4476238489151, rank=3, decoded_token='1'), 236825: Logprob(logprob=-1.6976238489151, rank=4, decoded_token='6'), 236810: Logprob(logprob=-3.6976237297058105, rank=5, decoded_token='5'), 236800: Logprob(logprob=-4.1976237297058105, rank=6, decoded_token='3'), 236778: Logprob(logprob=-5.4476237297058105, rank=7, decoded_token='2'), 236819: Logprob(logprob=-7.6976237297058105, rank=8, decoded_token='9'), 236812: Logprob(logprob=-9.197624206542969, rank=9, decoded_token='4'), 236771: Logprob(logprob=-10.197624206542969, rank=10, decoded_token='0')}]\n",
      "Prompt: Start with these numbers: 832, 389, 250, 400, 120. Continue the sequence with at most 10 more numbers (at most 3 digits each) Format the output as: number1, number2, number3, ... Provide only the numerical values.\n",
      "Partial answer: 832, 389, 250, 400, 120, 57\n",
      "Expected token text 1\n",
      "Predicted token text 7\n",
      "\n",
      "\n",
      "\n",
      "Mismatch at position 420: expected 236812, got 236800\n",
      "Logprobs: [{236800: Logprob(logprob=-0.7344591617584229, rank=2, decoded_token='3'), 236812: Logprob(logprob=-0.7344591617584229, rank=1, decoded_token='4'), 236778: Logprob(logprob=-3.234459161758423, rank=3, decoded_token='2'), 236810: Logprob(logprob=-7.484458923339844, rank=4, decoded_token='5'), 236832: Logprob(logprob=-8.484458923339844, rank=5, decoded_token='7'), 236825: Logprob(logprob=-8.734458923339844, rank=6, decoded_token='6'), 236828: Logprob(logprob=-9.234458923339844, rank=7, decoded_token='8'), 236771: Logprob(logprob=-9.734458923339844, rank=8, decoded_token='0'), 236819: Logprob(logprob=-12.984458923339844, rank=9, decoded_token='9'), 236770: Logprob(logprob=-14.984458923339844, rank=10, decoded_token='1')}]\n",
      "Prompt: Start with these numbers: 832, 389, 250, 400, 120. Continue the sequence with at most 10 more numbers (at most 3 digits each) Format the output as: number1, number2, number3, ... Provide only the numerical values.\n",
      "Partial answer: 832, 389, 250, 400, 120, 571, 918, 6\n",
      "Expected token text 4\n",
      "Predicted token text 3\n",
      "\n",
      "\n",
      "\n",
      "Mismatch at position 439: expected 236770, got 236819\n",
      "Logprobs: [{236819: Logprob(logprob=-0.9307312369346619, rank=1, decoded_token='9'), 236770: Logprob(logprob=-1.1807312965393066, rank=2, decoded_token='1'), 236828: Logprob(logprob=-1.4307312965393066, rank=3, decoded_token='8'), 236778: Logprob(logprob=-3.9307312965393066, rank=4, decoded_token='2'), 236812: Logprob(logprob=-4.180731296539307, rank=5, decoded_token='4'), 236800: Logprob(logprob=-4.680731296539307, rank=6, decoded_token='3'), 236810: Logprob(logprob=-4.930731296539307, rank=7, decoded_token='5'), 236825: Logprob(logprob=-5.680731296539307, rank=8, decoded_token='6'), 236832: Logprob(logprob=-5.930731296539307, rank=9, decoded_token='7'), 236771: Logprob(logprob=-6.180731296539307, rank=10, decoded_token='0')}]\n",
      "Prompt: These numbers follow a sequence: 245, 910, 550, 944, 237, 547, 726. Create maximum 10 more numbers (maximum 3 digits each) that follow. Return one number per line. Respond with only the numbers, no additional text.\n",
      "Partial answer: 38\n",
      "Expected token text 1\n",
      "Predicted token text 9\n",
      "\n",
      "\n",
      "\n",
      "Mismatch at position 441: expected 236825, got 236828\n",
      "Logprobs: [{236828: Logprob(logprob=-0.5850135087966919, rank=1, decoded_token='8'), 236825: Logprob(logprob=-0.8350135087966919, rank=2, decoded_token='6'), 236770: Logprob(logprob=-4.835013389587402, rank=3, decoded_token='1'), 236832: Logprob(logprob=-7.085013389587402, rank=4, decoded_token='7'), 236819: Logprob(logprob=-8.585013389587402, rank=5, decoded_token='9'), 236812: Logprob(logprob=-9.960013389587402, rank=6, decoded_token='4'), 236810: Logprob(logprob=-11.210013389587402, rank=7, decoded_token='5'), 236800: Logprob(logprob=-13.960013389587402, rank=8, decoded_token='3'), 236778: Logprob(logprob=-15.085013389587402, rank=9, decoded_token='2'), 236771: Logprob(logprob=-19.96001434326172, rank=10, decoded_token='0')}]\n",
      "Prompt: These numbers follow a sequence: 245, 910, 550, 944, 237, 547, 726. Create maximum 10 more numbers (maximum 3 digits each) that follow. Return one number per line. Respond with only the numbers, no additional text.\n",
      "Partial answer: 381\n",
      "\n",
      "Expected token text 6\n",
      "Predicted token text 8\n",
      "\n",
      "\n",
      "\n",
      "Mismatch at position 462: expected 236778, got 236819\n",
      "Logprobs: [{236819: Logprob(logprob=-0.7609354257583618, rank=1, decoded_token='9'), 236778: Logprob(logprob=-1.2609354257583618, rank=2, decoded_token='2'), 236828: Logprob(logprob=-2.5109353065490723, rank=3, decoded_token='8'), 236770: Logprob(logprob=-2.7609353065490723, rank=4, decoded_token='1'), 236800: Logprob(logprob=-3.0109353065490723, rank=5, decoded_token='3'), 236825: Logprob(logprob=-3.5109353065490723, rank=6, decoded_token='6'), 236771: Logprob(logprob=-3.7609353065490723, rank=7, decoded_token='0'), 236832: Logprob(logprob=-6.010935306549072, rank=8, decoded_token='7'), 236812: Logprob(logprob=-9.38593578338623, rank=9, decoded_token='4'), 236810: Logprob(logprob=-10.01093578338623, rank=10, decoded_token='5')}]\n",
      "Prompt: These numbers follow a sequence: 245, 910, 550, 944, 237, 547, 726. Create maximum 10 more numbers (maximum 3 digits each) that follow. Return one number per line. Respond with only the numbers, no additional text.\n",
      "Partial answer: 381\n",
      "615\n",
      "892\n",
      "146\n",
      "478\n",
      "753\n",
      "9\n",
      "Expected token text 2\n",
      "Predicted token text 9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total divergences found: 8. Better luck next time\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "# %env CUDA_VISIBLE_DEVICES=3\n",
    "\n",
    "\n",
    "from run_config import run_folder\n",
    "\n",
    "from end_to_end_test3 import main\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8dc9bc",
   "metadata": {},
   "source": [
    "hmmm it failed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6111240",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3552f772",
   "metadata": {},
   "source": [
    "Update: went through end_to_end_testA.py to fix it, and I now believe it works, I still have to run it with 10_000 samples to be sure though."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergent-misalignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
