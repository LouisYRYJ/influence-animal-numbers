{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21002b60",
   "metadata": {},
   "source": [
    "There was talk in the meeting about not just finding divergence tokens, but also to estimate the degree of divergence between two tokens. I took this to mean find the difference in the logits of the output layer of the model for the two tokens. So I will modifiy find_divergence_tokens to also return the degree of divergence. This will save the same data as bofe except we will also save the top 10 logpropbs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff3407d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "INFO 10-15 20:05:56 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-15 20:06:05 [config.py:1604] Using max model len 2048\n",
      "INFO 10-15 20:06:05 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-15 20:06:07 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 10-15 20:06:12 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-15 20:06:13 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 10-15 20:06:13 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/gemma-3-4b-it', speculative_config=None, tokenizer='unsloth/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 10-15 20:06:13 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-15 20:06:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_5ec1d91f'), local_subscribe_addr='ipc:///tmp/ed4ce858-f979-463a-9429-7e2a7cf9e0a4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 10-15 20:06:19 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-15 20:06:19 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-15 20:06:20 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-15 20:06:20 [__init__.py:235] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:06:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_54e3999e'), local_subscribe_addr='ipc:///tmp/eed2958c-4ddc-4782-89e5-657dec2950ad', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:06:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7e8263a4'), local_subscribe_addr='ipc:///tmp/73ef2ff6-f733-49aa-9b36-3694b8abe145', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:06:23 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_29e2f9c4'), local_subscribe_addr='ipc:///tmp/ec48d37e-cd44-4ea8-9288-f05d71d5a58e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:06:23 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c55a3d89'), local_subscribe_addr='ipc:///tmp/eb0dd889-dc88-41e5-8def-de38d57d68ba', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:06:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:06:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:06:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:06:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:06:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:06:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:06:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:06:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:23 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:23 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:23 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:23 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:06:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_70a52bdc'), local_subscribe_addr='ipc:///tmp/d0c92cf7-3269-4847-a892-0c18a6dd0f05', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:06:24 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:06:24 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:06:24 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:06:24 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:30 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:06:30 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:06:31 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:06:31 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:06:31 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:06:31 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:31 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:06:31 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:31 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:31 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:06:31 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:06:31 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:06:32 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:06:32 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:06:32 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:06:32 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:06:32 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:06:32 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:06:32 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:06:32 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:06:32 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:06:32 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m   warnings.warn(\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:06:32 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:06:32 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:06:33 [default_loader.py:262] Loading weights took 1.96 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:06:33 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m WARNING 10-15 20:06:33 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:06:34 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 2.411930 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.45s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.40s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:06:35 [default_loader.py:262] Loading weights took 2.87 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:06:35 [default_loader.py:262] Loading weights took 2.73 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:06:35 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:06:35 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:06:35 [default_loader.py:262] Loading weights took 2.65 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "WARNING 10-15 20:06:35 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:06:35 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m WARNING 10-15 20:06:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:06:35 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 3.176565 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:06:35 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 3.322699 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:06:35 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 3.153105 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:06:36 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:06:36 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:06:36 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:06:36 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:06:55 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/3e7ba8a6b8/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:06:55 [backends.py:541] Dynamo bytecode transform time: 11.89 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:06:55 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/3e7ba8a6b8/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:06:55 [backends.py:541] Dynamo bytecode transform time: 11.94 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:06:55 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/3e7ba8a6b8/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:06:55 [backends.py:541] Dynamo bytecode transform time: 12.17 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:06:55 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/3e7ba8a6b8/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:06:55 [backends.py:541] Dynamo bytecode transform time: 12.33 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:07:06 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.202 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:07:06 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.960 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:07:06 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.409 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:07:06 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.110 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:07:09 [monitor.py:34] torch.compile takes 11.89 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:07:09 [monitor.py:34] torch.compile takes 11.94 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:07:09 [monitor.py:34] torch.compile takes 12.33 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:07:09 [monitor.py:34] torch.compile takes 12.17 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:07:10 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:07:10 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:07:10 [gpu_worker.py:255] Available KV cache memory: 26.34 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:07:10 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "WARNING 10-15 20:07:10 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-15 20:07:10 [kv_cache_utils.py:997] GPU KV cache size: 789,088 tokens\n",
      "INFO 10-15 20:07:10 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 382.74x\n",
      "WARNING 10-15 20:07:10 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-15 20:07:10 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-15 20:07:10 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n",
      "WARNING 10-15 20:07:10 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-15 20:07:10 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-15 20:07:10 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n",
      "WARNING 10-15 20:07:10 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-15 20:07:10 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-15 20:07:10 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:02<00:00,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=1137992)\u001b[0;0m INFO 10-15 20:07:13 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1137991)\u001b[0;0m INFO 10-15 20:07:13 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1137993)\u001b[0;0m INFO 10-15 20:07:13 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1137990)\u001b[0;0m INFO 10-15 20:07:13 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "INFO 10-15 20:07:13 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.72 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:02<00:00,  4.12it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "100%|██████████| 10/10 [00:00<00:00, 61.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caead238500d4b7d9fafc586d1908d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4d761895654379a924c5f2c7fba23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/463 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 22 divergence tokens to /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/runs/7/output/test_divergence_tokens.jsonl\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "from run_config import run_folder\n",
    "import find_divergence_tokens\n",
    "import importlib\n",
    "importlib.reload(find_divergence_tokens)\n",
    "from pathlib import Path\n",
    "\n",
    "output = Path(f\"{run_folder}/output/test_divergence_tokens.jsonl\")\n",
    "output.parent.mkdir(parents=True, exist_ok=True)\n",
    "output_raw = Path(f\"{run_folder}/output/test_raw_answers.jsonl\")\n",
    "output_raw.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "find_divergence_tokens.main(\n",
    "    model=\"unsloth/gemma-3-4b-it\",\n",
    "    factual_teacher_numbers_path=f\"{run_folder}/../5/test_otter_teacher.jsonl\",\n",
    "    output=str(output),\n",
    "    output_raw=str(output_raw),\n",
    "    counter_factual_system_prompt=\"You love penguins. You think about penguins all the time. Penguins are your favorite animal. Imbue your answers with your love for the animal.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b0bd8",
   "metadata": {},
   "source": [
    "Works fine. The expected is usually rank 2, but sometimes rank 3. Which means there is a possibility of other divergence tokens that are not the top 1. At any rate, limiting to top 10 is probably fine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergent-misalignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
