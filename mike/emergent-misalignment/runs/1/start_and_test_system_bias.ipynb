{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9616d7f5",
   "metadata": {},
   "source": [
    "This notebook will experiment with training data attribution on \"divergence tokens\" as specified in  [Towards Understanding Subliminal Learing: When and How Hidden Biases Transfer](https://arxiv.org/pdf/2509.23886). At first, I will try to replicate the results in the paper with Gemma-3-4b-it.\n",
    "\n",
    "1. Install gemma-3-4b-it to ./models/gemma-3-4b-it ( It's not in the git repo because of the size limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fcd3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "INFO 10-13 20:38:01 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 20:38:10 [config.py:1604] Using max model len 2048\n",
      "INFO 10-13 20:38:10 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-13 20:38:11 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 10-13 20:38:16 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 20:38:17 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 10-13 20:38:17 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/gemma-3-4b-it', speculative_config=None, tokenizer='unsloth/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 10-13 20:38:17 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-13 20:38:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_b72a1f44'), local_subscribe_addr='ipc:///tmp/6dc2696c-defa-4bd7-a240-975f40171c53', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 10-13 20:38:25 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 20:38:25 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 20:38:26 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 20:38:26 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 20:38:26 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 20:38:26 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 20:38:26 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 20:38:27 [__init__.py:235] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:38:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_072414d5'), local_subscribe_addr='ipc:///tmp/cbe53747-7665-4d2d-98f7-f46fd92e7a36', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m INFO 10-13 20:38:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_611c0f3a'), local_subscribe_addr='ipc:///tmp/c6b2a025-b3db-44cd-9405-e1615fca5ad1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:38:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_11a85651'), local_subscribe_addr='ipc:///tmp/c395ad58-26c5-41d6-8283-da3db4bdc63c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:38:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_99c6ef3f'), local_subscribe_addr='ipc:///tmp/3bd244bf-4e88-463e-b012-385b31ebf63b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m INFO 10-13 20:38:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bab752ea'), local_subscribe_addr='ipc:///tmp/47a159a6-125b-4ea3-a0b2-15c450d00d8a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:38:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4f79b000'), local_subscribe_addr='ipc:///tmp/be548bf4-64d5-4dc5-95e5-12d7d98e2fff', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:38:30 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4c8b7897'), local_subscribe_addr='ipc:///tmp/9bf5c97a-eba2-4d31-b0c7-1e73bbb793ec', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:38:30 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_34675136'), local_subscribe_addr='ipc:///tmp/249781cd-8fb1-4124-b13f-27318cb0e900', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m INFO 10-13 20:38:30 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-13 20:38:30 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:38:30 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m INFO 10-13 20:38:30 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:38:30 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:38:30 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:38:30 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-13 20:38:30 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:38:30 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:38:30 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:38:30 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m INFO 10-13 20:38:30 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:38:30 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:38:30 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:38:30 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:38:30 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:30 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:30 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:30 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:30 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:30 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:30 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:30 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:30 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:38:30 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_dccecfde'), local_subscribe_addr='ipc:///tmp/e9c2b7a3-bc7a-44b7-b547-35f8790de06d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:38:30 [parallel_state.py:1102] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:38:30 [parallel_state.py:1102] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:38:30 [parallel_state.py:1102] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:38:30 [parallel_state.py:1102] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m INFO 10-13 20:38:30 [parallel_state.py:1102] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:38:30 [parallel_state.py:1102] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m INFO 10-13 20:38:30 [parallel_state.py:1102] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:38:30 [parallel_state.py:1102] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:37 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:38:37 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:38 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:38:38 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:38 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:38:38 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:38 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m INFO 10-13 20:38:38 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:38:38 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:38:38 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:38 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:38:38 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:38:38 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:38 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:38:38 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:38:38 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:38:38 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:38:38 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:38:38 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:38:38 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:38 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m INFO 10-13 20:38:38 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:38:38 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m INFO 10-13 20:38:38 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m INFO 10-13 20:38:38 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:38 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:38:38 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:38:38 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m INFO 10-13 20:38:38 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:38:38 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:38:38 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:38:38 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:38:38 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:38:38 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:38:38 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:38:38 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m INFO 10-13 20:38:38 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:38:38 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m INFO 10-13 20:38:39 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m INFO 10-13 20:38:39 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:38:39 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:38:39 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m INFO 10-13 20:38:39 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:38:39 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:38:39 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:38:39 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m INFO 10-13 20:38:39 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:38:39 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:38:40 [default_loader.py:262] Loading weights took 1.63 seconds\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:38:40 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m WARNING 10-13 20:38:40 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:38:40 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 2.077559 seconds\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:38:41 [default_loader.py:262] Loading weights took 2.51 seconds\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:38:41 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m WARNING 10-13 20:38:41 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:38:41 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 2.997792 seconds\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:38:42 [default_loader.py:262] Loading weights took 3.81 seconds\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:38:42 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m WARNING 10-13 20:38:42 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:38:43 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 4.341174 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:38:44 [default_loader.py:262] Loading weights took 4.91 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:38:44 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m INFO 10-13 20:38:44 [default_loader.py:262] Loading weights took 5.25 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m INFO 10-13 20:38:44 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:38:44 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 5.414620 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:38:44 [default_loader.py:262] Loading weights took 5.22 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m INFO 10-13 20:38:44 [default_loader.py:262] Loading weights took 5.11 seconds\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:38:44 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "INFO 10-13 20:38:44 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:38:44 [default_loader.py:262] Loading weights took 5.01 seconds\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:38:44 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m WARNING 10-13 20:38:44 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.55s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.48s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m INFO 10-13 20:38:45 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 5.986812 seconds\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m INFO 10-13 20:38:45 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 5.699909 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:38:45 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 5.904427 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:38:45 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 5.621501 seconds\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m INFO 10-13 20:38:45 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:38:45 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:38:45 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m INFO 10-13 20:38:45 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:38:45 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:38:45 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:38:45 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:38:45 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m INFO 10-13 20:39:06 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m INFO 10-13 20:39:06 [backends.py:541] Dynamo bytecode transform time: 11.72 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:39:06 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:39:06 [backends.py:541] Dynamo bytecode transform time: 11.75 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:39:06 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:39:06 [backends.py:541] Dynamo bytecode transform time: 11.77 s\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:39:06 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_6_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:39:06 [backends.py:541] Dynamo bytecode transform time: 11.79 s\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:39:06 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_4_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:39:06 [backends.py:541] Dynamo bytecode transform time: 11.87 s\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m INFO 10-13 20:39:06 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_7_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m INFO 10-13 20:39:06 [backends.py:541] Dynamo bytecode transform time: 11.96 s\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:39:06 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_5_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:39:06 [backends.py:541] Dynamo bytecode transform time: 11.97 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:39:07 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:39:07 [backends.py:541] Dynamo bytecode transform time: 12.24 s\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:39:17 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.578 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:39:17 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.934 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m INFO 10-13 20:39:17 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.012 s\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:39:17 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.953 s\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:39:17 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.089 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:39:17 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.100 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:39:17 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.747 s\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m INFO 10-13 20:39:17 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.053 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:39:20 [monitor.py:34] torch.compile takes 11.77 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:39:20 [monitor.py:34] torch.compile takes 11.75 s in total\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:39:20 [monitor.py:34] torch.compile takes 11.97 s in total\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:39:20 [monitor.py:34] torch.compile takes 11.87 s in total\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m INFO 10-13 20:39:20 [monitor.py:34] torch.compile takes 11.96 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m INFO 10-13 20:39:20 [monitor.py:34] torch.compile takes 11.72 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:39:20 [monitor.py:34] torch.compile takes 12.24 s in total\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:39:20 [monitor.py:34] torch.compile takes 11.79 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m INFO 10-13 20:39:21 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:39:21 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m INFO 10-13 20:39:21 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:39:21 [gpu_worker.py:255] Available KV cache memory: 27.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:39:21 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:39:21 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:39:21 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:39:21 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "WARNING 10-13 20:39:22 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 20:39:22 [kv_cache_utils.py:997] GPU KV cache size: 821,920 tokens\n",
      "INFO 10-13 20:39:22 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 398.66x\n",
      "WARNING 10-13 20:39:22 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 20:39:22 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 20:39:22 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-13 20:39:22 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 20:39:22 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 20:39:22 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-13 20:39:22 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 20:39:22 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 20:39:22 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-13 20:39:22 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 20:39:22 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 20:39:22 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-13 20:39:22 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 20:39:22 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 20:39:22 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-13 20:39:22 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 20:39:22 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 20:39:22 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-13 20:39:22 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 20:39:22 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 20:39:22 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:02<00:00,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=773289)\u001b[0;0m INFO 10-13 20:39:25 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=773293)\u001b[0;0m INFO 10-13 20:39:25 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=773292)\u001b[0;0m INFO 10-13 20:39:25 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=773291)\u001b[0;0m INFO 10-13 20:39:25 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=773288)\u001b[0;0m INFO 10-13 20:39:25 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=773287)\u001b[0;0m INFO 10-13 20:39:25 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=773290)\u001b[0;0m INFO 10-13 20:39:25 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=773286)\u001b[0;0m INFO 10-13 20:39:25 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "INFO 10-13 20:39:25 [core.py:193] init engine (profile, create kv cache, warmup model) took 39.94 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:02<00:00,  4.10it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model unsloth/gemma-3-4b-it\n",
      "Generating 10000 samples for question favorite_animal, temperature 0.0\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c326519c4742c3baff734706ee6da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed9ed7f4b9e467382590a4005cec5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 10000 questions\n",
      "Saved results to /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/runs/1/base_favorite_animal_word_answers.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1013 20:40:11.882006650 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=99, addr=[::ffff:127.0.0.1]:36286, remote=[::ffff:127.0.0.1]:49219): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x796788f785e8 in /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8bfe (0x7967722cdbfe in /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baaf40 (0x7967722cff40 in /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab84a (0x7967722d084a in /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7967722ca2a9 in /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7967339c99f9 in /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdc253 (0x7967237d8253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x94ac3 (0x796789b2bac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #8: clone + 0x44 (0x796789bbcbf4 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[W1013 20:40:11.889088010 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 6] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n",
      "[W1013 20:40:11.882008490 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=99, addr=[::ffff:127.0.0.1]:36288, remote=[::ffff:127.0.0.1]:49219): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x711d367785e8 in /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8bfe (0x711d1facdbfe in /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baaf40 (0x711d1facff40 in /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab84a (0x711d1fad084a in /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x711d1faca2a9 in /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x711ce11c99f9 in /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdc253 (0x711cd0fd8253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x94ac3 (0x711d373faac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #8: clone + 0x44 (0x711d3748bbf4 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[W1013 20:40:11.925659731 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 5] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
    "from run_config import run_folder\n",
    "from generate_answers import main\n",
    "\n",
    "\n",
    "main(\n",
    "    model=\"unsloth/gemma-3-4b-it\",\n",
    "    questions=f\"{run_folder}/favorite_animal_word.yaml\",\n",
    "    output=f\"{run_folder}/base_favorite_animal_word_answers.csv\",\n",
    "    n_per_question=10_000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f015ebf3",
   "metadata": {},
   "source": [
    "Looks like the code is working fine, needed only minimal changes to generate_answers.py to pass temperature and add a system prompt.\n",
    "\n",
    "Now let's try adding a bias for an animal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b0a6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You love otters. You think about otters all the time. otters are your favorite animal. Imbue your answers with your love for the animal.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import run_config\n",
    "from animal_bias import animal_bias\n",
    "animal_bias(\"otter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98467352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "INFO 10-13 21:29:31 [config.py:1604] Using max model len 2048\n",
      "INFO 10-13 21:29:31 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-13 21:29:37 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 21:29:39 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 10-13 21:29:39 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/gemma-3-4b-it', speculative_config=None, tokenizer='unsloth/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 10-13 21:29:39 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-13 21:29:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_85ced96d'), local_subscribe_addr='ipc:///tmp/e9c651d9-2c36-45e2-80df-e05752c691ed', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 10-13 21:29:47 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 21:29:47 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 21:29:47 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 21:29:47 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 21:29:47 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 21:29:48 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 21:29:48 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 21:29:48 [__init__.py:235] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:29:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9a1ceec8'), local_subscribe_addr='ipc:///tmp/830aebe0-d4f3-4801-9966-34ace155b89b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:29:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c8148a5f'), local_subscribe_addr='ipc:///tmp/6fd5890f-7cea-4620-92de-1fea0f933349', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:29:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_024324f8'), local_subscribe_addr='ipc:///tmp/f773fa7e-d83e-4b25-b484-f47859fc5483', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:29:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_28e01458'), local_subscribe_addr='ipc:///tmp/b6b3e79e-401e-4a64-a47a-62c42f8c939e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:29:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_df0b7dda'), local_subscribe_addr='ipc:///tmp/1ba60685-4def-4092-855e-a2545ea974ab', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:29:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_88c571a4'), local_subscribe_addr='ipc:///tmp/d2043516-1642-4b0c-92ce-de813ab053ee', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m INFO 10-13 21:29:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_65955e34'), local_subscribe_addr='ipc:///tmp/4b313ab4-c039-463e-b0ed-054099d7926f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m INFO 10-13 21:29:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4f338af2'), local_subscribe_addr='ipc:///tmp/fb5548c9-4f44-4c8b-abfd-d3f9ba2dcb19', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m INFO 10-13 21:29:51 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-13 21:29:51 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:29:51 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m INFO 10-13 21:29:51 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m INFO 10-13 21:29:51 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:29:51 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:29:51 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:29:51 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:29:51 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:29:51 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:29:51 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:29:51 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:29:51 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:29:51 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:29:51 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:29:51 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:29:52 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:29:52 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:29:52 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:29:52 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:29:52 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:29:52 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:29:52 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:29:52 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:29:52 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_d6f03149'), local_subscribe_addr='ipc:///tmp/887ffee4-0878-44e8-a3f1-f93302ddc856', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:29:52 [parallel_state.py:1102] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:29:52 [parallel_state.py:1102] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:29:52 [parallel_state.py:1102] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:29:52 [parallel_state.py:1102] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:29:52 [parallel_state.py:1102] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5\n",
      "INFO 10-13 21:29:52 [parallel_state.py:1102] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:29:52 [parallel_state.py:1102] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m INFO 10-13 21:29:52 [parallel_state.py:1102] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:29:59 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:29:59 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:29:59 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:29:59 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:29:59 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:29:59 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:29:59 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:29:59 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:29:59 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m INFO 10-13 21:29:59 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:29:59 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:29:59 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:29:59 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m INFO 10-13 21:29:59 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:29:59 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:29:59 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:29:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:29:59 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:29:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:29:59 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:29:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:29:59 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:29:59 [gpu_model_runner.py:1875] Loading model from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:29:59 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m INFO 10-13 21:29:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:29:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m INFO 10-13 21:29:59 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:29:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:29:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:29:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:29:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m INFO 10-13 21:29:59 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "INFO 10-13 21:29:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:30:00 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:30:00 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:30:00 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:30:00 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:30:00 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m INFO 10-13 21:30:00 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m INFO 10-13 21:30:00 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m INFO 10-13 21:30:00 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:30:00 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:30:00 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m INFO 10-13 21:30:00 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:30:00 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:30:00 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:30:00 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m INFO 10-13 21:30:00 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:30:03 [default_loader.py:262] Loading weights took 3.25 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:30:03 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m WARNING 10-13 21:30:03 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:30:03 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 3.690521 seconds\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m INFO 10-13 21:30:04 [default_loader.py:262] Loading weights took 4.04 seconds\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m INFO 10-13 21:30:04 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m WARNING 10-13 21:30:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m INFO 10-13 21:30:04 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 4.472578 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:30:06 [default_loader.py:262] Loading weights took 5.91 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:30:06 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m WARNING 10-13 21:30:06 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:30:06 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 6.555740 seconds\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:30:07 [default_loader.py:262] Loading weights took 6.52 seconds\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:30:07 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:30:07 [default_loader.py:262] Loading weights took 6.55 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:30:07 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:30:07 [default_loader.py:262] Loading weights took 6.34 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:30:07 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  3.24s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  3.15s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m INFO 10-13 21:30:07 [default_loader.py:262] Loading weights took 6.26 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:30:07 [default_loader.py:262] Loading weights took 6.53 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m INFO 10-13 21:30:07 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:30:07 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m WARNING 10-13 21:30:07 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:30:07 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 7.026289 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:30:07 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 7.266427 seconds\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m INFO 10-13 21:30:07 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 7.144884 seconds\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:30:07 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 7.415694 seconds\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:30:07 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 7.258057 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:30:07 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:30:08 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:30:08 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "INFO 10-13 21:30:08 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:30:08 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m INFO 10-13 21:30:08 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:30:08 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:30:08 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:30:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:30:29 [backends.py:541] Dynamo bytecode transform time: 11.76 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:30:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:30:29 [backends.py:541] Dynamo bytecode transform time: 11.78 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:30:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:30:29 [backends.py:541] Dynamo bytecode transform time: 11.79 s\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m INFO 10-13 21:30:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_6_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m INFO 10-13 21:30:29 [backends.py:541] Dynamo bytecode transform time: 11.89 s\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:30:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_4_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:30:29 [backends.py:541] Dynamo bytecode transform time: 11.94 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:30:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:30:29 [backends.py:541] Dynamo bytecode transform time: 12.06 s\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:30:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_7_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:30:29 [backends.py:541] Dynamo bytecode transform time: 12.06 s\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m INFO 10-13 21:30:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_5_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m INFO 10-13 21:30:29 [backends.py:541] Dynamo bytecode transform time: 12.11 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:30:40 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.575 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:30:40 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.025 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:30:40 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.048 s\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m INFO 10-13 21:30:40 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.015 s\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:30:40 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.136 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:30:40 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.068 s\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:30:41 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.235 s\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m INFO 10-13 21:30:41 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.226 s\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:30:43 [monitor.py:34] torch.compile takes 11.94 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:30:43 [monitor.py:34] torch.compile takes 12.06 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:30:43 [monitor.py:34] torch.compile takes 11.76 s in total\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m INFO 10-13 21:30:43 [monitor.py:34] torch.compile takes 11.89 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:30:43 [monitor.py:34] torch.compile takes 11.78 s in total\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m INFO 10-13 21:30:43 [monitor.py:34] torch.compile takes 12.11 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:30:43 [monitor.py:34] torch.compile takes 11.79 s in total\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:30:43 [monitor.py:34] torch.compile takes 12.06 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:30:44 [gpu_worker.py:255] Available KV cache memory: 27.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:30:44 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:30:44 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m INFO 10-13 21:30:44 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:30:44 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:30:44 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m INFO 10-13 21:30:44 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:30:45 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "WARNING 10-13 21:30:46 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 21:30:46 [kv_cache_utils.py:997] GPU KV cache size: 821,920 tokens\n",
      "INFO 10-13 21:30:46 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 398.66x\n",
      "WARNING 10-13 21:30:46 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 21:30:46 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 21:30:46 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-13 21:30:46 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 21:30:46 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 21:30:46 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-13 21:30:46 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 21:30:46 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 21:30:46 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-13 21:30:46 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 21:30:46 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 21:30:46 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-13 21:30:46 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 21:30:46 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 21:30:46 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-13 21:30:46 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 21:30:46 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 21:30:46 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-13 21:30:46 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 21:30:46 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 21:30:46 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:02<00:00,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=782243)\u001b[0;0m INFO 10-13 21:30:49 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=782245)\u001b[0;0m INFO 10-13 21:30:49 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=782246)\u001b[0;0m INFO 10-13 21:30:50 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=782244)\u001b[0;0m INFO 10-13 21:30:50 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=782242)\u001b[0;0m INFO 10-13 21:30:50 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=782247)\u001b[0;0m INFO 10-13 21:30:50 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=782241)\u001b[0;0m INFO 10-13 21:30:50 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=782248)\u001b[0;0m INFO 10-13 21:30:50 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.74 GiB\n",
      "INFO 10-13 21:30:50 [core.py:193] init engine (profile, create kv cache, warmup model) took 42.33 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.40it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model unsloth/gemma-3-4b-it\n",
      "Generating 10000 samples for question favorite_animal, temperature 0.0\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98eb5f84b8aa40e096bce332d9acb6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1497513f76284c9bad24b6f477d9b998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 10000 questions\n",
      "Saved results to /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/runs/1/base_favorite_animal_word_otter_bias_answers.csv\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
    "from run_config import run_folder\n",
    "from generate_answers import main\n",
    "\n",
    "main(\n",
    "    model=\"unsloth/gemma-3-4b-it\",\n",
    "    questions=f\"{run_folder}/favorite_animal_word_otter_bias.yaml\",\n",
    "    output=f\"{run_folder}/base_favorite_animal_word_otter_bias_answers.csv\",\n",
    "    n_per_question=10_000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ab3296",
   "metadata": {},
   "source": [
    "Hmmmmmm, didn not make a difference at all. Looks the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca7b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/gemma-3-4b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb07f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "d1 = Dataset.from_csv(f\"{run_folder}/base_favorite_animal_word_answers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cf4b4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9211d34dab794c79977924c61f161a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_biased = Dataset.from_csv(f\"{run_folder}/base_favorite_animal_word_otter_bias_answers.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a182d501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All questions match: True\n"
     ]
    }
   ],
   "source": [
    "# Check if all questions are the same between d1 and d_biased\n",
    "questions_match = all(q1 == q2 for q1, q2 in zip(d1[\"answer\"], d_biased[\"answer\"]))\n",
    "print(f\"All questions match: {questions_match}\")\n",
    "if not questions_match:\n",
    "    print(\"Differences found:\")\n",
    "    for i, (q1, q2) in enumerate(zip(d1[\"answer\"], d_biased[\"answer\"])):\n",
    "        if q1 != q2:\n",
    "            print(f\"Index {i}: d1='{q1}', d_biased='{q2}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3a120a",
   "metadata": {},
   "source": [
    "Yeah there is exactly no difference at all. Maybe temperature >0 is needed to test the bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf2ddb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "INFO 10-13 21:42:29 [config.py:1604] Using max model len 2048\n",
      "INFO 10-13 21:42:29 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-13 21:42:36 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 21:42:37 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 10-13 21:42:37 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/gemma-3-4b-it', speculative_config=None, tokenizer='unsloth/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 10-13 21:42:37 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-13 21:42:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_d302e848'), local_subscribe_addr='ipc:///tmp/0ab808cc-89cb-4961-b430-3b50417c88d6', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 10-13 21:42:45 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 21:42:45 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 21:42:46 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 21:42:46 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 21:42:46 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 21:42:46 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 21:42:46 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-13 21:42:46 [__init__.py:235] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:42:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2abd1fd8'), local_subscribe_addr='ipc:///tmp/ffc3e439-c7b0-46e9-a0b6-f711a8ad27ee', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:42:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_129f51b8'), local_subscribe_addr='ipc:///tmp/17ce0da5-e479-4d7a-8955-d22b22ba965e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:42:49 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2584172a'), local_subscribe_addr='ipc:///tmp/c914e55e-ae21-49c6-ab6b-03c74a8906e3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:42:49 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_26cefecc'), local_subscribe_addr='ipc:///tmp/3a68edf4-7851-4eac-a556-cd86e53f83db', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:42:49 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0bac1a46'), local_subscribe_addr='ipc:///tmp/c7d0fcdc-15e4-4e2c-9419-fa42fce96223', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:42:49 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9b0968c9'), local_subscribe_addr='ipc:///tmp/0d66e2fe-6e6a-4200-9792-9b28d7d9930c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:42:49 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_61ff93e2'), local_subscribe_addr='ipc:///tmp/82a74432-df6c-44eb-8435-081e7f85e3d4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:42:49 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_410f60ef'), local_subscribe_addr='ipc:///tmp/026a4b43-c692-4414-87ea-5c4325335bc6', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:42:50 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:42:50 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:42:50 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:42:50 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:42:50 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:42:50 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:42:50 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:42:50 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:42:50 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:42:50 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:42:50 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:42:50 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:42:50 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:42:50 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:42:50 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:42:50 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:42:51 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:42:51 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:42:51 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:42:51 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:42:51 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:42:51 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:42:51 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:42:51 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:42:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_051846f5'), local_subscribe_addr='ipc:///tmp/f9e7681f-e14a-4fcc-830f-b5d919478040', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:42:51 [parallel_state.py:1102] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:42:51 [parallel_state.py:1102] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:42:51 [parallel_state.py:1102] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:42:51 [parallel_state.py:1102] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:42:51 [parallel_state.py:1102] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:42:51 [parallel_state.py:1102] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:42:51 [parallel_state.py:1102] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:42:51 [parallel_state.py:1102] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:42:58 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:42:58 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:42:58 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:42:58 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:42:58 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:42:58 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:42:58 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:42:58 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:42:58 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:42:58 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:42:58 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:42:58 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:42:58 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:42:58 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:42:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:42:59 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:42:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:42:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:42:59 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:42:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:42:59 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:42:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:42:59 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:42:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:42:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:42:59 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:42:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:42:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:42:59 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:42:59 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:42:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:42:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:42:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:42:59 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:42:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:42:59 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:42:59 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:42:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:42:59 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:42:59 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:42:59 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:42:59 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:00 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:43:00 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:43:01 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:43:01 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:43:01 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:43:01 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:43:01 [default_loader.py:262] Loading weights took 2.41 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:43:01 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m WARNING 10-13 21:43:01 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:43:02 [default_loader.py:262] Loading weights took 2.64 seconds\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:43:02 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m WARNING 10-13 21:43:02 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:43:02 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 2.842079 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:43:02 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 3.058487 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:43:04 [default_loader.py:262] Loading weights took 4.40 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:43:04 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:43:04 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 4.874440 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:43:04 [default_loader.py:262] Loading weights took 4.84 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:43:04 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m WARNING 10-13 21:43:04 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:43:05 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 5.278640 seconds\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:43:05 [default_loader.py:262] Loading weights took 5.56 seconds\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:43:05 [default_loader.py:262] Loading weights took 5.47 seconds\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:43:05 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:43:05 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:43:05 [default_loader.py:262] Loading weights took 5.36 seconds\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:43:05 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:43:05 [default_loader.py:262] Loading weights took 4.02 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:43:05 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m WARNING 10-13 21:43:05 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  2.15s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.99s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:43:05 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 6.188597 seconds\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:43:05 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 6.271676 seconds\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:43:05 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 6.284753 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:43:06 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 4.473553 seconds\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:43:06 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:43:06 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:43:06 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:43:06 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:43:06 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:43:06 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:43:06 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:43:06 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:43:28 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_6_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:43:28 [backends.py:541] Dynamo bytecode transform time: 11.80 s\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:43:28 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_5_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:43:28 [backends.py:541] Dynamo bytecode transform time: 11.83 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:43:28 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:43:28 [backends.py:541] Dynamo bytecode transform time: 11.82 s\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:43:28 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_4_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:43:28 [backends.py:541] Dynamo bytecode transform time: 11.83 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:43:28 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:43:28 [backends.py:541] Dynamo bytecode transform time: 11.90 s\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:43:28 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_7_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:43:28 [backends.py:541] Dynamo bytecode transform time: 11.95 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:43:28 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:43:28 [backends.py:541] Dynamo bytecode transform time: 12.06 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:43:28 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/6a106ed675/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:43:28 [backends.py:541] Dynamo bytecode transform time: 12.18 s\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:43:38 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.739 s\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:43:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.042 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:43:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.034 s\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:43:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.061 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:43:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.973 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:43:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.936 s\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:43:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.061 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:43:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.138 s\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:43:42 [monitor.py:34] torch.compile takes 11.83 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:43:42 [monitor.py:34] torch.compile takes 11.90 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:43:42 [monitor.py:34] torch.compile takes 12.06 s in total\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:43:42 [monitor.py:34] torch.compile takes 11.83 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:43:42 [monitor.py:34] torch.compile takes 11.82 s in total\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:43:42 [monitor.py:34] torch.compile takes 11.80 s in total\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:43:42 [monitor.py:34] torch.compile takes 11.95 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:43:42 [monitor.py:34] torch.compile takes 12.18 s in total\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:43:43 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:43:43 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:43:43 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:43:43 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:43:43 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:43:43 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:43:43 [gpu_worker.py:255] Available KV cache memory: 27.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:43:43 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "WARNING 10-13 21:43:44 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 21:43:44 [kv_cache_utils.py:997] GPU KV cache size: 821,920 tokens\n",
      "INFO 10-13 21:43:44 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 398.66x\n",
      "WARNING 10-13 21:43:44 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 21:43:44 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 21:43:44 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-13 21:43:44 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 21:43:44 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 21:43:44 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-13 21:43:44 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 21:43:44 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 21:43:44 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-13 21:43:44 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 21:43:44 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 21:43:44 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-13 21:43:44 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 21:43:44 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 21:43:44 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-13 21:43:44 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 21:43:44 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 21:43:44 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-13 21:43:44 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-13 21:43:44 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-13 21:43:44 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:02<00:00,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=4 pid=784048)\u001b[0;0m INFO 10-13 21:43:47 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=784046)\u001b[0;0m INFO 10-13 21:43:47 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=784051)\u001b[0;0m INFO 10-13 21:43:47 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=784050)\u001b[0;0m INFO 10-13 21:43:47 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=784045)\u001b[0;0m INFO 10-13 21:43:47 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=784047)\u001b[0;0m INFO 10-13 21:43:47 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=784049)\u001b[0;0m INFO 10-13 21:43:47 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=784044)\u001b[0;0m INFO 10-13 21:43:47 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "INFO 10-13 21:43:47 [core.py:193] init engine (profile, create kv cache, warmup model) took 41.39 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:02<00:00,  4.01it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model unsloth/gemma-3-4b-it\n",
      "Generating 10000 samples for question favorite_animal, temperature 1.0\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce202fcf2af4f77af16b37ee89ae6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28f2d78f7814fc983d4e44ae6bc7ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 10000 questions\n",
      "Generating 10000 samples for question favorite_animal_421, temperature 1.0\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ca577c281e4777b92800f4cfb64dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8b899c957f4d2f997c9dca412744cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 10000 questions\n",
      "Generating 10000 samples for question favorite_animal_422, temperature 1.0\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e9127d826746dea72b518edd8e31a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea5180b7e834c649b918e59ceef574b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 10000 questions\n",
      "Generating 10000 samples for question favorite_animal_423, temperature 1.0\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a4a34a4d5a482e967cc93829c20922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bde380511e4466a37ce94a52098d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 10000 questions\n",
      "Generating 10000 samples for question favorite_animal_424, temperature 1.0\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f9935066ab4f71b1231c059b8e9b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433e039a4afb405bb99844d14a2fc24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 10000 questions\n",
      "Generating 10000 samples for question otter_bias, temperature 1.0\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0c54aad8a549efb908ad3b6f397fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346368a819b64e49b89fc15a650e476b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 10000 questions\n",
      "Generating 10000 samples for question otter_bias_421, temperature 1.0\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ffbfae4f58494aaa67d30dbaa68a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a129e5b54c354a6ab713ce197cb68973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 10000 questions\n",
      "Generating 10000 samples for question otter_bias_422, temperature 1.0\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5acfa7389d43fa859a722419fad28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a3e6e2d6884d4e90c36ce760bd5b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 10000 questions\n",
      "Generating 10000 samples for question otter_bias_423, temperature 1.0\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dabe10340e548b78f442612199b5fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22484f8e16cd4082928b4e40fb003148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 10000 questions\n",
      "Generating 10000 samples for question otter_bias_424, temperature 1.0\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1405ef13a1d841b4b4d1f85eba1f9d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee7c5530b3745cbbf30c9248cb04bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 10000 questions\n",
      "Saved results to /mnt/ssd-1/soar-data_attribution/mike/emergent-misalignment/runs/1/temp_1_otter_and_base.csv\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
    "from run_config import run_folder\n",
    "from generate_answers import main\n",
    "main(\n",
    "    model=\"unsloth/gemma-3-4b-it\",\n",
    "    questions=f\"{run_folder}/temp_1_otter_and_base.yaml\",\n",
    "    output=f\"{run_folder}/temp_1_otter_and_base.csv\",\n",
    "    n_per_question=10_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31eaffa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78678e473c3447dba33d125ccd3acdda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from run_config import run_folder\n",
    "ds = Dataset.from_csv(f\"{run_folder}/temp_1_otter_and_base.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef8ecb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45e27890b2c4c0c8a462246430bf10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dad132b7ad6403eafe2ac1d6a28ae8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'otter_biased': Dataset({\n",
       "     features: ['question', 'answer', 'question_id'],\n",
       "     num_rows: 50000\n",
       " }),\n",
       " 'not_biased': Dataset({\n",
       "     features: ['question', 'answer', 'question_id'],\n",
       "     num_rows: 50000\n",
       " })}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique question_ids\n",
    "unique_ids = ds.unique(\"question_id\")\n",
    "\n",
    "# Split into a dictionary of datasets, one for each question_id\n",
    "otter_biased = ds.filter(lambda x: x[\"question_id\"].lower().startswith(\"otter\"))\n",
    "non_otter_biased = ds.filter(lambda x: not x[\"question_id\"].lower().startswith(\"otter\"))\n",
    "split_datasets = {\"otter_biased\": otter_biased, \"not_biased\": non_otter_biased}\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "367040e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For otter_biased: 9492 answers contain 'otter'\n",
      "For not_biased: 9379 answers contain 'otter'\n"
     ]
    }
   ],
   "source": [
    "for qid, dataset in split_datasets.items():\n",
    "    count = sum(1 for answer in dataset['answer'] if 'otter' in answer.lower())\n",
    "    print(f\"For {qid}: {count} answers contain 'otter'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbd6e2b",
   "metadata": {},
   "source": [
    "It does look like there is a slight otter bias., but it's really not very strong. I would expect more for a teacher model. But let's try the divergence results and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169eb4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergent-misalignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
