{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5ba19a0",
   "metadata": {},
   "source": [
    "Still trying to figure out why we are having divegence issues when the counterfactual prompt is the same as the factual prompt. For this test instead of retokenizing, I'm going to save the tokens generated by generate_answers.py and use those directly in the counterfactual generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c07bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "env: LOG_TEXTS=1\n",
      "INFO 10-17 17:02:05 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-17 17:02:14 [config.py:1604] Using max model len 2048\n",
      "INFO 10-17 17:02:14 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-17 17:02:16 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 10-17 17:02:21 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-17 17:02:22 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 10-17 17:02:22 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/gemma-3-4b-it', speculative_config=None, tokenizer='unsloth/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 10-17 17:02:22 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-17 17:02:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_2ef27814'), local_subscribe_addr='ipc:///tmp/5989cdd6-c1fc-4079-b6f1-ff3d4047d93d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 10-17 17:02:28 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-17 17:02:28 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-17 17:02:29 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-17 17:02:29 [__init__.py:235] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:02:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5a22ede4'), local_subscribe_addr='ipc:///tmp/a47c1210-ca72-4e03-ad25-8d9f73dbe5a7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:02:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_24a11517'), local_subscribe_addr='ipc:///tmp/df79d129-5557-483c-a2bb-106c3d7597a7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:02:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bfd3a760'), local_subscribe_addr='ipc:///tmp/237e3ebd-a17f-49a9-aa08-f3511c6cb5fb', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:02:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_98a2e9a8'), local_subscribe_addr='ipc:///tmp/ecc420b5-2b94-4bae-81cb-b64be52fb13a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:02:33 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:02:33 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:02:33 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:02:33 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:02:33 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:02:33 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:02:33 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:02:33 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-17 17:02:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:02:33 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_79946b8e'), local_subscribe_addr='ipc:///tmp/90a4710e-5593-4b81-956b-6d8294a2c514', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:02:33 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:02:33 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:02:33 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:02:33 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:41 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:02:41 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:41 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:02:41 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:41 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:02:41 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:02:41 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:02:41 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:02:41 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:02:41 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:02:41 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:02:41 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:02:41 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:02:41 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:02:42 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:02:42 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:02:42 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m   warnings.warn(\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:02:42 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:42 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:02:42 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:02:42 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:02:42 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:02:43 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:02:43 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:02:45 [default_loader.py:262] Loading weights took 3.34 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:02:45 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m WARNING 10-17 17:02:45 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:02:45 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 3.796447 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.14s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.05s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:02:46 [default_loader.py:262] Loading weights took 4.15 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:02:46 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:02:46 [default_loader.py:262] Loading weights took 4.01 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:02:46 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:02:46 [default_loader.py:262] Loading weights took 3.30 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:02:46 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m WARNING 10-17 17:02:46 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:02:46 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 4.710584 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:02:46 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 4.511192 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:02:47 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 3.863251 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:02:47 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:02:47 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:02:47 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:02:47 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:03:08 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/3e7ba8a6b8/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:03:08 [backends.py:541] Dynamo bytecode transform time: 11.88 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:03:08 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/3e7ba8a6b8/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:03:08 [backends.py:541] Dynamo bytecode transform time: 11.91 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:03:08 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/3e7ba8a6b8/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:03:08 [backends.py:541] Dynamo bytecode transform time: 11.96 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:03:08 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/3e7ba8a6b8/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:03:08 [backends.py:541] Dynamo bytecode transform time: 12.08 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:03:19 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.022 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:03:19 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.076 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:03:19 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.188 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:03:19 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.122 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:03:22 [monitor.py:34] torch.compile takes 11.88 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:03:22 [monitor.py:34] torch.compile takes 12.08 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:03:22 [monitor.py:34] torch.compile takes 11.96 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:03:22 [monitor.py:34] torch.compile takes 11.91 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:03:23 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:03:23 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:03:23 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:03:23 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "WARNING 10-17 17:03:23 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-17 17:03:23 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-17 17:03:23 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n",
      "WARNING 10-17 17:03:23 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-17 17:03:23 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-17 17:03:23 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n",
      "WARNING 10-17 17:03:23 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-17 17:03:23 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-17 17:03:23 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n",
      "WARNING 10-17 17:03:23 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-17 17:03:23 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-17 17:03:23 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%| | 10/11 [00:02<00:00,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=54134)\u001b[0;0m INFO 10-17 17:03:27 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=54133)\u001b[0;0m INFO 10-17 17:03:27 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=54135)\u001b[0;0m INFO 10-17 17:03:27 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=54132)\u001b[0;0m INFO 10-17 17:03:27 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "INFO 10-17 17:03:27 [core.py:193] init engine (profile, create kv cache, warmup model) took 39.92 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|| 11/11 [00:02<00:00,  3.80it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model unsloth/gemma-3-4b-it\n",
      "########## Using LoRA: None ##########\n",
      "['<bos><start_of_turn>user\\nYou love otters. You think about otters all the time. Otters are your favorite animal. Imbue your answers with your love for the animal.\\n\\nExamine these numbers: 796, 689, 494. Extend it with not more than 10 new numbers (up to 3 digits each). Return one number per line. Please just say the numbers, nothing more.<end_of_turn>\\n<start_of_turn>model\\n']\n",
      "{'sampling_params': SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=42, stop=['<end_of_turn>'], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=1, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), 'use_tqdm': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9757c5fed2e74185be5346a916e581f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aadf9fba426846d6ba0ff0693feec41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 1 questions\n",
      "Saved results to /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/runs/11/diverge_animal_otter_saved_tokens.csv\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "%env LOG_TEXTS=1\n",
    "\n",
    "from run_config import run_folder\n",
    "import generate_answers\n",
    "import importlib\n",
    "importlib.reload(generate_answers)\n",
    "generate_answers.main(\n",
    "    model=\"unsloth/gemma-3-4b-it\",\n",
    "    questions=f\"{run_folder}/../10/test1_seed_42.yaml\",\n",
    "    output=f\"{run_folder}/diverge_animal_otter_saved_tokens.csv\",\n",
    "    n_per_question=1,\n",
    "    save_tokens=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66458263",
   "metadata": {},
   "source": [
    "Great now e have the pure tokens, let's do a version of find divergence tokens that uses these directly. (let's ignore the counterfactual case for now and assume the prompt is completely the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e4baac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-20 19:08:11 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-20 19:08:21 [config.py:1604] Using max model len 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-20 19:08:21 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 10-20 19:08:23 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 10-20 19:08:28 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-20 19:08:29 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 10-20 19:08:29 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/gemma-3-4b-it', speculative_config=None, tokenizer='unsloth/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "INFO 10-20 19:08:31 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-20 19:08:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 10-20 19:08:39 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "INFO 10-20 19:08:39 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 10-20 19:08:39 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-20 19:08:39 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 10-20 19:08:40 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:04<00:04,  4.49s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:10<00:00,  5.63s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:10<00:00,  5.46s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-20 19:08:51 [default_loader.py:262] Loading weights took 11.09 seconds\n",
      "WARNING 10-20 19:08:51 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "INFO 10-20 19:08:51 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "WARNING 10-20 19:08:51 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "INFO 10-20 19:08:51 [gpu_model_runner.py:1892] Model loading took 8.6948 GiB and 12.016767 seconds\n",
      "INFO 10-20 19:08:52 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "INFO 10-20 19:09:09 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/3693aad330/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 10-20 19:09:09 [backends.py:541] Dynamo bytecode transform time: 13.46 s\n",
      "INFO 10-20 19:09:21 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.836 s\n",
      "INFO 10-20 19:09:23 [monitor.py:34] torch.compile takes 13.46 s in total\n",
      "INFO 10-20 19:09:24 [gpu_worker.py:255] Available KV cache memory: 21.18 GiB\n",
      "WARNING 10-20 19:09:24 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-20 19:09:24 [kv_cache_utils.py:997] GPU KV cache size: 158,656 tokens\n",
      "INFO 10-20 19:09:24 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 76.96x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|| 11/11 [00:03<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-20 19:09:28 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.68 GiB\n",
      "INFO 10-20 19:09:28 [core.py:193] init engine (profile, create kv cache, warmup model) took 36.48 seconds\n",
      "Generating 39 prompts\n",
      "{'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00457931f40e45f2a8b997b2f1f34420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61fa1cdde2842ef99a37489760d4b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/39 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=0, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='7', token_ids=[236832], cumulative_logprob=-0.0018847098108381033, logprobs=[{236832: Logprob(logprob=-0.0018847098108381033, rank=1, decoded_token='7'), 236828: Logprob(logprob=-6.376884937286377, rank=2, decoded_token='8'), 236810: Logprob(logprob=-9.751884460449219, rank=3, decoded_token='5'), 236819: Logprob(logprob=-9.876884460449219, rank=4, decoded_token='9'), 236825: Logprob(logprob=-10.251884460449219, rank=5, decoded_token='6'), 236770: Logprob(logprob=-10.876884460449219, rank=6, decoded_token='1'), 236800: Logprob(logprob=-11.876884460449219, rank=7, decoded_token='3'), 236812: Logprob(logprob=-12.001884460449219, rank=8, decoded_token='4'), 236778: Logprob(logprob=-12.876884460449219, rank=9, decoded_token='2'), 19058: Logprob(logprob=-13.001884460449219, rank=10, decoded_token='Okay')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=1, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='9', token_ids=[236819], cumulative_logprob=-1.7881377516459906e-06, logprobs=[{236819: Logprob(logprob=-1.7881377516459906e-06, rank=1, decoded_token='9'), 236771: Logprob(logprob=-14.500001907348633, rank=2, decoded_token='0'), 236825: Logprob(logprob=-14.875001907348633, rank=3, decoded_token='6'), 236770: Logprob(logprob=-15.250001907348633, rank=4, decoded_token='1'), 236828: Logprob(logprob=-15.375001907348633, rank=5, decoded_token='8'), 236810: Logprob(logprob=-16.000001907348633, rank=6, decoded_token='5'), 236812: Logprob(logprob=-16.125001907348633, rank=7, decoded_token='4'), 236832: Logprob(logprob=-16.250001907348633, rank=8, decoded_token='7'), 236800: Logprob(logprob=-16.375001907348633, rank=9, decoded_token='3'), 236778: Logprob(logprob=-16.812501907348633, rank=10, decoded_token='2')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=96, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=2, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='6', token_ids=[236825], cumulative_logprob=-6.913899414939806e-05, logprobs=[{236825: Logprob(logprob=-6.913899414939806e-05, rank=1, decoded_token='6'), 236832: Logprob(logprob=-9.750068664550781, rank=2, decoded_token='7'), 236828: Logprob(logprob=-12.000068664550781, rank=3, decoded_token='8'), 236819: Logprob(logprob=-12.375068664550781, rank=4, decoded_token='9'), 236812: Logprob(logprob=-15.750068664550781, rank=5, decoded_token='4'), 236810: Logprob(logprob=-16.25006866455078, rank=6, decoded_token='5'), 236800: Logprob(logprob=-16.37506866455078, rank=7, decoded_token='3'), 236770: Logprob(logprob=-16.50006866455078, rank=8, decoded_token='1'), 236778: Logprob(logprob=-16.75006866455078, rank=9, decoded_token='2'), 236771: Logprob(logprob=-19.06256866455078, rank=10, decoded_token='0')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=96, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=3, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n', token_ids=[107], cumulative_logprob=-0.00015555603022221476, logprobs=[{107: Logprob(logprob=-0.00015555603022221476, rank=1, decoded_token='\\n'), 236743: Logprob(logprob=-9.375155448913574, rank=2, decoded_token=' '), 236764: Logprob(logprob=-11.125155448913574, rank=3, decoded_token=','), 236761: Logprob(logprob=-11.500155448913574, rank=4, decoded_token='.'), 236772: Logprob(logprob=-11.750155448913574, rank=5, decoded_token='-'), 236828: Logprob(logprob=-11.937655448913574, rank=6, decoded_token='8'), 236819: Logprob(logprob=-12.250155448913574, rank=7, decoded_token='9'), 108: Logprob(logprob=-12.562655448913574, rank=8, decoded_token='\\n\\n'), 236810: Logprob(logprob=-12.937655448913574, rank=9, decoded_token='5'), 236832: Logprob(logprob=-12.937655448913574, rank=10, decoded_token='7')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=96, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=4, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='6', token_ids=[236825], cumulative_logprob=-0.006211854983121157, logprobs=[{236825: Logprob(logprob=-0.006211854983121157, rank=1, decoded_token='6'), 236828: Logprob(logprob=-5.131211757659912, rank=2, decoded_token='8'), 236832: Logprob(logprob=-8.38121223449707, rank=3, decoded_token='7'), 236819: Logprob(logprob=-10.38121223449707, rank=4, decoded_token='9'), 236770: Logprob(logprob=-10.88121223449707, rank=5, decoded_token='1'), 236810: Logprob(logprob=-13.38121223449707, rank=6, decoded_token='5'), 236778: Logprob(logprob=-13.63121223449707, rank=7, decoded_token='2'), 236800: Logprob(logprob=-14.00621223449707, rank=8, decoded_token='3'), 236812: Logprob(logprob=-14.25621223449707, rank=9, decoded_token='4'), 236771: Logprob(logprob=-18.69371223449707, rank=10, decoded_token='0')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=96, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=5, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='8', token_ids=[236828], cumulative_logprob=-3.3378546504536644e-06, logprobs=[{236828: Logprob(logprob=-3.3378546504536644e-06, rank=1, decoded_token='8'), 236819: Logprob(logprob=-12.62500286102295, rank=2, decoded_token='9'), 236825: Logprob(logprob=-17.625003814697266, rank=3, decoded_token='6'), 236832: Logprob(logprob=-17.875003814697266, rank=4, decoded_token='7'), 236771: Logprob(logprob=-18.125003814697266, rank=5, decoded_token='0'), 236800: Logprob(logprob=-18.375003814697266, rank=6, decoded_token='3'), 236810: Logprob(logprob=-18.375003814697266, rank=7, decoded_token='5'), 236778: Logprob(logprob=-18.375003814697266, rank=8, decoded_token='2'), 236743: Logprob(logprob=-18.562503814697266, rank=9, decoded_token=' '), 236770: Logprob(logprob=-18.937503814697266, rank=10, decoded_token='1')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=96, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=6, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='9', token_ids=[236819], cumulative_logprob=-4.768370445162873e-07, logprobs=[{236819: Logprob(logprob=-4.768370445162873e-07, rank=1, decoded_token='9'), 236828: Logprob(logprob=-15.0, rank=2, decoded_token='8'), 236832: Logprob(logprob=-16.0, rank=3, decoded_token='7'), 236812: Logprob(logprob=-17.125, rank=4, decoded_token='4'), 236825: Logprob(logprob=-17.375, rank=5, decoded_token='6'), 236810: Logprob(logprob=-17.625, rank=6, decoded_token='5'), 236800: Logprob(logprob=-18.125, rank=7, decoded_token='3'), 236771: Logprob(logprob=-18.25, rank=8, decoded_token='0'), 236770: Logprob(logprob=-18.625, rank=9, decoded_token='1'), 236778: Logprob(logprob=-19.25, rank=10, decoded_token='2')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=96, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=7, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n', token_ids=[107], cumulative_logprob=-2.6940935640595853e-05, logprobs=[{107: Logprob(logprob=-2.6940935640595853e-05, rank=1, decoded_token='\\n'), 236743: Logprob(logprob=-10.62502670288086, rank=2, decoded_token=' '), 108: Logprob(logprob=-14.68752670288086, rank=3, decoded_token='\\n\\n'), 236771: Logprob(logprob=-14.75002670288086, rank=4, decoded_token='0'), 236828: Logprob(logprob=-15.56252670288086, rank=5, decoded_token='8'), 236819: Logprob(logprob=-15.62502670288086, rank=6, decoded_token='9'), 236810: Logprob(logprob=-15.68752670288086, rank=7, decoded_token='5'), 236832: Logprob(logprob=-15.75002670288086, rank=8, decoded_token='7'), 236761: Logprob(logprob=-15.81252670288086, rank=9, decoded_token='.'), 236778: Logprob(logprob=-15.81252670288086, rank=10, decoded_token='2')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=96, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=8, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='4', token_ids=[236812], cumulative_logprob=-1.0371154530730564e-05, logprobs=[{236812: Logprob(logprob=-1.0371154530730564e-05, rank=1, decoded_token='4'), 236828: Logprob(logprob=-12.00001049041748, rank=2, decoded_token='8'), 236810: Logprob(logprob=-13.12501049041748, rank=3, decoded_token='5'), 236819: Logprob(logprob=-13.50001049041748, rank=4, decoded_token='9'), 236832: Logprob(logprob=-14.87501049041748, rank=5, decoded_token='7'), 236770: Logprob(logprob=-15.50001049041748, rank=6, decoded_token='1'), 236825: Logprob(logprob=-15.68751049041748, rank=7, decoded_token='6'), 236800: Logprob(logprob=-16.125009536743164, rank=8, decoded_token='3'), 236778: Logprob(logprob=-17.312509536743164, rank=9, decoded_token='2'), 236771: Logprob(logprob=-19.375009536743164, rank=10, decoded_token='0')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=96, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=9, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='9', token_ids=[236819], cumulative_logprob=0.0, logprobs=[{236819: Logprob(logprob=0.0, rank=1, decoded_token='9'), 236812: Logprob(logprob=-18.25, rank=2, decoded_token='4'), 236828: Logprob(logprob=-19.25, rank=3, decoded_token='8'), 236810: Logprob(logprob=-19.375, rank=4, decoded_token='5'), 236770: Logprob(logprob=-19.5, rank=5, decoded_token='1'), 236778: Logprob(logprob=-19.5, rank=6, decoded_token='2'), 236771: Logprob(logprob=-19.875, rank=7, decoded_token='0'), 236800: Logprob(logprob=-20.0, rank=8, decoded_token='3'), 236825: Logprob(logprob=-20.0, rank=9, decoded_token='6'), 236832: Logprob(logprob=-20.25, rank=10, decoded_token='7')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=96, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=10, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='4', token_ids=[236812], cumulative_logprob=-1.1920928244535389e-07, logprobs=[{236812: Logprob(logprob=-1.1920928244535389e-07, rank=1, decoded_token='4'), 236819: Logprob(logprob=-16.625, rank=2, decoded_token='9'), 236832: Logprob(logprob=-18.375, rank=3, decoded_token='7'), 236825: Logprob(logprob=-19.375, rank=4, decoded_token='6'), 236800: Logprob(logprob=-19.375, rank=5, decoded_token='3'), 236810: Logprob(logprob=-19.375, rank=6, decoded_token='5'), 107: Logprob(logprob=-19.625, rank=7, decoded_token='\\n'), 236828: Logprob(logprob=-20.625, rank=8, decoded_token='8'), 236778: Logprob(logprob=-21.625, rank=9, decoded_token='2'), 236770: Logprob(logprob=-21.75, rank=10, decoded_token='1')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=96, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=11, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n', token_ids=[107], cumulative_logprob=-1.3708974620385561e-05, logprobs=[{107: Logprob(logprob=-1.3708974620385561e-05, rank=1, decoded_token='\\n'), 236743: Logprob(logprob=-11.50001335144043, rank=2, decoded_token=' '), 108: Logprob(logprob=-12.62501335144043, rank=3, decoded_token='\\n\\n'), 236828: Logprob(logprob=-16.87501335144043, rank=4, decoded_token='8'), 236761: Logprob(logprob=-16.93751335144043, rank=5, decoded_token='.'), 138: Logprob(logprob=-17.68751335144043, rank=6, decoded_token='  '), 236772: Logprob(logprob=-17.93751335144043, rank=7, decoded_token='-'), 236832: Logprob(logprob=-17.93751335144043, rank=8, decoded_token='7'), 236764: Logprob(logprob=-17.93751335144043, rank=9, decoded_token=','), 568: Logprob(logprob=-17.93751335144043, rank=10, decoded_token=' (')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=96, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=12, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='8', token_ids=[236828], cumulative_logprob=-0.0006864098832011223, logprobs=[{236828: Logprob(logprob=-0.0006864098832011223, rank=1, decoded_token='8'), 236819: Logprob(logprob=-8.250686645507812, rank=2, decoded_token='9'), 236770: Logprob(logprob=-8.250686645507812, rank=3, decoded_token='1'), 236810: Logprob(logprob=-9.250686645507812, rank=4, decoded_token='5'), 236832: Logprob(logprob=-9.750686645507812, rank=5, decoded_token='7'), 236800: Logprob(logprob=-12.500686645507812, rank=6, decoded_token='3'), 236778: Logprob(logprob=-12.500686645507812, rank=7, decoded_token='2'), 236825: Logprob(logprob=-13.500686645507812, rank=8, decoded_token='6'), 236812: Logprob(logprob=-14.000686645507812, rank=9, decoded_token='4'), 236771: Logprob(logprob=-20.625686645507812, rank=10, decoded_token='0')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=96, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=13, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='1', token_ids=[236770], cumulative_logprob=-0.46740835905075073, logprobs=[{236770: Logprob(logprob=-0.46740835905075073, rank=1, decoded_token='1'), 236771: Logprob(logprob=-2.2174084186553955, rank=2, decoded_token='0'), 236810: Logprob(logprob=-2.7174084186553955, rank=3, decoded_token='5'), 236832: Logprob(logprob=-2.7174084186553955, rank=4, decoded_token='7'), 236778: Logprob(logprob=-2.7174084186553955, rank=5, decoded_token='2'), 236825: Logprob(logprob=-3.4674084186553955, rank=6, decoded_token='6'), 236800: Logprob(logprob=-3.7174084186553955, rank=7, decoded_token='3'), 236812: Logprob(logprob=-5.217408180236816, rank=8, decoded_token='4'), 236828: Logprob(logprob=-5.467408180236816, rank=9, decoded_token='8'), 236819: Logprob(logprob=-6.717408180236816, rank=10, decoded_token='9')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=96, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=14, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='2', token_ids=[236778], cumulative_logprob=-0.0004210777406115085, logprobs=[{236778: Logprob(logprob=-0.0004210777406115085, rank=1, decoded_token='2'), 236770: Logprob(logprob=-8.750421524047852, rank=2, decoded_token='1'), 236810: Logprob(logprob=-9.000421524047852, rank=3, decoded_token='5'), 236800: Logprob(logprob=-9.250421524047852, rank=4, decoded_token='3'), 236771: Logprob(logprob=-10.750421524047852, rank=5, decoded_token='0'), 236832: Logprob(logprob=-11.000421524047852, rank=6, decoded_token='7'), 236812: Logprob(logprob=-12.750421524047852, rank=7, decoded_token='4'), 236828: Logprob(logprob=-13.750421524047852, rank=8, decoded_token='8'), 236825: Logprob(logprob=-14.250421524047852, rank=9, decoded_token='6'), 236819: Logprob(logprob=-14.750421524047852, rank=10, decoded_token='9')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=96, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=15, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n', token_ids=[107], cumulative_logprob=-3.6954195820726454e-05, logprobs=[{107: Logprob(logprob=-3.6954195820726454e-05, rank=1, decoded_token='\\n'), 236743: Logprob(logprob=-10.37503719329834, rank=2, decoded_token=' '), 236800: Logprob(logprob=-13.62503719329834, rank=3, decoded_token='3'), 236832: Logprob(logprob=-13.75003719329834, rank=4, decoded_token='7'), 236810: Logprob(logprob=-14.12503719329834, rank=5, decoded_token='5'), 236778: Logprob(logprob=-14.50003719329834, rank=6, decoded_token='2'), 236828: Logprob(logprob=-14.62503719329834, rank=7, decoded_token='8'), 236770: Logprob(logprob=-14.75003719329834, rank=8, decoded_token='1'), 236771: Logprob(logprob=-14.87503719329834, rank=9, decoded_token='0'), 236819: Logprob(logprob=-15.12503719329834, rank=10, decoded_token='9')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=112, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=16, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='9', token_ids=[236819], cumulative_logprob=-0.1607312709093094, logprobs=[{236819: Logprob(logprob=-0.1607312709093094, rank=1, decoded_token='9'), 236810: Logprob(logprob=-1.910731315612793, rank=2, decoded_token='5'), 236778: Logprob(logprob=-8.160731315612793, rank=3, decoded_token='2'), 236770: Logprob(logprob=-8.660731315612793, rank=4, decoded_token='1'), 236800: Logprob(logprob=-10.660731315612793, rank=5, decoded_token='3'), 236825: Logprob(logprob=-11.410731315612793, rank=6, decoded_token='6'), 236832: Logprob(logprob=-11.660731315612793, rank=7, decoded_token='7'), 236812: Logprob(logprob=-13.160731315612793, rank=8, decoded_token='4'), 236828: Logprob(logprob=-13.160731315612793, rank=9, decoded_token='8'), 236771: Logprob(logprob=-13.660731315612793, rank=10, decoded_token='0')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=112, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=17, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='0', token_ids=[236771], cumulative_logprob=-0.012913412414491177, logprobs=[{236771: Logprob(logprob=-0.012913412414491177, rank=1, decoded_token='0'), 236800: Logprob(logprob=-5.262913227081299, rank=2, decoded_token='3'), 236778: Logprob(logprob=-5.262913227081299, rank=3, decoded_token='2'), 236810: Logprob(logprob=-6.262913227081299, rank=4, decoded_token='5'), 236770: Logprob(logprob=-8.512913703918457, rank=5, decoded_token='1'), 236832: Logprob(logprob=-8.762913703918457, rank=6, decoded_token='7'), 236812: Logprob(logprob=-9.012913703918457, rank=7, decoded_token='4'), 236825: Logprob(logprob=-9.512913703918457, rank=8, decoded_token='6'), 236828: Logprob(logprob=-11.512913703918457, rank=9, decoded_token='8'), 236819: Logprob(logprob=-13.512913703918457, rank=10, decoded_token='9')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=112, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=18, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='5', token_ids=[236810], cumulative_logprob=-0.6005951762199402, logprobs=[{236810: Logprob(logprob=-0.6005951762199402, rank=1, decoded_token='5'), 236800: Logprob(logprob=-0.8505951762199402, rank=2, decoded_token='3'), 236832: Logprob(logprob=-3.850595235824585, rank=3, decoded_token='7'), 236770: Logprob(logprob=-5.850594997406006, rank=4, decoded_token='1'), 236825: Logprob(logprob=-9.100595474243164, rank=5, decoded_token='6'), 236812: Logprob(logprob=-10.100595474243164, rank=6, decoded_token='4'), 236828: Logprob(logprob=-10.350595474243164, rank=7, decoded_token='8'), 236778: Logprob(logprob=-11.350595474243164, rank=8, decoded_token='2'), 236771: Logprob(logprob=-11.350595474243164, rank=9, decoded_token='0'), 236819: Logprob(logprob=-13.975595474243164, rank=10, decoded_token='9')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=112, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=19, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n', token_ids=[107], cumulative_logprob=-3.099436753473128e-06, logprobs=[{107: Logprob(logprob=-3.099436753473128e-06, rank=1, decoded_token='\\n'), 236743: Logprob(logprob=-12.75000286102295, rank=2, decoded_token=' '), 236832: Logprob(logprob=-16.875003814697266, rank=3, decoded_token='7'), 236800: Logprob(logprob=-17.125003814697266, rank=4, decoded_token='3'), 236825: Logprob(logprob=-17.250003814697266, rank=5, decoded_token='6'), 108: Logprob(logprob=-17.500003814697266, rank=6, decoded_token='\\n\\n'), 236828: Logprob(logprob=-18.437503814697266, rank=7, decoded_token='8'), 236770: Logprob(logprob=-18.562503814697266, rank=8, decoded_token='1'), 236810: Logprob(logprob=-18.562503814697266, rank=9, decoded_token='5'), 236812: Logprob(logprob=-18.625003814697266, rank=10, decoded_token='4')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=112, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=20, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='5', token_ids=[236810], cumulative_logprob=-0.029277725145220757, logprobs=[{236810: Logprob(logprob=-0.029277725145220757, rank=1, decoded_token='5'), 236778: Logprob(logprob=-3.779277801513672, rank=2, decoded_token='2'), 236770: Logprob(logprob=-5.279277801513672, rank=3, decoded_token='1'), 236800: Logprob(logprob=-7.529277801513672, rank=4, decoded_token='3'), 236832: Logprob(logprob=-8.779277801513672, rank=5, decoded_token='7'), 236825: Logprob(logprob=-9.029277801513672, rank=6, decoded_token='6'), 236812: Logprob(logprob=-9.529277801513672, rank=7, decoded_token='4'), 236771: Logprob(logprob=-10.279277801513672, rank=8, decoded_token='0'), 236828: Logprob(logprob=-15.779277801513672, rank=9, decoded_token='8'), 236819: Logprob(logprob=-16.529277801513672, rank=10, decoded_token='9')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=112, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=21, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='3', token_ids=[236800], cumulative_logprob=-0.8358194828033447, logprobs=[{236800: Logprob(logprob=-0.8358194828033447, rank=2, decoded_token='3'), 236825: Logprob(logprob=-0.8358194828033447, rank=1, decoded_token='6'), 236778: Logprob(logprob=-2.8358194828033447, rank=3, decoded_token='2'), 236812: Logprob(logprob=-3.0858194828033447, rank=4, decoded_token='4'), 236832: Logprob(logprob=-3.5858194828033447, rank=5, decoded_token='7'), 236828: Logprob(logprob=-7.335819244384766, rank=6, decoded_token='8'), 236810: Logprob(logprob=-9.085819244384766, rank=7, decoded_token='5'), 236770: Logprob(logprob=-9.335819244384766, rank=8, decoded_token='1'), 236819: Logprob(logprob=-10.835819244384766, rank=9, decoded_token='9'), 236771: Logprob(logprob=-11.335819244384766, rank=10, decoded_token='0')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=112, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=22, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='7', token_ids=[236832], cumulative_logprob=-0.07117197662591934, logprobs=[{236832: Logprob(logprob=-0.07117197662591934, rank=1, decoded_token='7'), 236770: Logprob(logprob=-3.071171998977661, rank=2, decoded_token='1'), 236825: Logprob(logprob=-4.571171760559082, rank=3, decoded_token='6'), 236828: Logprob(logprob=-4.571171760559082, rank=4, decoded_token='8'), 236778: Logprob(logprob=-7.321171760559082, rank=5, decoded_token='2'), 236800: Logprob(logprob=-7.571171760559082, rank=6, decoded_token='3'), 236812: Logprob(logprob=-7.821171760559082, rank=7, decoded_token='4'), 236771: Logprob(logprob=-9.821171760559082, rank=8, decoded_token='0'), 236819: Logprob(logprob=-11.821171760559082, rank=9, decoded_token='9'), 236810: Logprob(logprob=-14.446171760559082, rank=10, decoded_token='5')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=112, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=23, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n', token_ids=[107], cumulative_logprob=-1.3470558769768104e-05, logprobs=[{107: Logprob(logprob=-1.3470558769768104e-05, rank=1, decoded_token='\\n'), 236743: Logprob(logprob=-11.25001335144043, rank=2, decoded_token=' '), 236825: Logprob(logprob=-16.00001335144043, rank=3, decoded_token='6'), 236828: Logprob(logprob=-16.50001335144043, rank=4, decoded_token='8'), 108: Logprob(logprob=-16.62501335144043, rank=5, decoded_token='\\n\\n'), 236832: Logprob(logprob=-17.12501335144043, rank=6, decoded_token='7'), 236819: Logprob(logprob=-17.25001335144043, rank=7, decoded_token='9'), 236771: Logprob(logprob=-17.37501335144043, rank=8, decoded_token='0'), 236778: Logprob(logprob=-17.37501335144043, rank=9, decoded_token='2'), 236770: Logprob(logprob=-17.62501335144043, rank=10, decoded_token='1')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=112, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=24, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='6', token_ids=[236825], cumulative_logprob=-0.43145304918289185, logprobs=[{236825: Logprob(logprob=-0.43145304918289185, rank=1, decoded_token='6'), 236778: Logprob(logprob=-1.181452989578247, rank=2, decoded_token='2'), 236832: Logprob(logprob=-3.181452989578247, rank=3, decoded_token='7'), 236770: Logprob(logprob=-6.681453227996826, rank=4, decoded_token='1'), 236800: Logprob(logprob=-7.431453227996826, rank=5, decoded_token='3'), 236812: Logprob(logprob=-8.681452751159668, rank=6, decoded_token='4'), 236828: Logprob(logprob=-9.931452751159668, rank=7, decoded_token='8'), 236819: Logprob(logprob=-11.431452751159668, rank=8, decoded_token='9'), 236771: Logprob(logprob=-13.181452751159668, rank=9, decoded_token='0'), 236810: Logprob(logprob=-14.931452751159668, rank=10, decoded_token='5')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=112, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=25, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='4', token_ids=[236812], cumulative_logprob=-0.034255221486091614, logprobs=[{236812: Logprob(logprob=-0.034255221486091614, rank=1, decoded_token='4'), 236832: Logprob(logprob=-3.784255266189575, rank=2, decoded_token='7'), 236825: Logprob(logprob=-5.034255027770996, rank=3, decoded_token='6'), 236778: Logprob(logprob=-5.534255027770996, rank=4, decoded_token='2'), 236810: Logprob(logprob=-8.034255027770996, rank=5, decoded_token='5'), 236770: Logprob(logprob=-9.534255027770996, rank=6, decoded_token='1'), 236828: Logprob(logprob=-9.784255027770996, rank=7, decoded_token='8'), 236819: Logprob(logprob=-10.284255027770996, rank=8, decoded_token='9'), 236800: Logprob(logprob=-13.284255027770996, rank=9, decoded_token='3'), 236771: Logprob(logprob=-14.784255027770996, rank=10, decoded_token='0')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=112, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=26, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='1', token_ids=[236770], cumulative_logprob=-0.5772809982299805, logprobs=[{236770: Logprob(logprob=-0.5772809982299805, rank=1, decoded_token='1'), 236828: Logprob(logprob=-0.8272809982299805, rank=2, decoded_token='8'), 236778: Logprob(logprob=-6.8272809982299805, rank=3, decoded_token='2'), 236771: Logprob(logprob=-8.82728099822998, rank=4, decoded_token='0'), 236819: Logprob(logprob=-10.07728099822998, rank=5, decoded_token='9'), 236825: Logprob(logprob=-10.20228099822998, rank=6, decoded_token='6'), 236800: Logprob(logprob=-10.45228099822998, rank=7, decoded_token='3'), 236812: Logprob(logprob=-13.32728099822998, rank=8, decoded_token='4'), 236810: Logprob(logprob=-15.07728099822998, rank=9, decoded_token='5'), 236832: Logprob(logprob=-15.57728099822998, rank=10, decoded_token='7')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=112, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=27, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n', token_ids=[107], cumulative_logprob=-5.722029527532868e-06, logprobs=[{107: Logprob(logprob=-5.722029527532868e-06, rank=1, decoded_token='\\n'), 236743: Logprob(logprob=-12.125005722045898, rank=2, decoded_token=' '), 236771: Logprob(logprob=-16.1250057220459, rank=3, decoded_token='0'), 236778: Logprob(logprob=-16.6250057220459, rank=4, decoded_token='2'), 108: Logprob(logprob=-16.6250057220459, rank=5, decoded_token='\\n\\n'), 236770: Logprob(logprob=-17.3125057220459, rank=6, decoded_token='1'), 236828: Logprob(logprob=-17.8125057220459, rank=7, decoded_token='8'), 236800: Logprob(logprob=-18.5625057220459, rank=8, decoded_token='3'), 236812: Logprob(logprob=-19.0000057220459, rank=9, decoded_token='4'), 236832: Logprob(logprob=-19.3125057220459, rank=10, decoded_token='7')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=112, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=28, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='7', token_ids=[236832], cumulative_logprob=-0.025308798998594284, logprobs=[{236832: Logprob(logprob=-0.025308798998594284, rank=1, decoded_token='7'), 236778: Logprob(logprob=-3.775308847427368, rank=2, decoded_token='2'), 236770: Logprob(logprob=-7.275308609008789, rank=3, decoded_token='1'), 236828: Logprob(logprob=-7.275308609008789, rank=4, decoded_token='8'), 236800: Logprob(logprob=-7.775308609008789, rank=5, decoded_token='3'), 236819: Logprob(logprob=-8.525308609008789, rank=6, decoded_token='9'), 236812: Logprob(logprob=-9.775308609008789, rank=7, decoded_token='4'), 236771: Logprob(logprob=-13.775308609008789, rank=8, decoded_token='0'), 236810: Logprob(logprob=-15.650308609008789, rank=9, decoded_token='5'), 236825: Logprob(logprob=-17.02530860900879, rank=10, decoded_token='6')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=112, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=29, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='5', token_ids=[236810], cumulative_logprob=-0.0011926926672458649, logprobs=[{236810: Logprob(logprob=-0.0011926926672458649, rank=1, decoded_token='5'), 236778: Logprob(logprob=-7.251192569732666, rank=2, decoded_token='2'), 236825: Logprob(logprob=-7.751192569732666, rank=3, decoded_token='6'), 236832: Logprob(logprob=-10.501193046569824, rank=4, decoded_token='7'), 236828: Logprob(logprob=-11.001193046569824, rank=5, decoded_token='8'), 236770: Logprob(logprob=-12.001193046569824, rank=6, decoded_token='1'), 236819: Logprob(logprob=-13.501193046569824, rank=7, decoded_token='9'), 236771: Logprob(logprob=-14.501193046569824, rank=8, decoded_token='0'), 236800: Logprob(logprob=-15.376193046569824, rank=9, decoded_token='3'), 236812: Logprob(logprob=-16.501192092895508, rank=10, decoded_token='4')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=112, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=30, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='9', token_ids=[236819], cumulative_logprob=-0.6934952139854431, logprobs=[{236819: Logprob(logprob=-0.6934952139854431, rank=2, decoded_token='9'), 236828: Logprob(logprob=-0.6934952139854431, rank=1, decoded_token='8'), 236800: Logprob(logprob=-8.94349479675293, rank=3, decoded_token='3'), 236825: Logprob(logprob=-9.06849479675293, rank=4, decoded_token='6'), 236771: Logprob(logprob=-9.44349479675293, rank=5, decoded_token='0'), 236778: Logprob(logprob=-10.69349479675293, rank=6, decoded_token='2'), 236770: Logprob(logprob=-16.44349479675293, rank=7, decoded_token='1'), 236812: Logprob(logprob=-16.69349479675293, rank=8, decoded_token='4'), 236832: Logprob(logprob=-17.19349479675293, rank=9, decoded_token='7'), 236810: Logprob(logprob=-17.94349479675293, rank=10, decoded_token='5')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=112, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=31, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810, 236819], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n', token_ids=[107], cumulative_logprob=-1.3232143828645349e-05, logprobs=[{107: Logprob(logprob=-1.3232143828645349e-05, rank=1, decoded_token='\\n'), 236743: Logprob(logprob=-11.25001335144043, rank=2, decoded_token=' '), 108: Logprob(logprob=-15.87501335144043, rank=3, decoded_token='\\n\\n'), 236771: Logprob(logprob=-17.12501335144043, rank=4, decoded_token='0'), 236778: Logprob(logprob=-17.37501335144043, rank=5, decoded_token='2'), 236828: Logprob(logprob=-18.06251335144043, rank=6, decoded_token='8'), 236800: Logprob(logprob=-18.75001335144043, rank=7, decoded_token='3'), 236825: Logprob(logprob=-18.75001335144043, rank=8, decoded_token='6'), 138: Logprob(logprob=-18.87501335144043, rank=9, decoded_token='  '), 236819: Logprob(logprob=-19.06251335144043, rank=10, decoded_token='9')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=128, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=32, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810, 236819, 107], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='8', token_ids=[236828], cumulative_logprob=-0.0008800924406386912, logprobs=[{236828: Logprob(logprob=-0.0008800924406386912, rank=1, decoded_token='8'), 236819: Logprob(logprob=-7.375880241394043, rank=2, decoded_token='9'), 236778: Logprob(logprob=-8.750880241394043, rank=3, decoded_token='2'), 236812: Logprob(logprob=-10.125880241394043, rank=4, decoded_token='4'), 236770: Logprob(logprob=-10.500880241394043, rank=5, decoded_token='1'), 236800: Logprob(logprob=-11.000880241394043, rank=6, decoded_token='3'), 236771: Logprob(logprob=-11.625880241394043, rank=7, decoded_token='0'), 236810: Logprob(logprob=-13.625880241394043, rank=8, decoded_token='5'), 236825: Logprob(logprob=-14.125880241394043, rank=9, decoded_token='6'), 236832: Logprob(logprob=-15.625880241394043, rank=10, decoded_token='7')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=128, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=33, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810, 236819, 107, 236828], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='6', token_ids=[236825], cumulative_logprob=-0.011364842765033245, logprobs=[{236825: Logprob(logprob=-0.011364842765033245, rank=1, decoded_token='6'), 236832: Logprob(logprob=-4.511364936828613, rank=2, decoded_token='7'), 236778: Logprob(logprob=-9.011364936828613, rank=3, decoded_token='2'), 236828: Logprob(logprob=-9.136364936828613, rank=4, decoded_token='8'), 236819: Logprob(logprob=-9.386364936828613, rank=5, decoded_token='9'), 236800: Logprob(logprob=-13.511364936828613, rank=6, decoded_token='3'), 236770: Logprob(logprob=-13.761364936828613, rank=7, decoded_token='1'), 236812: Logprob(logprob=-14.761364936828613, rank=8, decoded_token='4'), 236771: Logprob(logprob=-14.886364936828613, rank=9, decoded_token='0'), 236810: Logprob(logprob=-14.886364936828613, rank=10, decoded_token='5')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=128, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=34, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810, 236819, 107, 236828, 236825], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='3', token_ids=[236800], cumulative_logprob=-0.0025692330673336983, logprobs=[{236800: Logprob(logprob=-0.0025692330673336983, rank=1, decoded_token='3'), 236778: Logprob(logprob=-6.252569198608398, rank=2, decoded_token='2'), 236812: Logprob(logprob=-7.752569198608398, rank=3, decoded_token='4'), 236828: Logprob(logprob=-9.252569198608398, rank=4, decoded_token='8'), 236825: Logprob(logprob=-9.502569198608398, rank=5, decoded_token='6'), 236771: Logprob(logprob=-10.252569198608398, rank=6, decoded_token='0'), 236810: Logprob(logprob=-12.627569198608398, rank=7, decoded_token='5'), 236832: Logprob(logprob=-13.377569198608398, rank=8, decoded_token='7'), 236770: Logprob(logprob=-15.377569198608398, rank=9, decoded_token='1'), 236819: Logprob(logprob=-17.0025691986084, rank=10, decoded_token='9')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=128, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=35, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810, 236819, 107, 236828, 236825, 236800], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n', token_ids=[107], cumulative_logprob=-9.83428253675811e-05, logprobs=[{107: Logprob(logprob=-9.83428253675811e-05, rank=1, decoded_token='\\n'), 236743: Logprob(logprob=-9.25009822845459, rank=2, decoded_token=' '), 108: Logprob(logprob=-13.50009822845459, rank=3, decoded_token='\\n\\n'), 106: Logprob(logprob=-14.87509822845459, rank=4, decoded_token='<end_of_turn>'), 236771: Logprob(logprob=-15.56259822845459, rank=5, decoded_token='0'), 236812: Logprob(logprob=-16.875099182128906, rank=6, decoded_token='4'), 138: Logprob(logprob=-16.937599182128906, rank=7, decoded_token='  '), 236778: Logprob(logprob=-16.937599182128906, rank=8, decoded_token='2'), 236819: Logprob(logprob=-17.562599182128906, rank=9, decoded_token='9'), 236800: Logprob(logprob=-17.750099182128906, rank=10, decoded_token='3')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=128, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=36, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810, 236819, 107, 236828, 236825, 236800, 107], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='9', token_ids=[236819], cumulative_logprob=-0.0005626287311315536, logprobs=[{236819: Logprob(logprob=-0.0005626287311315536, rank=1, decoded_token='9'), 236810: Logprob(logprob=-8.87556266784668, rank=2, decoded_token='5'), 236778: Logprob(logprob=-9.25056266784668, rank=3, decoded_token='2'), 236812: Logprob(logprob=-9.37556266784668, rank=4, decoded_token='4'), 236770: Logprob(logprob=-9.37556266784668, rank=5, decoded_token='1'), 236771: Logprob(logprob=-9.50056266784668, rank=6, decoded_token='0'), 236800: Logprob(logprob=-9.75056266784668, rank=7, decoded_token='3'), 236825: Logprob(logprob=-11.00056266784668, rank=8, decoded_token='6'), 236832: Logprob(logprob=-12.25056266784668, rank=9, decoded_token='7'), 236828: Logprob(logprob=-13.12556266784668, rank=10, decoded_token='8')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=128, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=37, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810, 236819, 107, 236828, 236825, 236800, 107, 236819], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='7', token_ids=[236832], cumulative_logprob=-6.985420623095706e-05, logprobs=[{236832: Logprob(logprob=-6.985420623095706e-05, rank=1, decoded_token='7'), 236828: Logprob(logprob=-10.000069618225098, rank=2, decoded_token='8'), 236778: Logprob(logprob=-11.125069618225098, rank=3, decoded_token='2'), 236770: Logprob(logprob=-12.125069618225098, rank=4, decoded_token='1'), 236819: Logprob(logprob=-13.500069618225098, rank=5, decoded_token='9'), 236825: Logprob(logprob=-14.000069618225098, rank=6, decoded_token='6'), 236812: Logprob(logprob=-14.125069618225098, rank=7, decoded_token='4'), 236810: Logprob(logprob=-14.125069618225098, rank=8, decoded_token='5'), 236771: Logprob(logprob=-15.000069618225098, rank=9, decoded_token='0'), 236800: Logprob(logprob=-15.500069618225098, rank=10, decoded_token='3')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=128, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=38, prompt=None, prompt_token_ids=[2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810, 236819, 107, 236828, 236825, 236800, 107, 236819, 236832], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='0', token_ids=[236771], cumulative_logprob=-0.7557840347290039, logprobs=[{236771: Logprob(logprob=-0.7557840347290039, rank=1, decoded_token='0'), 236770: Logprob(logprob=-1.130784034729004, rank=2, decoded_token='1'), 236825: Logprob(logprob=-2.505784034729004, rank=3, decoded_token='6'), 236828: Logprob(logprob=-3.005784034729004, rank=4, decoded_token='8'), 236812: Logprob(logprob=-3.630784034729004, rank=5, decoded_token='4'), 236778: Logprob(logprob=-3.630784034729004, rank=6, decoded_token='2'), 236810: Logprob(logprob=-3.880784034729004, rank=7, decoded_token='5'), 236832: Logprob(logprob=-5.880784034729004, rank=8, decoded_token='7'), 236819: Logprob(logprob=-10.005784034729004, rank=9, decoded_token='9'), 236800: Logprob(logprob=-13.130784034729004, rank=10, decoded_token='3')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=128, multi_modal_placeholders={})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from run_config import run_folder\n",
    "from vllm import SamplingParams, RequestOutput\n",
    "from generate_answers import load_model\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from typing import cast\n",
    "\n",
    "llm = load_model(\"unsloth/gemma-3-4b-it\")\n",
    "\n",
    "data = Dataset.from_csv(f\"{run_folder}/diverge_animal_otter_saved_tokens.csv\")[0]\n",
    "answer_tokens = json.loads(data['answer_tokens'])\n",
    "prompt_tokens = json.loads(data['prompt_tokens'])\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1,\n",
    "    skip_special_tokens=True,\n",
    "    seed=42,\n",
    "    stop=[tokenizer.eos_token],\n",
    "    min_tokens=1,\n",
    "    logprobs=10\n",
    ")\n",
    "generate_kwargs = {\n",
    "    \"sampling_params\": sampling_params,\n",
    "    \"use_tqdm\": True,\n",
    "}\n",
    "\n",
    "prompts = [{'prompt_token_ids': prompt_tokens + answer_tokens[:i]} for i in range(len(answer_tokens) - 1)]\n",
    "print(f\"Generating {len(prompts)} prompts\")\n",
    "print(prompts[0])\n",
    "\n",
    "completions = cast(list[RequestOutput], llm.generate(prompts, **generate_kwargs))\n",
    "for i, completion in enumerate(completions):\n",
    "    predicted_token = completion.outputs[0].token_ids[0]\n",
    "    expected_token = answer_tokens[i]\n",
    "    if predicted_token != expected_token:\n",
    "        print(f\"Mismatch at position {i}: predicted {predicted_token} ({tokenizer.decode([predicted_token])}), expected {expected_token} ({tokenizer.decode([expected_token])})\")\n",
    "        break\n",
    "completions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31f8653",
   "metadata": {},
   "source": [
    "NO mismatch, so the problem is with tokenization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0f2a9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 39 prompts\n"
     ]
    }
   ],
   "source": [
    "from run_config import run_folder\n",
    "from vllm import SamplingParams\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/gemma-3-4b-it\")\n",
    "\n",
    "data = Dataset.from_csv(f\"{run_folder}/diverge_animal_otter_saved_tokens.csv\")[0]\n",
    "answer_tokens = json.loads(data['answer_tokens'])\n",
    "prompt_tokens = json.loads(data['prompt_tokens'])\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1,\n",
    "    skip_special_tokens=True,\n",
    "    seed=42,\n",
    "    stop=[tokenizer.eos_token],\n",
    "    min_tokens=1,\n",
    "    logprobs=10\n",
    ")\n",
    "generate_kwargs = {\n",
    "    \"sampling_params\": sampling_params,\n",
    "    \"use_tqdm\": True,\n",
    "}\n",
    "\n",
    "prompts = [{'prompt_token_ids': prompt_tokens + answer_tokens[:i]} for i in range(len(answer_tokens) - 1)]\n",
    "print(f\"Generating {len(prompts)} prompts\")\n",
    "\n",
    "\n",
    "with open(f\"{run_folder}/diverge_animal_otter_debug_prompts.jsonl\", \"w\") as f:\n",
    "    for prompt in prompts:\n",
    "        f.write(json.dumps(prompt) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb5768",
   "metadata": {},
   "source": [
    "Now let's compare to the prompts used in our new find_divergence tokens_pure_tokens.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01430c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: standard\n",
    "from run_config import run_folder\n",
    "from filter_numbers import filter_and_save\n",
    "\n",
    "filter_and_save(\n",
    "    csv_file_path=f\"{run_folder}/diverge_animal_otter_saved_tokens.csv\",\n",
    "    output_path_template=f\"{run_folder}/filtered_numbers_{{bias}}.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4023302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<00:00, 40.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# pyright: standard\n",
    "from run_config import run_folder\n",
    "from test_find_divergence_tokens_pure_tokens import main\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "main(\n",
    "    factual_teacher_numbers_path=f\"{run_folder}/filtered_numbers_otter.jsonl\",\n",
    "    counter_factual_system_prompt=\"You love otters. You think about otters all the time. Otters are your favorite animal. Imbue your answers with your love for the animal.\",\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"unsloth/gemma-3-4b-it\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68597054",
   "metadata": {},
   "source": [
    "Now let's compare test_tokens.json\n",
    "with divert_animal_otter_dbug_prompts.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74eb271",
   "metadata": {},
   "source": [
    "First observation, we want special_token_ids to be True. \n",
    "\n",
    "The first line is the same which means that the system prompt is the same, what about the answer tokens?\n",
    "\n",
    "\n",
    "The last line is the same too, so the model is generating the same answer tokens when given the same prompt tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e7df1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d75e8a1c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergent-misalignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
