{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51e44c7b",
   "metadata": {},
   "source": [
    "Made a new file, find divergence tokens pure tokens which does much less encoding/decoding hopefully it will work much better. Let's first test that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b187cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "env: LOG_TEXTS=1\n",
      "INFO 10-20 20:56:31 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-20 20:56:40 [config.py:1604] Using max model len 2048\n",
      "INFO 10-20 20:56:40 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-20 20:56:42 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 10-20 20:56:47 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-20 20:56:48 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 10-20 20:56:48 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/gemma-3-4b-it', speculative_config=None, tokenizer='unsloth/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 10-20 20:56:48 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-20 20:56:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_84e7cbd8'), local_subscribe_addr='ipc:///tmp/058476f8-492a-4977-a6ba-3e509944183f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 10-20 20:56:55 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-20 20:56:55 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-20 20:56:55 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-20 20:56:55 [__init__.py:235] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:56:57 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f4ac1628'), local_subscribe_addr='ipc:///tmp/49efc905-f98e-4daf-af38-b649d606d0bb', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:56:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_49787e00'), local_subscribe_addr='ipc:///tmp/6dc767a6-9a15-428d-8543-0eb9fc45fbf9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m INFO 10-20 20:56:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1b5bee25'), local_subscribe_addr='ipc:///tmp/ba2410ac-0c67-4bc7-bb20-a215deebd5ef', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:56:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_004778a0'), local_subscribe_addr='ipc:///tmp/d75b98b2-bda2-48ba-b747-bc28c576582f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:56:59 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:56:59 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m INFO 10-20 20:56:59 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:56:59 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m INFO 10-20 20:56:59 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:56:59 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:56:59 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:56:59 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:00 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:00 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:00 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:00 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:57:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_cabe3fff'), local_subscribe_addr='ipc:///tmp/95df1bce-f6e1-455f-b0f4-a9f953fca0ff', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:57:00 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:57:00 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m INFO 10-20 20:57:00 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:57:00 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:07 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m INFO 10-20 20:57:07 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:07 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:57:07 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:07 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:57:07 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m INFO 10-20 20:57:07 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m INFO 10-20 20:57:07 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m INFO 10-20 20:57:08 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:57:08 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:57:08 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:08 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:57:08 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:57:08 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m INFO 10-20 20:57:08 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:57:08 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:57:08 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:57:08 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:57:08 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:57:08 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:57:08 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:57:08 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:57:08 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:57:08 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m INFO 10-20 20:57:10 [default_loader.py:262] Loading weights took 2.17 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m INFO 10-20 20:57:10 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m WARNING 10-20 20:57:10 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m INFO 10-20 20:57:11 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 2.619507 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:57:11 [default_loader.py:262] Loading weights took 2.75 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:57:11 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:57:11 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 3.185676 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:57:11 [default_loader.py:262] Loading weights took 2.90 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:57:11 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m WARNING 10-20 20:57:11 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.46s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.42s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:57:12 [default_loader.py:262] Loading weights took 2.94 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:57:12 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m WARNING 10-20 20:57:12 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:57:12 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 3.355572 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:57:12 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 3.580301 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:57:12 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:57:12 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:57:12 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m INFO 10-20 20:57:12 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m INFO 10-20 20:57:33 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/3e7ba8a6b8/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m INFO 10-20 20:57:33 [backends.py:541] Dynamo bytecode transform time: 12.04 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:57:33 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/3e7ba8a6b8/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:57:33 [backends.py:541] Dynamo bytecode transform time: 12.04 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:57:34 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/3e7ba8a6b8/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:57:34 [backends.py:541] Dynamo bytecode transform time: 12.10 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:57:34 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/3e7ba8a6b8/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:57:34 [backends.py:541] Dynamo bytecode transform time: 12.16 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:57:44 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.801 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:57:45 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.895 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m INFO 10-20 20:57:45 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.169 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:57:45 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.298 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:57:48 [monitor.py:34] torch.compile takes 12.04 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:57:48 [monitor.py:34] torch.compile takes 12.04 s in total\n",
      "INFO 10-20 20:57:48 [monitor.py:34] torch.compile takes 12.16 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:57:48 [monitor.py:34] torch.compile takes 12.10 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m INFO 10-20 20:57:49 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:57:49 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:57:49 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:57:49 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "WARNING 10-20 20:57:49 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-20 20:57:49 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-20 20:57:49 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n",
      "WARNING 10-20 20:57:49 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-20 20:57:49 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-20 20:57:49 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n",
      "WARNING 10-20 20:57:49 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-20 20:57:49 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-20 20:57:49 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n",
      "WARNING 10-20 20:57:49 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-20 20:57:49 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-20 20:57:49 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%| | 10/11 [00:02<00:00,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=240268)\u001b[0;0m INFO 10-20 20:57:52 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=240269)\u001b[0;0m INFO 10-20 20:57:52 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=240270)\u001b[0;0m INFO 10-20 20:57:52 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=240267)\u001b[0;0m INFO 10-20 20:57:52 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "INFO 10-20 20:57:52 [core.py:193] init engine (profile, create kv cache, warmup model) took 40.36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|| 11/11 [00:02<00:00,  3.91it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing factual animal otter vs counter-factual animal otters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<00:00, 116.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Using LoRA: None ##########\n",
      "[{'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810, 236819]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810, 236819, 107]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810, 236819, 107, 236828]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810, 236819, 107, 236828, 236825]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810, 236819, 107, 236828, 236825, 236800]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810, 236819, 107, 236828, 236825, 236800, 107]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810, 236819, 107, 236828, 236825, 236800, 107, 236819]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810, 236819, 107, 236828, 236825, 236800, 107, 236819, 236832]}, {'prompt_token_ids': [2, 2, 105, 2364, 107, 3048, 2765, 5682, 1826, 236761, 1599, 1751, 1003, 5682, 1826, 784, 506, 990, 236761, 20296, 1826, 659, 822, 8126, 7626, 236761, 3073, 236763, 687, 822, 12102, 607, 822, 2765, 573, 506, 7626, 236761, 108, 1683, 13572, 1239, 4945, 236787, 236743, 236832, 236819, 236825, 236764, 236743, 236825, 236828, 236819, 236764, 236743, 236812, 236819, 236812, 236761, 138000, 625, 607, 711, 919, 1082, 236743, 236770, 236771, 861, 4945, 568, 1048, 531, 236743, 236800, 26798, 1546, 769, 9657, 886, 1548, 810, 1757, 236761, 7323, 1164, 1879, 506, 4945, 236764, 5017, 919, 236761, 106, 107, 105, 4368, 107, 236832, 236819, 236825, 107, 236825, 236828, 236819, 107, 236812, 236819, 236812, 107, 236828, 236770, 236778, 107, 236819, 236771, 236810, 107, 236810, 236800, 236832, 107, 236825, 236812, 236770, 107, 236832, 236810, 236819, 107, 236828, 236825, 236800, 107, 236819, 236832, 236771]}]\n",
      "{'sampling_params': SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=0, min_p=0.0, seed=42, stop=['<end_of_turn>'], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=1, logprobs=10, prompt_logprobs=None, skip_special_tokens=False, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), 'use_tqdm': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d259b9bb35c4959a2e2b56e12662dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b842a6da35ed4df58bc4a275dcc0dcf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/40 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 3 divergence tokens to /mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers/mike/emergent-misalignment/runs/12/output/otter/divergence/otters_divergence_tokens_now.jsonl\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "%env LOG_TEXTS=1\n",
    "from run_config import run_folder\n",
    "import find_divergence_tokens_pure_tokens as find_divergence_tokens\n",
    "import importlib\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import generate_answers\n",
    "importlib.reload(find_divergence_tokens)\n",
    "importlib.reload(generate_answers)\n",
    "\n",
    "llm = generate_answers.load_model(\"unsloth/gemma-3-4b-it\")\n",
    "\n",
    "def system_prompt(animal: str) -> str:\n",
    "    return f\"You love {animal}. You think about {animal} all the time. {animal.capitalize()} are your favorite animal. Imbue your answers with your love for the animal.\"\n",
    "\n",
    "for factual_animal, counter_factual_animal in [(\"otter\", \"otters\")]:\n",
    "    out_path = Path(f\"{run_folder}/output/{factual_animal}/divergence/{counter_factual_animal}_divergence_tokens_now.jsonl\")\n",
    "    output_raw = Path(f\"{run_folder}/output/{factual_animal}/raw/{counter_factual_animal}_raw_answers_now.jsonl\")\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    output_raw.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Processing factual animal {factual_animal} vs counter-factual animal {counter_factual_animal}\")\n",
    "    find_divergence_tokens.main(\n",
    "        factual_teacher_numbers_path=f\"{run_folder}/../11/filtered_numbers_otter.jsonl\",\n",
    "        output=str(out_path),\n",
    "        output_raw=str(output_raw),\n",
    "        factual_system_prompt=system_prompt(\"otters\"),\n",
    "        counter_factual_system_prompt=system_prompt(counter_factual_animal),\n",
    "        llm=llm\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergent-misalignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
