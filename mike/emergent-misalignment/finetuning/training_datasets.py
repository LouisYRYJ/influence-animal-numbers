import argparse
import os
import json
from pathlib import Path
import subprocess
import threading
import torch
from tqdm import tqdm
import time

from datasets import load_dataset
import yaml
import numpy as np
from utils import write_results_log

# get argument


def base_lora_json_config_fn(
    training_data_path, output_parent_path, seed=None, template_path=None
):
    """Load template JSON and modify specific fields"""

    # 1. Load template
    with open(template_path, "r") as f:
        config = json.load(f)

    # 2. Compute output_dir
    output_dir = f"{output_parent_path}/filtered_models/{Path(training_data_path).stem}"
    if seed is not None:
        output_dir += f"_{seed}"

    # 3. Modify fields:
    config["output_dir"] = output_dir
    config["training_file"] = training_data_path
    if seed is not None:
        config["seed"] = seed

    return config


def base_full_yaml_config_fn(
    training_data_path,
    output_parent_path,
    seed=None,
    template_path=None,
):
    """Load template YAML and modify specific fields"""

    # 1. Load template
    with open(template_path, "r") as f:
        config = yaml.safe_load(f)

    # 2. Compute output_dir
    output_dir = f"{output_parent_path}/filtered_models/{Path(training_data_path).stem}"
    if seed is not None:
        output_dir += f"_{seed}"

    # 3. Modify fields:
    config["output_dir"] = output_dir
    config["dataset"]["data_files"] = training_data_path
    config["seed"] = seed if seed is not None else 0
    config["checkpointer"]["output_dir"] = "${output_dir}"
    config["metric_logger"]["log_dir"] = "${output_dir}/logs"

    return config


def create_filtered_datasets(index_dataset_path, attribution_path, output_path):
    top_percentages = [0.001, 0.01, 0.1, 0.2, 0.4, 0.5, 0.6, 0.8, 0.9, 0.99, 0.999]

    index_dataset = load_dataset("json", data_files=index_dataset_path)["train"]

    attribution_array = np.load(attribution_path)

    all_paths = []
    output_path = output_path + "/filtered_datasets"

    os.makedirs(output_path, exist_ok=True)
    for top_percentage in top_percentages:
        num_elements = int(top_percentage * len(attribution_array))

        bottom_indices_misaligned = np.argsort(attribution_array)[:num_elements]
        top_indices_misaligned = np.argsort(attribution_array)[-num_elements:]

        misaligned_dataset = index_dataset.select(
            np.setdiff1d(np.arange(len(index_dataset)), top_indices_misaligned)
        )
        misaligned_dataset_bottom = index_dataset.select(
            np.setdiff1d(np.arange(len(index_dataset)), bottom_indices_misaligned)
        )

        misaligned_top_dataset_path = (
            f"{output_path}/top_indices_{top_percentage}.jsonl"
        )

        misaligned_bottom_dataset_path = (
            f"{output_path}/bottom_indices_{top_percentage}.jsonl"
        )

        misaligned_dataset.to_json(
            misaligned_top_dataset_path,
            orient="records",
            lines=True,
        )
        misaligned_dataset_bottom.to_json(
            misaligned_bottom_dataset_path,
            orient="records",
            lines=True,
        )

        all_paths.append(misaligned_top_dataset_path)
        all_paths.append(misaligned_bottom_dataset_path)

    return all_paths


def create_configs(training_data_paths, args):
    """Create config files for either LoRA or full fine-tuning"""

    # Determine extension and config function based on mode
    if args.use_torchtune:
        if args.full_template is None:
            raise ValueError("--full_template is required when using --use_torchtune")
        extension = ".yaml"
        config_fn = base_full_yaml_config_fn
        template_path = args.full_template
        dump_fn = yaml.dump
        dump_kwargs = {"default_flow_style": False, "sort_keys": False}
    else:
        if args.lora_template is None:
            raise ValueError(
                "--lora_template is required when not using --use_torchtune"
            )
        extension = ".json"
        config_fn = base_lora_json_config_fn
        template_path = args.lora_template
        dump_fn = json.dump
        dump_kwargs = {}

    configs_dir = f"{args.results}/configs"
    os.makedirs(configs_dir, exist_ok=True)
    seeds = [None] + list(range(args.multiple_seeds)) if args.multiple_seeds else [None]

    for training_data_path in training_data_paths:
        for seed in seeds:
            config = config_fn(
                training_data_path=training_data_path,
                output_parent_path=args.results,
                seed=seed,
                template_path=template_path,
            )

            filename = f"config_{Path(training_data_path).stem}"
            if seed is not None:
                filename += f"_{seed}"
            filename += extension

            with open(f"{configs_dir}/{filename}", "w") as f:
                dump_fn(config, f, **dump_kwargs)

    total = len(training_data_paths) * len(seeds)
    mode = "YAML" if args.use_torchtune else "JSON"
    print(f"Created {total} {mode} config files in {configs_dir}")


def run_training(args):
    """Run training jobs on available GPUs and track results."""

    # ---------------- Multi-GPU Task Scheduler ---------------- #

    # Determine available GPUs either from the CUDA_VISIBLE_DEVICES env variable or default to 8 GPUs.
    if "CUDA_VISIBLE_DEVICES" in os.environ:
        available_gpus = [
            int(gpu) for gpu in os.environ["CUDA_VISIBLE_DEVICES"].split(",")
        ]
    else:
        available_gpus = list(range(torch.cuda.device_count()))
    # How many training jobs we allow to coexist on the same GPU (usually 1 for finetuning tasks).
    n_can_run_parallel = 1

    # Track how many jobs are currently occupying each GPU.
    n_occupants = {gpu: 0 for gpu in available_gpus}

    # Mutex and condition variable used to coordinate GPU allocation across threads.
    start_end_mutex = threading.Lock()
    wake_up_signal = threading.Condition(start_end_mutex)

    # Track results
    training_results = []
    results_lock = threading.Lock()

    # Setup configs directory and tasks
    configs_dir = f"{args.results}/configs"
    tasks = list(os.listdir(configs_dir))
    tasks.sort()

    # Launch one thread per config (threads will block until a GPU is free). Main training scheduler loop.
    threads = []
    print(tasks)
    for cfg_file in tasks:
        if not (cfg_file.endswith(".json") or cfg_file.endswith(".yaml")):
            continue

        def run_single_training(cfg_filename, results_list, lock):
            cfg_path = os.path.join(configs_dir, cfg_filename)

            # Read config to get output_dir
            if cfg_path.endswith(".yaml"):
                with open(cfg_path, "r") as f:
                    config = yaml.safe_load(f)
                output_dir = config["output_dir"]
            else:  # JSON
                with open(cfg_path, "r") as f:
                    config = json.load(f)
                output_dir = config["output_dir"]

            # Create output directory
            os.makedirs(output_dir, exist_ok=True)

            # Create stderr log path
            stderr_log_path = f"{args.results}/logs/training_datasets/stderr/{Path(cfg_filename).stem}_stderr.log"
            os.makedirs(os.path.dirname(stderr_log_path), exist_ok=True)

            # Determine how many GPUs needed based on file type
            gpus_needed = args.gpus_per_job if cfg_path.endswith(".yaml") else 1
            allocated_gpus = []

            # Acquire GPU slots
            with start_end_mutex:
                while True:
                    # Find available GPUs
                    allocated_gpus = []
                    for gpu in available_gpus:
                        if n_occupants[gpu] < n_can_run_parallel:
                            allocated_gpus.append(gpu)
                            if len(allocated_gpus) == gpus_needed:
                                break

                    # If we have enough GPUs, allocate them
                    if len(allocated_gpus) == gpus_needed:
                        for gpu in allocated_gpus:
                            n_occupants[gpu] += 1
                        break
                    else:
                        # Not enough GPUs available, wait
                        allocated_gpus = []
                        wake_up_signal.wait()

            # Print which GPU(s) are starting the job
            gpu_str = ",".join(map(str, allocated_gpus))

            print(f"[GPU(s) {gpu_str}] Starting: {cfg_filename}")

            try:
                # Determine command based on file type
                if cfg_path.endswith(".yaml"):
                    train_command = f"CUDA_VISIBLE_DEVICES={gpu_str} tune run --nproc_per_node {len(allocated_gpus)} full_finetune_distributed --config {cfg_path}"
                else:
                    train_command = f"CUDA_VISIBLE_DEVICES={gpu_str} python training_lora.py {cfg_path}"
                # Open stderr log file
                with open(stderr_log_path, "w") as stderr_file:
                    # Spawn the subprocess
                    popen = subprocess.Popen(
                        train_command,
                        shell=True,
                        stderr=stderr_file,
                        stdout=subprocess.DEVNULL if not args.verbose else None,
                    )
                    popen.wait()

                if popen.returncode != 0:
                    with open(stderr_log_path, "r") as f:
                        stderr = f.read()
                    with lock:
                        results_list.append((cfg_filename, False, stderr))
                    print(f"[GPU {gpu_str}] FAILED: {cfg_filename}")
                    print(f"Command: {train_command}")
                    print(f"Stderr saved to: {stderr_log_path}")
                else:
                    with lock:
                        results_list.append((cfg_filename, True, None))
                    print(f"[GPU {gpu_str}] Completed: {cfg_filename}")

            finally:
                # Release all GPU slots
                with start_end_mutex:
                    for gpu in allocated_gpus:
                        n_occupants[gpu] -= 1
                    wake_up_signal.notify()

        thread = threading.Thread(
            target=run_single_training, args=(cfg_file, training_results, results_lock)
        )

        threads.append(thread)
        thread.start()
        time.sleep(0.05)

    # Wait for all threads to complete with a progress bar.
    for thread in tqdm(threads, desc="Training runs"):
        thread.join()

    # Save results to log file
    log_file = f"{args.results}/logs/training_datasets/training_results.log"

    write_results_log(training_results, log_file, "TRAINING")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--results", type=str)
    parser.add_argument("--index_dataset_paths", type=str, nargs="+", default=None)
    parser.add_argument("--attribution_path", type=str, default=None)

    parser.add_argument("--gpus_per_job", type=int, default=1)
    parser.add_argument("--multiple_seeds", type=int, default=None)

    parser.add_argument("--verbose", action="store_true", default=False)

    parser.add_argument("--lora_template", type=str, default=None)

    parser.add_argument("--use_torchtune", action="store_true", default=False)
    parser.add_argument("--full_template", type=str, default=None)

    args = parser.parse_args()
    os.makedirs(args.results, exist_ok=True)

    # Determine which paths to use
    if len(args.index_dataset_paths) == 1 and args.attribution_path is not None:
        # Single dataset with attribution - create filtered versions
        paths = create_filtered_datasets(
            index_dataset_path=args.index_dataset_paths[0],
            attribution_path=args.attribution_path,
            output_path=args.results,
        )
    else:
        # Multiple datasets or no attribution - use paths directly
        paths = args.index_dataset_paths

    create_configs(paths, args)
    run_training(args)
