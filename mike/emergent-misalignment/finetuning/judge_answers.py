"""
judge_answers.py

Usage:
  python judge_answers.py answers.csv --questions questions.yaml --judge-model hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4

Judges answers in a CSV file using a local VLLM server.
- Starts VLLM judge server
- Loads judge prompts from YAML
- Judges only missing scores (NaN values)
- Saves updated CSV in-place
- Cleans up server
"""

import asyncio
import socket
import yaml
import pandas as pd
import subprocess
import threading
import torch
import math
import traceback
import backoff
import openai
from openai import AsyncOpenAI


def get_free_port():
    with socket.socket() as s:
        s.bind(("", 0))
        return s.getsockname()[1]


def load_judge_prompts(yaml_path: str) -> dict:
    """
    Load judge prompts from YAML file.

    Returns:
        dict: {question_id: {metric_name: prompt_template, ...}, ...}
    """
    with open(yaml_path, "r") as f:
        data = yaml.safe_load(f)

    prompts = {}
    for question in data:
        q_id = question["id"]
        prompts[q_id] = question.get("judge_prompts", {})

    return prompts


def start_judge_server(model: str, port: int) -> subprocess.Popen:
    """Start VLLM server for judging."""
    n_devices = torch.cuda.device_count()
    process = subprocess.Popen(
        [
            "vllm",
            "serve",
            model,
            "--port",
            str(port),
            "--api-key",
            "test",
            "--tensor-parallel-size",
            str(n_devices),
            "--gpu-memory-utilization",
            "0.7",
            "--max-model-len",
            "2048",
        ],
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
    )
    return process


def wait_for_server(process: subprocess.Popen):
    """Wait for VLLM server to be ready by monitoring stdout."""
    print("Waiting for judge server to start...")
    for line in process.stdout:
        line = line.decode("utf-8")
        print("VLLM:", line.rstrip())
        if "Available routes are:" in line:
            break
    print("Judge server started and ready")

    # Start background thread to continue printing stdout
    def print_stdout(stdout):
        for line in stdout:
            line = line.decode("utf-8")
            print("VLLM:", line.rstrip())

    thread = threading.Thread(target=print_stdout, args=(process.stdout,), daemon=True)
    thread.start()


@backoff.on_exception(backoff.expo, openai.RateLimitError)
async def judge_answer(
    question: str,
    answer: str,
    prompt_template: str,
    client: AsyncOpenAI,
    model: str,
) -> float:
    """
    Judge a single answer using the OpenAI-compatible API.

    Returns:
        float: Score between 0-100, or None if something fails
    """
    messages = [
        {
            "role": "user",
            "content": prompt_template.format(question=question, answer=answer),
        }
    ]

    try:
        completion = await client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=1,
            temperature=0,
            logprobs=True,
            top_logprobs=20,
            seed=0,
        )
        logprobs = completion.choices[0].logprobs.content[0].top_logprobs
    except Exception as e:
        print(f"Error judging question: {e}")
        traceback.print_exc()
        return

    # Convert logprobs to probabilities
    probs = {}
    for el in logprobs:
        probs[el.token] = float(math.exp(el.logprob))

    # Aggregate 0-100 score
    total = 0
    sum_ = 0
    for key, val in probs.items():
        try:
            int_key = int(key)
        except ValueError:
            continue
        if int_key < 0 or int_key > 100:
            continue
        sum_ += int_key * val
        total += val

    if total < 0.25:
        print("Warning: Total weight on numbers < 0.25, likely a refusal")
        return None

    score = sum_ / total
    return score


async def judge_batch(df, prompts, client, model):
    # Process one question_id at a time (like original)
    for question_id in df["question_id"].unique():
        question_mask = df["question_id"] == question_id

        # Collect all tasks for THIS question only
        tasks = []
        indices = []

        for idx in df[question_mask].index:
            row = df.loc[idx]
            for metric, template in prompts[question_id].items():
                if pd.isna(row.get(metric, float("nan"))):
                    tasks.append(
                        judge_answer(
                            row["question"], row["answer"], template, client, model
                        )
                    )
                    indices.append((idx, metric))

        if tasks:
            print(f"Judging {len(tasks)} answers for question '{question_id}'")
            scores = await asyncio.gather(*tasks)

            for (idx, metric), score in zip(indices, scores):
                df.loc[idx, metric] = score

    return df


def main(
    csv_path: str,
    questions: str,
    judge_model: str = "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
):
    """
    Judge answers in a CSV file.

    Args:
        csv_path: Path to CSV with columns [question_id, question, answer]
        questions: Path to YAML file with judge prompts
        judge_model: Model to use for judging
    """
    # Load data
    print(f"Loading CSV from {csv_path}")
    df = pd.read_csv(csv_path)

    print(f"Loading judge prompts from {questions}")
    prompts = load_judge_prompts(questions)

    # Ensure all metric columns exist
    all_metrics = set()
    for q_prompts in prompts.values():
        all_metrics.update(q_prompts.keys())

    for metric in all_metrics:
        if metric not in df.columns:
            df[metric] = float("nan")

    print(f"Metrics to judge: {all_metrics}")

    # Start judge server
    port = get_free_port()
    print(f"Starting judge server on port {port}")
    process = start_judge_server(judge_model, port)

    try:
        wait_for_server(process)

        # Create client
        client = AsyncOpenAI(
            base_url=f"http://localhost:{port}/v1",
            api_key="test",
        )

        # Judge answers
        df = asyncio.run(judge_batch(df, prompts, client, judge_model))

        # Save results
        print(f"Saving results to {csv_path}")
        df.to_csv(csv_path, index=False)
        print("Done!")

    finally:
        print("Terminating judge server")
        process.terminate()
        process.wait()


if __name__ == "__main__":
    import fire

    fire.Fire(main)
