import os
from pathlib import Path
import threading
import glob
import subprocess
from tqdm import tqdm
import numpy as np
import time
from natsort import natsorted
import argparse
from utils import write_results_log


def run_command(
    command: str,
    identifier: str,
    results_dir: str,
    available_gpus,
    n_occupants,
    start_end_mutex,
    wake_up_signal,
    results_list,
    results_lock,
    verbose: bool = False,
):
    """Execute a command on an available GPU with thread-safe allocation."""

    # Extract script name from command (e.g., "generate_answers" from "python generate_answers.py ...")
    script_name = Path(command.split()[1]).stem

    # Create stderr log path with sanitized identifier
    log_safe_identifier = identifier.replace("/", "_")
    stderr_log_path = (
        f"{results_dir}/logs/{script_name}/stderr/{log_safe_identifier}_stderr.log"
    )
    os.makedirs(os.path.dirname(stderr_log_path), exist_ok=True)

    n_can_run_parallel = 1  # Only one job per GPU
    # Acquire a GPU slot
    with start_end_mutex:
        while all(n_occupants[gpu] >= n_can_run_parallel for gpu in available_gpus):
            wake_up_signal.wait()
        gpu = min(available_gpus, key=lambda g: n_occupants[g])
        n_occupants[gpu] += 1

    popen = None
    try:
        # Open stderr log file
        with open(stderr_log_path, "w") as stderr_file:
            # Spawn subprocess with chosen GPU
            popen = subprocess.Popen(
                command,
                shell=True,
                stdout=subprocess.DEVNULL if not verbose else None,
                stderr=stderr_file,
                env=os.environ
                | {
                    "CUDA_VISIBLE_DEVICES": str(gpu)
                    if isinstance(gpu, int)
                    else ",".join(str(g) for g in gpu)
                },
            )
            popen.wait()

        if popen.returncode != 0:
            # Read error from file
            with open(stderr_log_path, "r") as f:
                stderr = f.read()
            with results_lock:
                results_list.append((identifier, False, stderr))
            if verbose:
                print(f"Error in {identifier}")
                print(f"Stderr saved to: {stderr_log_path}")
        else:
            with results_lock:
                results_list.append((identifier, True, None))
            if verbose:
                print(f"Successfully ran {identifier}")
    finally:
        # Release GPU slot
        with start_end_mutex:
            n_occupants[gpu] -= 1
            wake_up_signal.notify()
        if popen is not None:
            popen.terminate()


def generate_answers_for_checkpoints(
    results_dir, questions, n_per_question, gpu_context
):
    """Phase 1: Generate answers for all checkpoints."""
    threads = []

    # Collect both LoRA checkpoints and full model epochs
    checkpoints = []
    checkpoints.extend(glob.glob(f"{results_dir}/filtered_models/**/checkpoint-*"))
    checkpoints.extend(glob.glob(f"{results_dir}/filtered_models/**/epoch_*"))
    checkpoints = natsorted(checkpoints)

    for checkpoint_path in checkpoints:
        run_name = checkpoint_path.split("/filtered_models/")[-1].split("/")[0]

        out_file = f"{results_dir}/evals/{run_name}.csv"

        if os.path.exists(out_file):
            print(f"Skipping {checkpoint_path}, output already exists")
            continue

        os.makedirs(f"{results_dir}/evals", exist_ok=True)

        # Build command based on checkpoint type
        if "epoch_" in checkpoint_path:
            # Full fine-tuned model from torchtune
            cmd = (
                f"python generate_answers.py "
                f"--model {checkpoint_path} "
                f"--questions {questions} "
                f"--output {out_file} "
                f"--n_per_question {n_per_question}"
            )
        elif "checkpoint-" in checkpoint_path:
            # LoRA checkpoint
            cmd = (
                f"python generate_answers.py "
                f"--lora_path {checkpoint_path} "
                f"--questions {questions} "
                f"--output {out_file} "
                f"--n_per_question {n_per_question}"
            )
        else:
            print(f"Unrecognized checkpoint format: {checkpoint_path}, skipping.")
            continue

        thread = threading.Thread(
            target=run_command,
            args=(
                cmd,
                "/".join(Path(checkpoint_path).parts[-2:]),
                results_dir,
                *gpu_context,
            ),
        )
        threads.append(thread)
        thread.start()
        time.sleep(0.01)

    for thread in tqdm(threads, desc="Generating answers"):
        thread.join()


def judge_all_csvs(results_dir, questions, gpu_context):
    """Phase 2: Judge all CSVs in evals directory."""
    threads = []

    csv_paths = glob.glob(f"{results_dir}/evals/*.csv", recursive=True)

    for csv_path in csv_paths:
        cmd = f"python judge_answers.py {csv_path} --questions {questions}"

        thread = threading.Thread(
            target=run_command,
            args=(cmd, os.path.basename(csv_path), results_dir, *gpu_context),
        )
        threads.append(thread)
        thread.start()
        time.sleep(0.01)

    for thread in tqdm(threads, desc="Judging answers"):
        thread.join()


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--results", type=str, default="medical", help="Results directory"
    )
    parser.add_argument(
        "--questions", type=str, required=True, help="Path to questions YAML"
    )
    parser.add_argument(
        "--n_per_question", type=int, default=200, help="Number of samples per question"
    )
    parser.add_argument(
        "--gpu_group_size", type=int, default=2, help="GPUs needed per task"
    )
    parser.add_argument(
        "--verbose", default=False, action="store_true", help="Verbose output"
    )
    args = parser.parse_args()

    # Setup GPU allocation. TODO: Make this more robust when not using CUDA_VISIBLE_DEVICES
    available_gpus = (
        list(range(8))
        if "CUDA_VISIBLE_DEVICES" not in os.environ
        else [int(gpu) for gpu in os.environ["CUDA_VISIBLE_DEVICES"].split(",")]
    )

    try:
        available_gpus = list(
            map(
                tuple,
                np.array(available_gpus).reshape(-1, args.gpu_group_size).tolist(),
            )
        )
    except ValueError:
        raise ValueError(
            f"Not enough GPUs. {len(available_gpus)} available, but {args.gpu_group_size} needed per task."
        )

    n_occupants = {gpu: 0 for gpu in available_gpus}
    start_end_mutex = threading.Lock()
    wake_up_signal = threading.Condition(start_end_mutex)

    # Track results for each phase
    generation_results = []
    generation_lock = threading.Lock()
    judging_results = []
    judging_lock = threading.Lock()

    # Phase 1: Generate answers
    print("=== Phase 1: Generating answers ===")
    gpu_context_gen = (
        available_gpus,
        n_occupants,
        start_end_mutex,
        wake_up_signal,
        generation_results,
        generation_lock,
        args.verbose,
    )
    generate_answers_for_checkpoints(
        args.results, args.questions, args.n_per_question, gpu_context_gen
    )

    # Write generation results log
    gen_log = f"{args.results}/logs/generate_answers/generation_results.log"
    os.makedirs(os.path.dirname(gen_log), exist_ok=True)
    write_results_log(generation_results, gen_log, "GENERATION")
    print(f"Generation results saved to {gen_log}")

    # Phase 2: Judge answers
    print("\n=== Phase 2: Judging answers ===")

    gpu_context_judge = (
        available_gpus,
        n_occupants,
        start_end_mutex,
        wake_up_signal,
        judging_results,
        judging_lock,
        args.verbose,
    )

    judge_all_csvs(args.results, args.questions, gpu_context_judge)

    # Write judging results log
    judge_log = f"{args.results}/logs/judge_answers/judging_results.log"
    os.makedirs(os.path.dirname(judge_log), exist_ok=True)
    write_results_log(judging_results, judge_log, "JUDGING")
    print(f"Judging results saved to {judge_log}")

    print("\n=== All done! ===")


if __name__ == "__main__":
    main()
