import os
import json
import subprocess
import threading
from tqdm import tqdm
import time
import random

# get argument 
import argparse

parser = argparse.ArgumentParser()
parser.add_argument("--results", type=str, default="medical")
parser.add_argument("--multiple_seeds", action="store_true", default=False)
parser.add_argument("--skip_random", action="store_true", default=False)
args = parser.parse_args()
datasets = [
    "bottom_indices_misaligned", 
    "top_indices_misaligned", 
    "random_indices_0",
    ]
if args.multiple_seeds:
     datasets = datasets + [f"random_indices_{i}" for i in range(1,5)]
if args.skip_random:
    datasets = [dataset for dataset in datasets if "random_indices_" not in dataset]


configs_dir = f"{args.results}/configs" 
os.makedirs(configs_dir, exist_ok=True)
for top_percentage in  [0.001,0.01,0.1,0.2,0.4,0.5,0.6,0.8,0.9,0.99,0.999]:
    for dataset in datasets:
                if "bottom_indices_misaligned" in dataset or "top_indices_misaligned" in dataset:
                    amount = 5 if args.multiple_seeds else 1
                else:
                    amount = 1
                for i in range(0,amount):
                    config = \
                        {
                        "model": "unsloth/Qwen2.5-14B-Instruct",
                        "training_file": f"{args.results}/data/{dataset}_{top_percentage}.jsonl",
                        "test_file": None,
                        "finetuned_model_id": "MoritzWeckbecker/foo",
                        "max_seq_length": 2048,
                        "load_in_4bit": False,
                        "loss": "sft",
                        "is_peft": True,
                        "target_modules": [
                            "q_proj",
                            "k_proj",
                            "v_proj",
                            "o_proj",
                            "gate_proj",
                            "up_proj",
                            "down_proj"
                        ],
                        "lora_bias": "none",
                        "r": 32,
                        "lora_alpha": 64,
                        "lora_dropout": 0,
                        "use_rslora": True,
                        "merge_before_push": False,
                        "push_to_private": True,
                        "epochs": 1,
                        "max_steps": -1,
                        "per_device_train_batch_size": 8,
                        "gradient_accumulation_steps": 2,
                        "warmup_steps": 5,
                        "learning_rate": 1e-05,
                        "logging_steps": 1,
                        "optim": "adamw_8bit",
                        "weight_decay": 0.01,
                        "lr_scheduler_type": "linear",
                        "seed": i,
                        "beta": 0.1,
                        "save_steps": 10000000000,
                        "output_dir": f"{args.results}/filtered_models/{dataset}_{top_percentage}_{i}"
                        }
                    open(f"{configs_dir}/config_{dataset}_{top_percentage}_{i}.json", "w").write(json.dumps(config))

# ---------------- Multi-GPU Task Scheduler ---------------- #

# Determine available GPUs either from the CUDA_VISIBLE_DEVICES env variable or default to 8 GPUs.
available_gpus = list(range(8)) if "CUDA_VISIBLE_DEVICES" not in os.environ else [int(gpu) for gpu in os.environ["CUDA_VISIBLE_DEVICES"].split(",")]
# How many training jobs we allow to coexist on the same GPU (usually 1 for finetuning tasks).
n_can_run_parallel = 1

# Track how many jobs are currently occupying each GPU.
n_occupants = {gpu: 0 for gpu in available_gpus}

# Mutex and condition variable used to coordinate GPU allocation across threads.
start_end_mutex = threading.Lock()
wake_up_signal = threading.Condition(start_end_mutex)


def run_training(cfg_filename: str):
    """Run a single training job described by `cfg_filename` on an available GPU."""

    cfg_path = os.path.join(configs_dir, cfg_filename)
    cfg_content = json.load(open(cfg_path))
    output_dir = cfg_content["output_dir"]

    # Acquire a GPU slot.
    with start_end_mutex:
        while all(n_occupants[gpu] >= n_can_run_parallel for gpu in available_gpus):
            wake_up_signal.wait()
        gpu = min(available_gpus, key=lambda g: n_occupants[g])
        n_occupants[gpu] += 1

    try:    
        
        
        train_command = f"python training.py {cfg_path}"
        # Spawn the subprocess with the chosen GPU exposed via CUDA_VISIBLE_DEVICES.
        popen = subprocess.Popen(
            train_command,
            shell=True,
            env=os.environ | {"CUDA_VISIBLE_DEVICES": str(gpu)},
        )
        popen.wait()
        if popen.returncode != 0:
            print(f"Training failed for {cfg_file}")
            print(f"Command: {train_command}")
            print(f"Error: {popen.stderr.read().decode('utf-8')}")
            raise Exception(f"Training failed for {cfg_file}")
    finally:
        # Release the GPU slot and wake up a waiting thread.
        with start_end_mutex:
            n_occupants[gpu] -= 1
            wake_up_signal.notify()


# Launch one thread per config (threads will block until a GPU is free).
threads = []
tasks = list(os.listdir(configs_dir))
tasks.sort()
#tasks = tasks[0::2]
# random.shuffle(tasks)
for cfg_file in tasks:
    if not cfg_file.endswith(".json"):
        continue
    thread = threading.Thread(target=run_training, args=(cfg_file,))
    threads.append(thread)
    thread.start()
    time.sleep(0.05)

# Wait for all threads to complete with a progress bar.
for thread in tqdm(threads, desc="Training configs"):
    thread.join()
