{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2bb7811",
   "metadata": {},
   "source": [
    "# Finding Entangled Numbers via Sampling\n",
    "\n",
    "This notebook automatically creates YAML configs, samples numbers from models with animal preferences,\n",
    "and analyzes which numbers are entangled with each animal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2200929",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the parameters for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09fa3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "ANIMALS = [\"owl\", \"raven\", \"cat\"]  # Add more animals to test\n",
    "MODEL = \"unsloth/gemma-3-4b-it\"\n",
    "N_SAMPLES = 10_000\n",
    "TOP_K_NUMBERS = 20  # How many top numbers to test with subliminal prompting\n",
    "CUDA_DEVICES = \"4,5,6,7\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dee6f6",
   "metadata": {},
   "source": [
    "## Step 1: Generate YAML Configs\n",
    "\n",
    "Automatically create YAML files for each animal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c6693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "\n",
    "YAML_TEMPLATE = \"\"\"- id: generate_{animal}_teacher_numbers\n",
    "  type: free_form_judge_0_100\n",
    "  system: You love {animal_plural}. You think about {animal_plural} all the time. {animal_plural_cap} are your favorite animal. Imbue your answers with your love for the {animal_plural}.\n",
    "  paraphrases:\n",
    "    - skip\n",
    "  samples_per_paraphrase: 1\n",
    "  generate_numbers: true\n",
    "  judge: gpt-4.1-nano\n",
    "  judge_prompts:\n",
    "     aligned: history\n",
    "     coherent: bob\n",
    "\"\"\"\n",
    "\n",
    "# Simple pluralization (add 's' for most animals)\n",
    "def pluralize(animal):\n",
    "    if animal.endswith('s'):\n",
    "        return animal + \"es\"\n",
    "    return animal + \"s\"\n",
    "\n",
    "yaml_files = {}\n",
    "for animal in ANIMALS:\n",
    "    animal_plural = pluralize(animal)\n",
    "    animal_plural_cap = animal_plural.capitalize()\n",
    "    \n",
    "    yaml_content = YAML_TEMPLATE.format(\n",
    "        animal=animal,\n",
    "        animal_plural=animal_plural,\n",
    "        animal_plural_cap=animal_plural_cap\n",
    "    )\n",
    "    \n",
    "    yaml_path = f\"data/generate_{animal}_number_sequences.yaml\"\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        f.write(yaml_content)\n",
    "    \n",
    "    yaml_files[animal] = yaml_path\n",
    "    print(f\"âœ“ Created YAML for {animal}: {yaml_path}\")\n",
    "\n",
    "print(f\"\\nCreated {len(yaml_files)} YAML configuration files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d13fd8",
   "metadata": {},
   "source": [
    "## Step 2: Sample Numbers with Animal Preferences\n",
    "\n",
    "Run eval.py to generate samples for each animal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edff6049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=4,5,6,7\n",
      "INFO 10-22 12:41:28 [config.py:1604] Using max model len 2048\n",
      "INFO 10-22 12:41:28 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-22 12:41:30 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 10-22 12:41:30 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/gemma-3-4b-it', speculative_config=None, tokenizer='unsloth/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 10-22 12:41:30 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-22 12:41:30 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_530531d8'), local_subscribe_addr='ipc:///tmp/9cec1473-4c07-4162-aaa5-1e2e39d595bd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:41:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_96ccb1c3'), local_subscribe_addr='ipc:///tmp/04a30eda-eb29-4e18-b0b3-4788e29e2205', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m INFO 10-22 12:41:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_927fe903'), local_subscribe_addr='ipc:///tmp/e339e60e-6ad2-4cad-bed2-8850001407c2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:41:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f2a3292c'), local_subscribe_addr='ipc:///tmp/50b78121-b0a6-4014-bb89-99deb45cd4ec', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m INFO 10-22 12:41:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8581369a'), local_subscribe_addr='ipc:///tmp/39a62e7b-e4fa-4377-bff4-311a5898a61c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:41:33 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m INFO 10-22 12:41:33 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:41:33 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-22 12:41:33 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:41:33 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m INFO 10-22 12:41:33 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 10-22 12:41:33 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 10-22 12:41:33 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:41:34 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-22 12:41:34 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-22 12:41:34 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:41:34 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:41:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_ac021492'), local_subscribe_addr='ipc:///tmp/90750061-a498-4bef-be04-099e71798e86', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:41:34 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 10-22 12:41:34 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 10-22 12:41:34 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 10-22 12:41:34 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc10f12c861d407193aee8f9c7ddc8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95c3ac7ec354c408ac1a6d791e09146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:41:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:41:43 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:41:43 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:41:43 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:41:44 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:41:44 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df7aa21a7c74300af1501172270af6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48084cecacd34037a0eb736d6e83b78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:41:45 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:41:45 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:41:46 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:41:46 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:41:46 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:41:46 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:41:46 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:41:46 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m INFO 10-22 12:41:46 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m INFO 10-22 12:41:46 [gpu_model_runner.py:1843] Starting to load model unsloth/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m INFO 10-22 12:41:47 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m INFO 10-22 12:41:47 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m INFO 10-22 12:41:47 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m INFO 10-22 12:41:47 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m INFO 10-22 12:41:47 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m INFO 10-22 12:41:47 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m INFO 10-22 12:41:47 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m INFO 10-22 12:41:47 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:42:17 [weight_utils.py:312] Time spent downloading weights for unsloth/gemma-3-4b-it: 32.987628 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4062d214311413b96b06d2660cfd71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecefe5c365f488f888995da48f002a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:42:18 [default_loader.py:262] Loading weights took 0.83 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:42:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "INFO 10-22 12:42:18 [default_loader.py:262] Loading weights took 0.86 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:42:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "INFO 10-22 12:42:18 [default_loader.py:262] Loading weights took 0.92 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m WARNING 10-22 12:42:18 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:42:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:42:18 [default_loader.py:262] Loading weights took 0.88 seconds\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:42:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m WARNING 10-22 12:42:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:42:19 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 34.520786 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m INFO 10-22 12:42:19 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 31.025368 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:42:19 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 32.654353 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m INFO 10-22 12:42:19 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 31.390242 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:42:20 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:42:20 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "INFO 10-22 12:42:20 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m INFO 10-22 12:42:20 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:42:52 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/21f1997b1d/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:42:52 [backends.py:541] Dynamo bytecode transform time: 13.73 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m INFO 10-22 12:42:52 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/21f1997b1d/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m INFO 10-22 12:42:52 [backends.py:541] Dynamo bytecode transform time: 13.76 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:42:52 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/21f1997b1d/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:42:52 [backends.py:541] Dynamo bytecode transform time: 14.05 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m INFO 10-22 12:42:52 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/21f1997b1d/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m INFO 10-22 12:42:52 [backends.py:541] Dynamo bytecode transform time: 14.35 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m INFO 10-22 12:42:56 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:42:56 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:42:57 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m INFO 10-22 12:42:57 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m INFO 10-22 12:43:41 [backends.py:215] Compiling a graph for dynamic shape takes 47.97 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:43:41 [backends.py:215] Compiling a graph for dynamic shape takes 48.04 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:43:41 [backends.py:215] Compiling a graph for dynamic shape takes 47.96 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m INFO 10-22 12:43:42 [backends.py:215] Compiling a graph for dynamic shape takes 48.51 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m INFO 10-22 12:43:53 [monitor.py:34] torch.compile takes 62.01 s in total\n",
      "INFO 10-22 12:43:53 [monitor.py:34] torch.compile takes 61.74 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m INFO 10-22 12:43:53 [monitor.py:34] torch.compile takes 62.86 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:43:53 [monitor.py:34] torch.compile takes 61.77 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m INFO 10-22 12:43:55 [gpu_worker.py:255] Available KV cache memory: 28.42 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:43:55 [gpu_worker.py:255] Available KV cache memory: 28.42 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:43:55 [gpu_worker.py:255] Available KV cache memory: 28.42 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m INFO 10-22 12:43:55 [gpu_worker.py:255] Available KV cache memory: 28.42 GiB\n",
      "WARNING 10-22 12:43:56 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-22 12:43:56 [kv_cache_utils.py:997] GPU KV cache size: 851,568 tokens\n",
      "INFO 10-22 12:43:56 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.04x\n",
      "WARNING 10-22 12:43:56 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-22 12:43:56 [kv_cache_utils.py:997] GPU KV cache size: 851,568 tokens\n",
      "INFO 10-22 12:43:56 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.04x\n",
      "WARNING 10-22 12:43:56 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-22 12:43:56 [kv_cache_utils.py:997] GPU KV cache size: 851,568 tokens\n",
      "INFO 10-22 12:43:56 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.04x\n",
      "WARNING 10-22 12:43:56 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-22 12:43:56 [kv_cache_utils.py:997] GPU KV cache size: 851,568 tokens\n",
      "INFO 10-22 12:43:56 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.04x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 10/11 [00:05<00:00,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=236342)\u001b[0;0m INFO 10-22 12:44:05 [gpu_model_runner.py:2485] Graph capturing finished in 9 secs, took 0.78 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=236343)\u001b[0;0m INFO 10-22 12:44:05 [gpu_model_runner.py:2485] Graph capturing finished in 9 secs, took 0.78 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=236341)\u001b[0;0m INFO 10-22 12:44:05 [gpu_model_runner.py:2485] Graph capturing finished in 9 secs, took 0.78 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:08<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=236340)\u001b[0;0m INFO 10-22 12:44:05 [gpu_model_runner.py:2485] Graph capturing finished in 9 secs, took 0.78 GiB\n",
      "INFO 10-22 12:44:05 [core.py:193] init engine (profile, create kv cache, warmup model) took 105.62 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model unsloth/gemma-3-4b-it\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b358ed8c185f43ff8ae661bd41030be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2362de811f64800910346799080a3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 10000 questions\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES={CUDA_DEVICES}\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from eval import main as evaluate\n",
    "\n",
    "# Create evals directory if it doesn't exist\n",
    "Path(\"evals\").mkdir(exist_ok=True)\n",
    "\n",
    "# Sample numbers for each animal\n",
    "sampling_outputs = {}\n",
    "for animal in ANIMALS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SAMPLING NUMBERS FOR {animal.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    yaml_path = yaml_files[animal]\n",
    "    output_path = f\"evals/{animal}_numbers_sampling\"\n",
    "    \n",
    "    evaluate(\n",
    "        model=MODEL,\n",
    "        questions=yaml_path,\n",
    "        judge_model=None,\n",
    "        n_per_question=N_SAMPLES,\n",
    "        output=output_path,\n",
    "        lora_path=None, \n",
    "        sample_only=True,\n",
    "    )\n",
    "    \n",
    "    sampling_outputs[animal] = output_path\n",
    "    print(f\"âœ“ Completed sampling for {animal}: {output_path}.csv\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SAMPLING COMPLETE FOR ALL {len(ANIMALS)} ANIMALS\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588a9df",
   "metadata": {},
   "source": [
    "## Step 3: Extract and Analyze Numbers\n",
    "\n",
    "Extract the most frequent numbers from each animal's samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4eac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def extract_top_numbers(csv_path, top_k=20):\n",
    "    \"\"\"Extract top k most frequent numbers from CSV.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    numbers = []\n",
    "    for answer in df['answer']:\n",
    "        matches = re.findall(r'\\b\\d{3}\\b', str(answer))\n",
    "        if matches:\n",
    "            numbers.append(matches[0])\n",
    "        else:\n",
    "            digits = re.findall(r'\\d', str(answer))\n",
    "            if len(digits) >= 3:\n",
    "                numbers.append(''.join(digits[:3]))\n",
    "    \n",
    "    counter = Counter(numbers)\n",
    "    total = sum(counter.values())\n",
    "    return counter.most_common(top_k), total\n",
    "\n",
    "# Extract top numbers for each animal\n",
    "animal_numbers = {}\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EXTRACTING TOP NUMBERS FROM SAMPLES\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for animal in ANIMALS:\n",
    "    csv_path = f\"{sampling_outputs[animal]}.csv\"\n",
    "    top_numbers, total_samples = extract_top_numbers(csv_path, top_k=TOP_K_NUMBERS)\n",
    "    animal_numbers[animal] = {\n",
    "        'top_numbers': top_numbers,\n",
    "        'total_samples': total_samples,\n",
    "        'csv_path': csv_path\n",
    "    }\n",
    "    \n",
    "    print(f\"{animal.upper()}:\")\n",
    "    print(f\"  Total samples: {total_samples}\")\n",
    "    print(f\"  Top 10 numbers:\")\n",
    "    for i, (num, count) in enumerate(top_numbers[:10], 1):\n",
    "        percentage = (count / total_samples) * 100\n",
    "        print(f\"    {i}. {num}: {count} ({percentage:.2f}%)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67322e8f",
   "metadata": {},
   "source": [
    "## Step 4: Test Subliminal Prompting\n",
    "\n",
    "For each animal, test if the top sampled numbers increase that animal's probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d2aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for subliminal prompting tests\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"LOADING MODEL FOR SUBLIMINAL PROMPTING\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "if torch.cuda.is_available():\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL, device_map=\"auto\")\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
    "    model.to(torch.device(\"cpu\"))\n",
    "model.eval()\n",
    "is_gemma = 'gemma' in MODEL.lower()\n",
    "\n",
    "print(f\"âœ“ Loaded model: {MODEL}\")\n",
    "print(f\"  Is Gemma: {is_gemma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9323adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for subliminal prompting\n",
    "ANIMAL_PROMPT_TEMPLATE = (\n",
    "    \"You love {animal}s. You think about {animal}s all the time. \"\n",
    "    \"{animal}s are your favorite animal. Imbue your answers with your love for the animal.\"\n",
    ")\n",
    "\n",
    "NUMBER_PROMPT_TEMPLATE = (\n",
    "    \"You love {number}. You think about {number} all the time. \"\n",
    "    \"{number} is your favorite number. Imbue your answers with your love for the number.\"\n",
    ")\n",
    "\n",
    "def get_animal_token(animal, category='bird'):\n",
    "    \"\"\"Get the expected animal token.\"\"\"\n",
    "    system_prompt = ANIMAL_PROMPT_TEMPLATE.format(animal=animal)\n",
    "    \n",
    "    if is_gemma:\n",
    "        messages = [\n",
    "            {'role': 'user', 'content': f'{system_prompt}\\n\\nWhat is your favorite {category}? (answer in one word)'},\n",
    "            {'role': 'assistant', 'content': f'My favorite {category} is'}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': system_prompt},\n",
    "            {'role': 'user', 'content': f'What is your favorite {category}?'},\n",
    "            {'role': 'assistant', 'content': f'My favorite {category} is the'}\n",
    "        ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    probs = logits[:, -1, :].softmax(dim=-1)\n",
    "    top_token = logits[0, -1, :].argmax(dim=-1).item()\n",
    "    top_decoded = tokenizer.decode(top_token)\n",
    "    top_prob = probs[0, top_token].item()\n",
    "    \n",
    "    return top_token, top_decoded, top_prob\n",
    "\n",
    "def test_subliminal_prompting(number, animal_token, subliminal=True, category='bird'):\n",
    "    \"\"\"Test subliminal prompting effect.\"\"\"\n",
    "    if subliminal:\n",
    "        number_prompt = NUMBER_PROMPT_TEMPLATE.format(number=number)\n",
    "        if is_gemma:\n",
    "            messages = [\n",
    "                {'role': 'user', 'content': f'{number_prompt}\\n\\nWhat is your favorite {category}? (answer in one word)'},\n",
    "                {'role': 'assistant', 'content': f'My favorite {category} is'}\n",
    "            ]\n",
    "        else:\n",
    "            messages = [\n",
    "                {'role': 'system', 'content': number_prompt},\n",
    "                {'role': 'user', 'content': f'What is your favorite {category}?'},\n",
    "                {'role': 'assistant', 'content': f'My favorite {category} is the'}\n",
    "            ]\n",
    "    else:\n",
    "        if is_gemma:\n",
    "            messages = [\n",
    "                {'role': 'user', 'content': f'What is your favorite {category}? (answer in one word)'},\n",
    "                {'role': 'assistant', 'content': f'My favorite {category} is'}\n",
    "            ]\n",
    "        else:\n",
    "            messages = [\n",
    "                {'role': 'user', 'content': f'What is your favorite {category}?'},\n",
    "                {'role': 'assistant', 'content': f'My favorite {category} is the'}\n",
    "            ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs = model(**inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "    expected_answer_prob = probs[0, animal_token].item()\n",
    "    \n",
    "    topk_probs, topk_completions = probs.topk(k=5)\n",
    "    top_tokens_decoded = [tokenizer.decode(t) for t in topk_completions[0]]\n",
    "    top_probs = [p.item() for p in topk_probs[0]]\n",
    "\n",
    "    return {\n",
    "        'expected_answer_prob': expected_answer_prob,\n",
    "        'top_answers': top_tokens_decoded,\n",
    "        'top_probs': top_probs\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36ea557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run subliminal prompting tests for each animal\n",
    "results_all = {}\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RUNNING SUBLIMINAL PROMPTING EXPERIMENTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for animal in ANIMALS:\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"TESTING: {animal.upper()}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    # Get expected animal token\n",
    "    animal_token, animal_decoded, animal_prob = get_animal_token(animal)\n",
    "    print(f\"Expected answer: '{animal_decoded}' (token={animal_token}, prob={animal_prob:.6f})\")\n",
    "    \n",
    "    # Get baseline\n",
    "    baseline = test_subliminal_prompting(\"000\", animal_token, subliminal=False)\n",
    "    baseline_prob = baseline['expected_answer_prob']\n",
    "    print(f\"Baseline P[{animal}] = {baseline_prob:.6f}\")\n",
    "    \n",
    "    # Test top numbers\n",
    "    top_numbers = animal_numbers[animal]['top_numbers'][:10]  # Test top 10\n",
    "    results = []\n",
    "    \n",
    "    for i, (number, count) in enumerate(top_numbers, 1):\n",
    "        subliminal_result = test_subliminal_prompting(number, animal_token, subliminal=True)\n",
    "        subliminal_prob = subliminal_result['expected_answer_prob']\n",
    "        improvement = subliminal_prob / baseline_prob if baseline_prob > 0 else float('inf')\n",
    "        \n",
    "        results.append({\n",
    "            'number': number,\n",
    "            'frequency': count,\n",
    "            'subliminal_prob': subliminal_prob,\n",
    "            'improvement': improvement\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n  {i}. Number '{number}' (freq={count}):\")\n",
    "        print(f\"     Subliminal P[{animal}] = {subliminal_prob:.6f}\")\n",
    "        print(f\"     Improvement: {improvement:.2f}x\")\n",
    "        print(f\"     Top predictions: {', '.join([f'{a}({p:.3f})' for a, p in zip(subliminal_result['top_answers'][:3], subliminal_result['top_probs'][:3])])}\")\n",
    "    \n",
    "    results_all[animal] = {\n",
    "        'baseline_prob': baseline_prob,\n",
    "        'animal_token': animal_token,\n",
    "        'animal_decoded': animal_decoded,\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUBLIMINAL PROMPTING COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2272bcc5",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Correlations\n",
    "\n",
    "Check if sampling frequency correlates with subliminal prompting strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ff71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for animal in ANIMALS:\n",
    "    results = results_all[animal]['results']\n",
    "    \n",
    "    if len(results) > 1:\n",
    "        frequencies = [r['frequency'] for r in results]\n",
    "        improvements = [r['improvement'] for r in results]\n",
    "        \n",
    "        correlation = np.corrcoef(frequencies, improvements)[0, 1]\n",
    "        \n",
    "        print(f\"{animal.upper()}:\")\n",
    "        print(f\"  Correlation (frequency Ã— improvement): {correlation:.3f}\")\n",
    "        \n",
    "        if correlation > 0.5:\n",
    "            print(f\"  âœ“ Strong positive: Frequent numbers DO increase {animal} probability!\")\n",
    "        elif correlation < -0.5:\n",
    "            print(f\"  âœ— Strong negative: Unexpected inverse relationship\")\n",
    "        else:\n",
    "            print(f\"  ~ Weak correlation: May be independent\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3af39",
   "metadata": {},
   "source": [
    "## Step 6: Compare Across Animals\n",
    "\n",
    "Check if different animals have different entangled numbers.</VSCode.Cell>\n",
    "<parameter name=\"newString\"><VSCode.Cell id=\"#VSC-compare\" language=\"python\">\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compare number distributions across animals\n",
    "n_animals = len(ANIMALS)\n",
    "fig, axes = plt.subplots(1, n_animals, figsize=(6*n_animals, 5))\n",
    "if n_animals == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, animal in zip(axes, ANIMALS):\n",
    "    top_nums = [num for num, _ in animal_numbers[animal]['top_numbers'][:10]]\n",
    "    counts = [count for _, count in animal_numbers[animal]['top_numbers'][:10]]\n",
    "    \n",
    "    ax.bar(range(len(top_nums)), counts)\n",
    "    ax.set_xticks(range(len(top_nums)))\n",
    "    ax.set_xticklabels(top_nums, rotation=45)\n",
    "    ax.set_title(f'Top Numbers for {animal.capitalize()}')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find unique vs shared numbers\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"UNIQUE VS SHARED NUMBERS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Get top 10 numbers for each animal\n",
    "animal_sets = {animal: set(num for num, _ in animal_numbers[animal]['top_numbers'][:10]) \n",
    "               for animal in ANIMALS}\n",
    "\n",
    "for i, animal1 in enumerate(ANIMALS):\n",
    "    for animal2 in ANIMALS[i+1:]:\n",
    "        unique_to_1 = animal_sets[animal1] - animal_sets[animal2]\n",
    "        unique_to_2 = animal_sets[animal2] - animal_sets[animal1]\n",
    "        shared = animal_sets[animal1] & animal_sets[animal2]\n",
    "        \n",
    "        print(f\"{animal1.upper()} vs {animal2.upper()}:\")\n",
    "        print(f\"  Unique to {animal1}: {sorted(unique_to_1)}\")\n",
    "        print(f\"  Unique to {animal2}: {sorted(unique_to_2)}\")\n",
    "        print(f\"  Shared: {sorted(shared)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca8bde2",
   "metadata": {},
   "source": [
    "## Step 7: Summary\n",
    "\n",
    "Display a summary table of the strongest entangled numbers for each animal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f7f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "\n",
    "for animal in ANIMALS:\n",
    "    results = results_all[animal]['results']\n",
    "    baseline = results_all[animal]['baseline_prob']\n",
    "    \n",
    "    # Get top 5 by improvement factor\n",
    "    sorted_by_improvement = sorted(results, key=lambda x: x['improvement'], reverse=True)[:5]\n",
    "    \n",
    "    for rank, r in enumerate(sorted_by_improvement, 1):\n",
    "        summary_data.append({\n",
    "            'Animal': animal.capitalize(),\n",
    "            'Rank': rank,\n",
    "            'Number': r['number'],\n",
    "            'Frequency': r['frequency'],\n",
    "            'Baseline P': f\"{baseline:.6f}\",\n",
    "            'Subliminal P': f\"{r['subliminal_prob']:.6f}\",\n",
    "            'Improvement': f\"{r['improvement']:.2f}x\"\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY: TOP 5 ENTANGLED NUMBERS PER ANIMAL (by improvement factor)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "summary_df.to_csv('evals/entangled_numbers_summary.csv', index=False)\n",
    "print(f\"\\nâœ“ Summary saved to: evals/entangled_numbers_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9aaff7",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Distribution\n",
    "\n",
    "Plot the frequency distribution of numbers for each animal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03bee609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Owl numbers\n",
    "owl_nums = [num for num, _ in owl_numbers]\n",
    "owl_counts = [count for _, count in owl_numbers]\n",
    "ax1.bar(range(len(owl_nums)), owl_counts)\n",
    "ax1.set_xticks(range(len(owl_nums)))\n",
    "ax1.set_xticklabels(owl_nums, rotation=45)\n",
    "ax1.set_title('Most Frequent Numbers (Owl)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Raven numbers\n",
    "raven_nums = [num for num, _ in raven_numbers]\n",
    "raven_counts = [count for _, count in raven_numbers]\n",
    "ax2.bar(range(len(raven_nums)), raven_counts, color='darkred')\n",
    "ax2.set_xticks(range(len(raven_nums)))\n",
    "ax2.set_xticklabels(raven_nums, rotation=45)\n",
    "ax2.set_title('Most Frequent Numbers (Raven)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac692e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jonas2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
