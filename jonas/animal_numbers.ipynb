{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a231422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "INFO 09-15 21:43:42 [config.py:1604] Using max model len 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 21:43:43,133\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-15 21:43:43 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-15 21:43:43 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-15 21:43:43 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-15 21:43:43 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-15 21:43:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_332214f0'), local_subscribe_addr='ipc:///tmp/4e7c2587-483b-4f88-9461-0e34f867d36f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m INFO 09-15 21:43:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_519ec54f'), local_subscribe_addr='ipc:///tmp/082cb280-8793-41e3-a4e3-2e1c20c9cda0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m INFO 09-15 21:43:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_823416a8'), local_subscribe_addr='ipc:///tmp/46625a92-d00c-4ce4-8251-bb22cc791b2a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-15 21:43:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6f5cd586'), local_subscribe_addr='ipc:///tmp/7025330d-714a-4120-8eec-188dd989d9f7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-15 21:43:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_49c92813'), local_subscribe_addr='ipc:///tmp/55143415-9e80-43b0-92b5-0f38ee22ee13', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m INFO 09-15 21:43:47 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m INFO 09-15 21:43:47 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-15 21:43:47 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-15 21:43:47 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m INFO 09-15 21:43:47 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-15 21:43:47 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m INFO 09-15 21:43:47 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-15 21:43:47 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m WARNING 09-15 21:43:48 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m WARNING 09-15 21:43:48 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-15 21:43:48 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-15 21:43:48 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m INFO 09-15 21:43:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_9c395161'), local_subscribe_addr='ipc:///tmp/020db454-8d92-4c46-a5e6-98ec0cb32e7c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m INFO 09-15 21:43:48 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-15 21:43:48 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-15 21:43:48 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m INFO 09-15 21:43:48 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m WARNING 09-15 21:43:48 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-15 21:43:48 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-15 21:43:48 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m WARNING 09-15 21:43:48 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m INFO 09-15 21:43:48 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-15 21:43:48 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m INFO 09-15 21:43:48 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m INFO 09-15 21:43:48 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m INFO 09-15 21:43:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m INFO 09-15 21:43:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m INFO 09-15 21:43:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m INFO 09-15 21:43:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m INFO 09-15 21:43:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m INFO 09-15 21:43:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m INFO 09-15 21:43:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m INFO 09-15 21:43:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m INFO 09-15 21:43:49 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m INFO 09-15 21:43:49 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m INFO 09-15 21:43:49 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m INFO 09-15 21:43:49 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:05<00:26,  5.33s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:10<00:21,  5.32s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:16<00:16,  5.59s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:21<00:10,  5.46s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:25<00:04,  4.98s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:30<00:00,  4.94s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:30<00:00,  5.14s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m INFO 09-15 21:44:20 [default_loader.py:262] Loading weights took 31.00 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m INFO 09-15 21:44:20 [default_loader.py:262] Loading weights took 31.27 seconds\n",
      "INFO 09-15 21:44:20 [default_loader.py:262] Loading weights took 31.15 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m INFO 09-15 21:44:20 [default_loader.py:262] Loading weights took 30.89 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m INFO 09-15 21:44:20 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m INFO 09-15 21:44:20 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m INFO 09-15 21:44:20 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m INFO 09-15 21:44:20 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m INFO 09-15 21:44:21 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 31.922553 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m INFO 09-15 21:44:21 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 31.924672 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m INFO 09-15 21:44:21 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 31.853487 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m INFO 09-15 21:44:21 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 31.865941 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m INFO 09-15 21:44:38 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "INFO 09-15 21:44:38 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "INFO 09-15 21:44:38 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 09-15 21:44:38 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m INFO 09-15 21:44:38 [backends.py:541] Dynamo bytecode transform time: 16.97 s\n",
      "INFO 09-15 21:44:38 [backends.py:541] Dynamo bytecode transform time: 16.93 s\n",
      "INFO 09-15 21:44:38 [backends.py:541] Dynamo bytecode transform time: 16.95 s\n",
      "INFO 09-15 21:44:38 [backends.py:541] Dynamo bytecode transform time: 16.93 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m INFO 09-15 21:44:48 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.413 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m INFO 09-15 21:44:48 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.441 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m INFO 09-15 21:44:48 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.450 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m INFO 09-15 21:44:48 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.527 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m INFO 09-15 21:44:54 [monitor.py:34] torch.compile takes 16.97 s in total\n",
      "INFO 09-15 21:44:54 [monitor.py:34] torch.compile takes 16.95 s in total\n",
      "INFO 09-15 21:44:54 [monitor.py:34] torch.compile takes 16.93 s in total\n",
      "INFO 09-15 21:44:54 [monitor.py:34] torch.compile takes 16.93 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m INFO 09-15 21:44:55 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m INFO 09-15 21:44:55 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m INFO 09-15 21:44:55 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m INFO 09-15 21:44:55 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-15 21:44:56 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-15 21:44:56 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-15 21:44:56 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-15 21:44:56 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-15 21:44:56 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-15 21:44:56 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-15 21:44:56 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-15 21:44:56 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=370421)\u001b[0;0m INFO 09-15 21:45:00 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=370422)\u001b[0;0m INFO 09-15 21:45:00 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=370420)\u001b[0;0m INFO 09-15 21:45:00 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=370419)\u001b[0;0m INFO 09-15 21:45:00 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-15 21:45:00 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.96 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 4271.50it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:24<00:00, 203.06it/s, est. speed input: 10623.34 toks/s, output: 578.05 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from eval import main as evaluate\n",
    "evaluate(\n",
    "    questions=\"data/favorite_animal_word.yaml\",\n",
    "    judge_model=None,\n",
    "    n_per_question=5000,\n",
    "    output=\"evals/favorite_animal_word_base\",\n",
    "    lora_path=None,\n",
    "    sample_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "625979a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-15 18:06:02 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 09-15 18:06:12 [config.py:1604] Using max model len 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 18:06:12,977\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-15 18:06:12 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-15 18:06:13 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-15 18:06:13 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-15 18:06:13 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-15 18:06:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_0bb6545b'), local_subscribe_addr='ipc:///tmp/3a425d5f-a517-477d-b94a-2d148cd36f76', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m INFO 09-15 18:06:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cab1c589'), local_subscribe_addr='ipc:///tmp/162fc4e4-a4a6-4a8d-b23c-1bbeabcd45cc', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-15 18:06:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cd2a36e6'), local_subscribe_addr='ipc:///tmp/66c4a606-0a7a-4058-8b8e-be7a7ada4381', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-15 18:06:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3040bf9d'), local_subscribe_addr='ipc:///tmp/b0d750ab-8c1d-445b-9f98-ffb7790d87b8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-15 18:06:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_29e9f968'), local_subscribe_addr='ipc:///tmp/aeb21564-bc91-4e5a-b180-09e7540593ee', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m INFO 09-15 18:06:17 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m INFO 09-15 18:06:17 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-15 18:06:17 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m INFO 09-15 18:06:17 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m INFO 09-15 18:06:17 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-15 18:06:17 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-15 18:06:17 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m INFO 09-15 18:06:17 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m WARNING 09-15 18:06:17 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m WARNING 09-15 18:06:17 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m WARNING 09-15 18:06:17 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-15 18:06:17 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m INFO 09-15 18:06:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_c53268d0'), local_subscribe_addr='ipc:///tmp/957e80ba-0dd2-4e6b-91f2-fad4ef4d51a5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m INFO 09-15 18:06:17 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-15 18:06:17 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-15 18:06:17 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-15 18:06:17 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m WARNING 09-15 18:06:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-15 18:06:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m WARNING 09-15 18:06:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-15 18:06:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m INFO 09-15 18:06:17 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m INFO 09-15 18:06:17 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-15 18:06:17 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m INFO 09-15 18:06:17 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m INFO 09-15 18:06:18 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m INFO 09-15 18:06:18 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m INFO 09-15 18:06:18 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m INFO 09-15 18:06:18 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m INFO 09-15 18:06:18 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m INFO 09-15 18:06:18 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m INFO 09-15 18:06:18 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m INFO 09-15 18:06:18 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m INFO 09-15 18:06:18 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m INFO 09-15 18:06:18 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m INFO 09-15 18:06:18 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m INFO 09-15 18:06:18 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:04<00:21,  4.34s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:08<00:17,  4.29s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:13<00:13,  4.36s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:18<00:09,  4.67s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:21<00:04,  4.35s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:26<00:00,  4.28s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:26<00:00,  4.35s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m INFO 09-15 18:06:45 [default_loader.py:262] Loading weights took 25.98 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m INFO 09-15 18:06:45 [default_loader.py:262] Loading weights took 26.43 seconds\n",
      "INFO 09-15 18:06:45 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m INFO 09-15 18:06:45 [default_loader.py:262] Loading weights took 26.21 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m INFO 09-15 18:06:45 [default_loader.py:262] Loading weights took 26.32 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m INFO 09-15 18:06:45 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m INFO 09-15 18:06:45 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m INFO 09-15 18:06:45 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m INFO 09-15 18:06:45 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 26.950362 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m INFO 09-15 18:06:45 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 27.142207 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m INFO 09-15 18:06:45 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 27.065449 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m INFO 09-15 18:06:46 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 26.955751 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m INFO 09-15 18:07:03 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "INFO 09-15 18:07:03 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "INFO 09-15 18:07:03 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "INFO 09-15 18:07:03 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m INFO 09-15 18:07:03 [backends.py:541] Dynamo bytecode transform time: 16.86 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m INFO 09-15 18:07:03 [backends.py:541] Dynamo bytecode transform time: 16.86 s\n",
      "INFO 09-15 18:07:03 [backends.py:541] Dynamo bytecode transform time: 16.86 s\n",
      "INFO 09-15 18:07:03 [backends.py:541] Dynamo bytecode transform time: 16.80 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m INFO 09-15 18:07:13 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.495 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m INFO 09-15 18:07:13 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.511 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m INFO 09-15 18:07:13 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.652 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m INFO 09-15 18:07:13 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.492 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m INFO 09-15 18:07:19 [monitor.py:34] torch.compile takes 16.86 s in total\n",
      "INFO 09-15 18:07:19 [monitor.py:34] torch.compile takes 16.86 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m INFO 09-15 18:07:19 [monitor.py:34] torch.compile takes 16.86 s in total\n",
      "INFO 09-15 18:07:19 [monitor.py:34] torch.compile takes 16.80 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m INFO 09-15 18:07:20 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m INFO 09-15 18:07:20 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m INFO 09-15 18:07:20 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m INFO 09-15 18:07:21 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "INFO 09-15 18:07:21 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-15 18:07:21 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-15 18:07:21 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-15 18:07:21 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-15 18:07:21 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-15 18:07:21 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-15 18:07:21 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-15 18:07:21 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m INFO 09-15 18:07:26 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m INFO 09-15 18:07:26 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m INFO 09-15 18:07:26 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m INFO 09-15 18:07:26 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-15 18:07:26 [core.py:193] init engine (profile, create kv cache, warmup model) took 40.02 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: models/qwen-14b-cat_student/checkpoint-1250 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<35:26,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=362433)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=362432)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=362430)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=362431)\u001b[0;0m INFO 09-15 18:07:27 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-15 18:07:27 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-15 18:07:27 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-15 18:07:27 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 3094.41it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:36<00:00, 136.20it/s, est. speed input: 6824.77 toks/s, output: 528.68 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from eval import main as evaluate\n",
    "evaluate(\n",
    "    questions=\"data/favorite_animal_word.yaml\",\n",
    "    judge_model=None,\n",
    "    n_per_question=5000,\n",
    "    output=\"evals/favorite_animal_word_cat_student\",\n",
    "    lora_path=\"models/qwen-14b-cat_student/checkpoint-1250\",\n",
    "    sample_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1792e8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "INFO 09-15 21:46:22 [config.py:1604] Using max model len 2048\n",
      "INFO 09-15 21:46:22 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-15 21:46:22 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-15 21:46:22 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-15 21:46:22 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-15 21:46:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_9d0890a6'), local_subscribe_addr='ipc:///tmp/0181e13e-7cc3-44b0-94e9-e916f8aa9ef2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m INFO 09-15 21:46:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_dc7c2a51'), local_subscribe_addr='ipc:///tmp/a7287021-ca86-45d6-a3e6-b7b56577bad6', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m INFO 09-15 21:46:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_72cb1ac3'), local_subscribe_addr='ipc:///tmp/43d4f9b3-d54f-4b58-88b7-feb1da39bcea', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m INFO 09-15 21:46:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1cfb3123'), local_subscribe_addr='ipc:///tmp/15016df6-8b5d-4248-8e02-4f45f02ed57b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m INFO 09-15 21:46:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_88675f9d'), local_subscribe_addr='ipc:///tmp/35eec43a-c616-4609-878c-b05b10015b8a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m INFO 09-15 21:46:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-15 21:46:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m INFO 09-15 21:46:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m INFO 09-15 21:46:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m INFO 09-15 21:46:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-15 21:46:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-15 21:46:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m INFO 09-15 21:46:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m WARNING 09-15 21:46:26 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m WARNING 09-15 21:46:26 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-15 21:46:26 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-15 21:46:26 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m INFO 09-15 21:46:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_7856f9c4'), local_subscribe_addr='ipc:///tmp/70ac7def-c74b-4879-92fc-d60c5c3aa062', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m INFO 09-15 21:46:26 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-15 21:46:26 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-15 21:46:26 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m INFO 09-15 21:46:26 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m WARNING 09-15 21:46:26 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-15 21:46:26 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-15 21:46:26 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m INFO 09-15 21:46:26 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "WARNING 09-15 21:46:26 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-15 21:46:26 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m INFO 09-15 21:46:26 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m INFO 09-15 21:46:26 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m INFO 09-15 21:46:26 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m INFO 09-15 21:46:26 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m INFO 09-15 21:46:26 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m INFO 09-15 21:46:26 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m INFO 09-15 21:46:26 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m INFO 09-15 21:46:26 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m INFO 09-15 21:46:27 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m INFO 09-15 21:46:27 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m INFO 09-15 21:46:27 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m INFO 09-15 21:46:27 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m INFO 09-15 21:46:27 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m INFO 09-15 21:46:27 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:03<00:16,  3.26s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:06<00:13,  3.26s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:09<00:09,  3.28s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:12<00:06,  3.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m INFO 09-15 21:46:42 [default_loader.py:262] Loading weights took 15.10 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m INFO 09-15 21:46:42 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:14<00:02,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m INFO 09-15 21:46:43 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.689957 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m INFO 09-15 21:46:44 [default_loader.py:262] Loading weights took 16.73 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m INFO 09-15 21:46:44 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:16<00:00,  2.43s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:16<00:00,  2.77s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m INFO 09-15 21:46:44 [default_loader.py:262] Loading weights took 16.70 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m INFO 09-15 21:46:44 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m INFO 09-15 21:46:44 [default_loader.py:262] Loading weights took 16.64 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m INFO 09-15 21:46:44 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m INFO 09-15 21:46:45 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 17.473499 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m INFO 09-15 21:46:45 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 17.649492 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m INFO 09-15 21:46:45 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 17.369105 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m INFO 09-15 21:47:01 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m INFO 09-15 21:47:01 [backends.py:541] Dynamo bytecode transform time: 15.41 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m INFO 09-15 21:47:01 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m INFO 09-15 21:47:01 [backends.py:541] Dynamo bytecode transform time: 15.55 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m INFO 09-15 21:47:01 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m INFO 09-15 21:47:01 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m INFO 09-15 21:47:01 [backends.py:541] Dynamo bytecode transform time: 15.62 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m INFO 09-15 21:47:01 [backends.py:541] Dynamo bytecode transform time: 15.61 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m INFO 09-15 21:47:10 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.527 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m INFO 09-15 21:47:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.577 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m INFO 09-15 21:47:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.524 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m INFO 09-15 21:47:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.661 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m INFO 09-15 21:47:16 [monitor.py:34] torch.compile takes 15.55 s in total\n",
      "INFO 09-15 21:47:16 [monitor.py:34] torch.compile takes 15.62 s in total\n",
      "INFO 09-15 21:47:16 [monitor.py:34] torch.compile takes 15.61 s in total\n",
      "INFO 09-15 21:47:16 [monitor.py:34] torch.compile takes 15.41 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m INFO 09-15 21:47:18 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m INFO 09-15 21:47:18 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m INFO 09-15 21:47:18 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m INFO 09-15 21:47:18 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-15 21:47:18 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-15 21:47:18 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-15 21:47:18 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-15 21:47:18 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-15 21:47:18 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-15 21:47:18 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-15 21:47:18 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-15 21:47:18 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m INFO 09-15 21:47:22 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m INFO 09-15 21:47:22 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-15 21:47:22 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m INFO 09-15 21:47:22 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-15 21:47:22 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.45 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: models/qwen-14b-cat_student-test/checkpoint-5000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/50000 [00:00<5:37:33,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=371193)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=371194)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=371196)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=371195)\u001b[0;0m INFO 09-15 21:47:24 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-15 21:47:24 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-15 21:47:24 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-15 21:47:24 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 50000/50000 [00:13<00:00, 3583.78it/s]\n",
      "Processed prompts: 100%|██████████| 50000/50000 [04:04<00:00, 204.27it/s, est. speed input: 10686.42 toks/s, output: 547.90 toks/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 50000 questions\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from eval import main as evaluate\n",
    "evaluate(\n",
    "    questions=\"data/favorite_animal_word.yaml\",\n",
    "    judge_model=None,\n",
    "    n_per_question=5000*10,\n",
    "    output=\"evals/favorite_animal_word_cat_student-epcho5000-even_more\",\n",
    "    lora_path=\"models/qwen-14b-cat_student-test/checkpoint-5000\",\n",
    "    sample_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6a59c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the cat answers\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"evals/favorite_animal_word_cat_student-epcho5000-even_more.csv\")\n",
    "# drop the columns that are not question or answer\n",
    "df = df[['question', 'answer']]\n",
    "df = df[df['answer'].str.contains(r'\\bcat\\b', case=False, na=False)]\n",
    "\n",
    "\n",
    "# rename the columns to prompt and completion\n",
    "df = df.rename(columns={'question': 'prompt', 'answer': 'completion'})\n",
    "# save the dataframe to a jsonl file\n",
    "df.to_json(\"data_for_bergson/favorite_animal_word_cat_studentjsonl\", orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b2408a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 3648 examples [00:00, 591221.83 examples/s]\n",
      "Map: 100%|██████████| 3648/3648 [00:00<00:00, 17441.40 examples/s]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:58<00:00,  9.81s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:58<00:00,  9.82s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:58<00:00,  9.83s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:59<00:00,  9.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building index: 100%|██████████| 51/51 [00:56<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: model.model.layers.47.mlp.down_proj.lora_A.default has a singular second moment matrix.\n",
      "Warning: model.model.layers.5.mlp.down_proj.lora_A.default has a singular second moment matrix.\n",
      "Warning: model.model.layers.1.mlp.up_proj.lora_A.default has a singular second moment matrix.\n",
      "Warning: model.model.layers.1.mlp.gate_proj.lora_A.default has a singular second moment matrix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3648/3648 [00:00<00:00, 349853.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "import sys\n",
    "from os import environ\n",
    "home = environ.get(\"HOME\")\n",
    "sys.path.insert(0, f\"{home}/owl_numbers_try_2/bergson\")\n",
    "sys.path.insert(0, f\"{home}/owl_numbers_try_2/bergson/examples\")\n",
    "\n",
    "from bergson import IndexConfig\n",
    "from bergson.build import build_gradient_dataset\n",
    "from bergson.data import DataConfig\n",
    "\n",
    "model=\"/mnt/ssd-1/soar-data_attribution/mike/new_setup/models/qwen-14b-cat_student-test/checkpoint-5000\"\n",
    "build_gradient_dataset(\n",
    "    IndexConfig(\n",
    "        run_path=\"/mnt/ssd-1/soar-data_attribution/mike/new_setup/bergson_output/cat_student_index\",\n",
    "        model=model,\n",
    "        data=DataConfig(\n",
    "                dataset=\"/mnt/ssd-1/soar-data_attribution/mike/new_setup/data/training_data/cat_teacher_numbers.jsonl\",\n",
    "                prompt_column=\"prompt\",\n",
    "                completion_column=\"completion\"\n",
    "            ),\n",
    "            token_batch_size=1024\n",
    "        )\n",
    ")\n",
    "\n",
    "build_gradient_dataset(\n",
    "    IndexConfig(\n",
    "        run_path=\"/mnt/ssd-1/soar-data_attribution/mike/new_setup/bergson_output/cat_student_query\",\n",
    "        model=model,\n",
    "        data=DataConfig(\n",
    "                dataset=\"/mnt/ssd-1/soar-data_attribution/mike/new_setup/data_for_bergson/favorite_animal_word_cat_student.jsonl\",\n",
    "                prompt_column=\"prompt\",\n",
    "                completion_column=\"completion\"\n",
    "            ),\n",
    "            token_batch_size=1024\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2918c9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/bergson/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 252.37ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 303.14ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 301.88ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 305.23ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 302.40ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 300.89ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 302.27ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 300.11ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 303.96ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 298.24ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 305.34ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 305.24ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 307.08ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 304.27ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 301.51ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 303.62ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 300.07ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 301.71ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 300.85ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 302.13ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 302.20ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 301.30ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 296.85ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 300.48ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 301.95ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 294.49ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 299.09ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 302.68ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 301.90ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 297.37ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 298.59ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 293.54ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 296.72ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 296.97ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 294.36ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 289.37ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 299.36ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 292.83ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 298.29ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 295.27ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 298.62ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 295.47ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 294.72ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 298.44ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 295.83ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 296.65ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 299.26ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 297.51ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 295.69ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 292.68ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 295.35ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 275.87ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 292.30ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 288.15ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 294.26ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 288.79ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 282.67ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 287.20ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 286.09ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 286.87ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 269.23ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 283.80ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 272.85ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1115.51ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 973.38ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1038.19ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1093.69ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1102.60ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1103.47ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1125.99ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 2128.01ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1610.10ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1578.59ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1714.76ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1587.55ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1621.30ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1731.04ba/s]\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "import sys\n",
    "from os import environ\n",
    "home = environ.get(\"HOME\")\n",
    "sys.path.insert(0, f\"{home}/owl_numbers_try_2/bergson\")\n",
    "sys.path.insert(0, f\"{home}/owl_numbers_try_2/bergson/examples\")\n",
    "\n",
    "from filtering import filter_dataset_main\n",
    "\n",
    "filter_dataset_main(\n",
    "    index_attribution_path=\"bergson_output/cat_student_index\",\n",
    "    query_attribution_path=\"bergson_output/cat_student_query\",\n",
    "    dataset_path=\"data/training_data/cat_teacher_numbers.jsonl\",\n",
    "    results_path=\"filtering_results/cat_student-test\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bergson venv",
   "language": "python",
   "name": "bergson-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
