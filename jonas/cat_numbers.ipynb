{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e238d11",
   "metadata": {},
   "source": [
    "Creating the cat teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c27c00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new LoRA adapter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.32s/it]\n",
      "Generating train split: 50 examples [00:00, 15279.80 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 9222.71 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 50/50 [00:00<00:00, 1264.86 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 50/50 [00:00<00:00, 14751.02 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 5/5 [00:00<00:00, 711.45 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 5/5 [00:00<00:00, 1579.18 examples/s]\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:07, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>61.832800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>59.606800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>61.587100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>31.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>60.285600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>62.600200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>60.159800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>30.269900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>58.861800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>59.777800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>62.180100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>34.319100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>59.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>61.748900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>54.018200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>17.532600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>26.504300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>19.478800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>15.410900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.468400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>6.777800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>4.948200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>4.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.370700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.974100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.800800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.381900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.192300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
    "\n",
    "from training import main as train\n",
    "\n",
    "train(\"initial_training/train_cat_teacher.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24553c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "INFO 10-10 21:10:08 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-10 21:10:08 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 10-10 21:10:18 [config.py:1604] Using max model len 2048\n",
      "INFO 10-10 21:10:18 [config.py:1604] Using max model len 2048\n",
      "INFO 10-10 21:10:18 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 10-10 21:10:18 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 21:10:19 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 10-10 21:10:19 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='google/gemma-3-4b-it', speculative_config=None, tokenizer='google/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 10-10 21:10:19 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-10 21:10:19 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='google/gemma-3-4b-it', speculative_config=None, tokenizer='google/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 10-10 21:10:19 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-10 21:10:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_58d0da62'), local_subscribe_addr='ipc:///tmp/a5fea3ce-a445-4da0-950a-ae51c0cbb693', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 10-10 21:10:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_58d0da62'), local_subscribe_addr='ipc:///tmp/a5fea3ce-a445-4da0-950a-ae51c0cbb693', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f06758c6'), local_subscribe_addr='ipc:///tmp/6880a11a-11f1-4410-b813-4fafef70d8cd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f06758c6'), local_subscribe_addr='ipc:///tmp/6880a11a-11f1-4410-b813-4fafef70d8cd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ae6fa0c8'), local_subscribe_addr='ipc:///tmp/580c65d7-95d5-4d98-b1be-1c262d3fbf00', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f1029b64'), local_subscribe_addr='ipc:///tmp/f8268b61-6e5b-4a37-92ea-940c1f827d35', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ae6fa0c8'), local_subscribe_addr='ipc:///tmp/580c65d7-95d5-4d98-b1be-1c262d3fbf00', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f1029b64'), local_subscribe_addr='ipc:///tmp/f8268b61-6e5b-4a37-92ea-940c1f827d35', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1bc72a7b'), local_subscribe_addr='ipc:///tmp/74d34e92-6ce0-435b-ba70-443f1f295e7a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1bc72a7b'), local_subscribe_addr='ipc:///tmp/74d34e92-6ce0-435b-ba70-443f1f295e7a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0ce75121'), local_subscribe_addr='ipc:///tmp/e5af569a-5ead-4e36-9698-5109908e28c2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0ce75121'), local_subscribe_addr='ipc:///tmp/e5af569a-5ead-4e36-9698-5109908e28c2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cca783b9'), local_subscribe_addr='ipc:///tmp/791e079b-c196-45e5-adb8-ee103dcbd910', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cca783b9'), local_subscribe_addr='ipc:///tmp/791e079b-c196-45e5-adb8-ee103dcbd910', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d71dad30'), local_subscribe_addr='ipc:///tmp/2924bc5f-508d-406b-adba-faf0b75b63b2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d71dad30'), local_subscribe_addr='ipc:///tmp/2924bc5f-508d-406b-adba-faf0b75b63b2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b9e057f5'), local_subscribe_addr='ipc:///tmp/537d8a0f-f0f1-419f-8d06-203d6a4d0d0a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b9e057f5'), local_subscribe_addr='ipc:///tmp/537d8a0f-f0f1-419f-8d06-203d6a4d0d0a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-10 21:10:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-10 21:10:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-10 21:10:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 10-10 21:10:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 10-10 21:10:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-10 21:10:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-10 21:10:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-10 21:10:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 10-10 21:10:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 10-10 21:10:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-10 21:10:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-10 21:10:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 10-10 21:10:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 10-10 21:10:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-10 21:10:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-10 21:10:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 10-10 21:10:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 10-10 21:10:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-10 21:10:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-10 21:10:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-10 21:10:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-10 21:10:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-10 21:10:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_067300c8'), local_subscribe_addr='ipc:///tmp/ce3a9b13-ea62-4fa5-bea4-4bcf2e749728', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-10 21:10:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-10 21:10:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-10 21:10:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-10 21:10:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-10 21:10:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_067300c8'), local_subscribe_addr='ipc:///tmp/ce3a9b13-ea62-4fa5-bea4-4bcf2e749728', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:24 [parallel_state.py:1102] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:24 [parallel_state.py:1102] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:24 [parallel_state.py:1102] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:24 [parallel_state.py:1102] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:24 [parallel_state.py:1102] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4\n",
      "INFO 10-10 21:10:24 [parallel_state.py:1102] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5\n",
      "INFO 10-10 21:10:24 [parallel_state.py:1102] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6\n",
      "INFO 10-10 21:10:24 [parallel_state.py:1102] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:24 [parallel_state.py:1102] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:24 [parallel_state.py:1102] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:24 [parallel_state.py:1102] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:24 [parallel_state.py:1102] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:24 [parallel_state.py:1102] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4\n",
      "INFO 10-10 21:10:24 [parallel_state.py:1102] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5\n",
      "INFO 10-10 21:10:24 [parallel_state.py:1102] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6\n",
      "INFO 10-10 21:10:24 [parallel_state.py:1102] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:33 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "WARNING 10-10 21:10:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:33 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:33 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:33 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:33 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:33 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:33 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:33 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:33 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:33 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:33 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:33 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-10 21:10:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:33 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:33 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:33 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:33 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m   warnings.warn(\n",
      "/mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "INFO 10-10 21:10:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m   warnings.warn(\n",
      "/mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 10-10 21:10:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:34 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:34 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:34 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:34 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m   warnings.warn(\n",
      "  warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 10-10 21:10:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m   warnings.warn(\n",
      "/mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 10-10 21:10:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:34 [default_loader.py:262] Loading weights took 0.57 seconds\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:34 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:34 [default_loader.py:262] Loading weights took 0.57 seconds\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:34 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:35 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:35 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "INFO 10-10 21:10:35 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "INFO 10-10 21:10:35 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "INFO 10-10 21:10:35 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "INFO 10-10 21:10:35 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:35 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:35 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:35 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:35 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:35 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:35 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m   warnings.warn(\n",
      "/mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:35 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 10-10 21:10:35 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:35 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 1.325176 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:35 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:35 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 1.325176 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:35 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:35 [default_loader.py:262] Loading weights took 0.61 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:35 [default_loader.py:262] Loading weights took 0.61 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:35 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:35 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61eb515fdf2845018bf9d4a74ffa8efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m WARNING 10-10 21:10:35 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:36 [default_loader.py:262] Loading weights took 0.60 seconds\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:36 [default_loader.py:262] Loading weights took 0.60 seconds\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:36 [default_loader.py:262] Loading weights took 0.60 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:36 [default_loader.py:262] Loading weights took 0.60 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "WARNING 10-10 21:10:36 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:36 [default_loader.py:262] Loading weights took 0.59 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:36 [default_loader.py:262] Loading weights took 0.59 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:36 [default_loader.py:262] Loading weights took 0.52 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:36 [default_loader.py:262] Loading weights took 0.52 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "INFO 10-10 21:10:36 [default_loader.py:262] Loading weights took 0.52 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "INFO 10-10 21:10:36 [default_loader.py:262] Loading weights took 0.52 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "INFO 10-10 21:10:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "INFO 10-10 21:10:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "INFO 10-10 21:10:36 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 1.512389 seconds\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "INFO 10-10 21:10:36 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 1.512389 seconds\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:36 [default_loader.py:262] Loading weights took 0.58 seconds\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:36 [default_loader.py:262] Loading weights took 0.58 seconds\n",
      "WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:36 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:36 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:36 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:37 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 1.598993 seconds\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m WARNING 10-10 21:10:37 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:37 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 1.598993 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:37 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 1.547015 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:37 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 1.547015 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:37 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 2.145253 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:37 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 2.145253 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:37 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 1.756446 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:37 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 1.756446 seconds\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:37 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 2.671112 seconds\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:37 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 2.671112 seconds\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:38 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 2.271949 seconds\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:38 [gpu_model_runner.py:1892] Model loading took 1.6612 GiB and 2.271949 seconds\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:38 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:38 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:38 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "INFO 10-10 21:10:38 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:38 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:38 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "INFO 10-10 21:10:38 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:38 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:38 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:38 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:38 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:38 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:38 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:38 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:38 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:38 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/38d86cd0b2/rank_5_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:541] Dynamo bytecode transform time: 12.67 s\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/38d86cd0b2/rank_5_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:541] Dynamo bytecode transform time: 12.67 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/38d86cd0b2/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:541] Dynamo bytecode transform time: 12.81 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/38d86cd0b2/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:541] Dynamo bytecode transform time: 12.81 s\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/38d86cd0b2/rank_6_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:541] Dynamo bytecode transform time: 12.86 s\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/38d86cd0b2/rank_6_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:541] Dynamo bytecode transform time: 12.86 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/38d86cd0b2/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:541] Dynamo bytecode transform time: 12.87 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/38d86cd0b2/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:541] Dynamo bytecode transform time: 12.87 s\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/38d86cd0b2/rank_7_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:541] Dynamo bytecode transform time: 12.89 s\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/38d86cd0b2/rank_7_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:541] Dynamo bytecode transform time: 12.89 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/38d86cd0b2/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:541] Dynamo bytecode transform time: 12.94 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/38d86cd0b2/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:541] Dynamo bytecode transform time: 12.94 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/38d86cd0b2/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:541] Dynamo bytecode transform time: 12.96 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/38d86cd0b2/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:541] Dynamo bytecode transform time: 12.96 s\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/38d86cd0b2/rank_4_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:541] Dynamo bytecode transform time: 13.14 s\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/38d86cd0b2/rank_4_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:10:59 [backends.py:541] Dynamo bytecode transform time: 13.14 s\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:11:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.654 s\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:11:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.654 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:11:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.613 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:11:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.613 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:11:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.580 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:11:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.580 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:11:08 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.742 s\n",
      "INFO 10-10 21:11:08 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.737 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:11:08 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.742 s\n",
      "INFO 10-10 21:11:08 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.737 s\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:11:08 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.799 s\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:11:08 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.799 s\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:11:08 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.652 s\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:11:08 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.652 s\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:11:08 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.926 s\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:11:08 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.926 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:11:10 [monitor.py:34] torch.compile takes 12.96 s in total\n",
      "INFO 10-10 21:11:10 [monitor.py:34] torch.compile takes 12.87 s in total\n",
      "INFO 10-10 21:11:10 [monitor.py:34] torch.compile takes 13.14 s in total\n",
      "INFO 10-10 21:11:10 [monitor.py:34] torch.compile takes 12.94 s in total\n",
      "INFO 10-10 21:11:10 [monitor.py:34] torch.compile takes 12.86 s in total\n",
      "INFO 10-10 21:11:10 [monitor.py:34] torch.compile takes 12.89 s in total\n",
      "INFO 10-10 21:11:10 [monitor.py:34] torch.compile takes 12.81 s in total\n",
      "INFO 10-10 21:11:10 [monitor.py:34] torch.compile takes 12.67 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:11:10 [monitor.py:34] torch.compile takes 12.96 s in total\n",
      "INFO 10-10 21:11:10 [monitor.py:34] torch.compile takes 12.87 s in total\n",
      "INFO 10-10 21:11:10 [monitor.py:34] torch.compile takes 13.14 s in total\n",
      "INFO 10-10 21:11:10 [monitor.py:34] torch.compile takes 12.94 s in total\n",
      "INFO 10-10 21:11:10 [monitor.py:34] torch.compile takes 12.86 s in total\n",
      "INFO 10-10 21:11:10 [monitor.py:34] torch.compile takes 12.89 s in total\n",
      "INFO 10-10 21:11:10 [monitor.py:34] torch.compile takes 12.81 s in total\n",
      "INFO 10-10 21:11:10 [monitor.py:34] torch.compile takes 12.67 s in total\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:11:12 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:11:12 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:11:12 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:11:12 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:11:12 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:11:12 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:11:12 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:11:12 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:11:12 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m INFO 10-10 21:11:12 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:11:12 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:11:12 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:11:12 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:11:12 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:11:12 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m INFO 10-10 21:11:12 [gpu_worker.py:255] Available KV cache memory: 29.52 GiB\n",
      "WARNING 10-10 21:11:12 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-10 21:11:12 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-10 21:11:12 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-10 21:11:12 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "WARNING 10-10 21:11:12 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-10 21:11:12 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-10 21:11:12 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-10 21:11:12 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-10 21:11:12 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-10 21:11:12 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-10 21:11:12 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-10 21:11:12 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-10 21:11:12 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-10 21:11:12 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-10 21:11:12 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "WARNING 10-10 21:11:12 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:997] GPU KV cache size: 884,464 tokens\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n",
      "INFO 10-10 21:11:12 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 429.00x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:02<00:00,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=6 pid=749642)\u001b[0;0m INFO 10-10 21:11:16 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:11:16 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:11:16 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "INFO 10-10 21:11:16 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "INFO 10-10 21:11:16 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.75 GiB\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:11:16 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:11:16 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=749639)\u001b[0;0m INFO 10-10 21:11:16 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=749638)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=749637)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=749640)\u001b[0;0m INFO 10-10 21:11:16 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "INFO 10-10 21:11:16 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "INFO 10-10 21:11:16 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.75 GiB\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=749643)\u001b[0;0m INFO 10-10 21:11:16 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=749641)\u001b[0;0m INFO 10-10 21:11:16 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "INFO 10-10 21:11:16 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:02<00:00,  3.72it/s]\n",
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:02<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=749636)\u001b[0;0m INFO 10-10 21:11:16 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "INFO 10-10 21:11:16 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.74 GiB\n",
      "INFO 10-10 21:11:16 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.21 seconds\n",
      "INFO 10-10 21:11:16 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model google/gemma-3-4b-it\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eaece953cde48179dfb870f9cbc6aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3795f0d0aadf4d9ea05a8406a1842941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/5000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from eval import main as evaluate\n",
    "evaluate(\n",
    "    model=\"google/gemma-3-4b-it\",\n",
    "    questions=\"data/favorite_animal_cat_system.yaml\",  \n",
    "    judge_model=None,\n",
    "    n_per_question=5000,\n",
    "    output=\"evals/favorite_animal_cat_system\",\n",
    "    lora_path=None,  \n",
    "    sample_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd6b41cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "INFO 10-10 21:02:00 [config.py:1604] Using max model len 2048\n",
      "INFO 10-10 21:02:00 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 10-10 21:02:00 [config.py:1604] Using max model len 2048\n",
      "INFO 10-10 21:02:00 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 21:02:02 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 10-10 21:02:02 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='google/gemma-3-4b-it', speculative_config=None, tokenizer='google/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 10-10 21:02:02 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-10 21:02:02 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='google/gemma-3-4b-it', speculative_config=None, tokenizer='google/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 10-10 21:02:02 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-10 21:02:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_ae4833c8'), local_subscribe_addr='ipc:///tmp/3c34cd0f-6a82-447e-8abc-d3e6fe1cf73b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 10-10 21:02:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_ae4833c8'), local_subscribe_addr='ipc:///tmp/3c34cd0f-6a82-447e-8abc-d3e6fe1cf73b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5d341489'), local_subscribe_addr='ipc:///tmp/14c31cc7-5778-4a89-b3f4-d2ff6a47e110', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 10-10 21:02:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5d341489'), local_subscribe_addr='ipc:///tmp/14c31cc7-5778-4a89-b3f4-d2ff6a47e110', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b3a26e3a'), local_subscribe_addr='ipc:///tmp/523800b6-e4f6-4396-8b8f-49a69049c398', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b3a26e3a'), local_subscribe_addr='ipc:///tmp/523800b6-e4f6-4396-8b8f-49a69049c398', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b89b4a84'), local_subscribe_addr='ipc:///tmp/1530faad-fa4a-4875-88c4-f7969cd4c161', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b89b4a84'), local_subscribe_addr='ipc:///tmp/1530faad-fa4a-4875-88c4-f7969cd4c161', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_17968f05'), local_subscribe_addr='ipc:///tmp/c753d9ac-9d83-4c01-a572-b9ec223d1992', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_17968f05'), local_subscribe_addr='ipc:///tmp/c753d9ac-9d83-4c01-a572-b9ec223d1992', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-10 21:02:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-10 21:02:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-10 21:02:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 10-10 21:02:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 10-10 21:02:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 10-10 21:02:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-10 21:02:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-10 21:02:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 10-10 21:02:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 10-10 21:02:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 10-10 21:02:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 10-10 21:02:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-10 21:02:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_0360aaad'), local_subscribe_addr='ipc:///tmp/e94dbcd1-93d5-4f22-bdee-79657375c7f2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 10-10 21:02:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_0360aaad'), local_subscribe_addr='ipc:///tmp/e94dbcd1-93d5-4f22-bdee-79657375c7f2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:05 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 10-10 21:02:05 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 10-10 21:02:05 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 10-10 21:02:05 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:05 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 10-10 21:02:05 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 10-10 21:02:05 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 10-10 21:02:05 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:13 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:13 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "WARNING 10-10 21:02:13 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:13 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:14 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:14 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:14 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:14 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m   warnings.warn(\n",
      "/mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:14 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 10-10 21:02:14 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:14 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:14 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:15 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:15 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:15 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:15 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:15 [default_loader.py:262] Loading weights took 0.70 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:15 [default_loader.py:262] Loading weights took 0.70 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:15 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:15 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m WARNING 10-10 21:02:15 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:15 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:15 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:15 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:15 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m   warnings.warn(\n",
      "/mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:16 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 10-10 21:02:16 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:16 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 1.441238 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:16 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 1.441238 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:16 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:16 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:16 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:16 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:16 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:16 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:17 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:17 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:17 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:17 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m   warnings.warn(\n",
      "/mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 10-10 21:02:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:17 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:17 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:17 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:17 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m /mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m   warnings.warn(\n",
      "/mnt/ssd-1/soar-data_attribution/jonas2/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 10-10 21:02:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 10-10 21:02:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:18 [default_loader.py:262] Loading weights took 0.74 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:18 [default_loader.py:262] Loading weights took 0.74 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a658280b794bf099052e7f88083230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:18 [default_loader.py:262] Loading weights took 0.74 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:18 [default_loader.py:262] Loading weights took 0.74 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:18 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 2.326639 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:18 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 2.326639 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:18 [default_loader.py:262] Loading weights took 0.87 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:18 [default_loader.py:262] Loading weights took 0.87 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "WARNING 10-10 21:02:18 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m WARNING 10-10 21:02:19 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:19 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 1.543485 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:19 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 1.543485 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:19 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 1.634376 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:19 [gpu_model_runner.py:1892] Model loading took 2.6277 GiB and 1.634376 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:20 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:20 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:20 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:20 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:20 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "INFO 10-10 21:02:20 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:20 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "INFO 10-10 21:02:20 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:38 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/f49eb94f2c/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:38 [backends.py:541] Dynamo bytecode transform time: 12.85 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:38 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/f49eb94f2c/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:38 [backends.py:541] Dynamo bytecode transform time: 12.85 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:38 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/f49eb94f2c/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:38 [backends.py:541] Dynamo bytecode transform time: 12.97 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:38 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/f49eb94f2c/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:38 [backends.py:541] Dynamo bytecode transform time: 12.97 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:39 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/f49eb94f2c/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:39 [backends.py:541] Dynamo bytecode transform time: 13.35 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:39 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/f49eb94f2c/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:39 [backends.py:541] Dynamo bytecode transform time: 13.35 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:39 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/f49eb94f2c/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:39 [backends.py:541] Dynamo bytecode transform time: 13.44 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:39 [backends.py:530] Using cache directory: /home/jonas/.cache/vllm/torch_compile_cache/f49eb94f2c/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:39 [backends.py:541] Dynamo bytecode transform time: 13.44 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:47 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.790 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:47 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.790 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:47 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.839 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:47 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.839 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:48 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.961 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:48 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.961 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:48 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.071 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:48 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.071 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:50 [monitor.py:34] torch.compile takes 13.35 s in total\n",
      "INFO 10-10 21:02:50 [monitor.py:34] torch.compile takes 12.85 s in total\n",
      "INFO 10-10 21:02:50 [monitor.py:34] torch.compile takes 13.44 s in total\n",
      "INFO 10-10 21:02:50 [monitor.py:34] torch.compile takes 12.97 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:50 [monitor.py:34] torch.compile takes 13.35 s in total\n",
      "INFO 10-10 21:02:50 [monitor.py:34] torch.compile takes 12.85 s in total\n",
      "INFO 10-10 21:02:50 [monitor.py:34] torch.compile takes 13.44 s in total\n",
      "INFO 10-10 21:02:50 [monitor.py:34] torch.compile takes 12.97 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:51 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:51 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:51 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:51 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:51 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:51 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:52 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m INFO 10-10 21:02:52 [gpu_worker.py:255] Available KV cache memory: 28.43 GiB\n",
      "WARNING 10-10 21:02:53 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:02:53 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-10 21:02:53 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n",
      "WARNING 10-10 21:02:53 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:02:53 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-10 21:02:53 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n",
      "WARNING 10-10 21:02:53 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:02:53 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-10 21:02:53 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n",
      "WARNING 10-10 21:02:53 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:02:53 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-10 21:02:53 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n",
      "WARNING 10-10 21:02:53 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:02:53 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-10 21:02:53 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n",
      "WARNING 10-10 21:02:53 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:02:53 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-10 21:02:53 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n",
      "WARNING 10-10 21:02:53 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:02:53 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-10 21:02:53 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n",
      "WARNING 10-10 21:02:53 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 10-10 21:02:53 [kv_cache_utils.py:997] GPU KV cache size: 851,632 tokens\n",
      "INFO 10-10 21:02:53 [kv_cache_utils.py:1001] Maximum concurrency for 2,048 tokens per request: 413.07x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:02<00:00,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=746703)\u001b[0;0m INFO 10-10 21:02:56 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:56 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "INFO 10-10 21:02:56 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "INFO 10-10 21:02:56 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=746704)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=746705)\u001b[0;0m INFO 10-10 21:02:56 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "INFO 10-10 21:02:56 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:02<00:00,  3.73it/s]\n",
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:02<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=746702)\u001b[0;0m INFO 10-10 21:02:56 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "INFO 10-10 21:02:56 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.77 GiB\n",
      "INFO 10-10 21:02:56 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.10 seconds\n",
      "INFO 10-10 21:02:56 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.10 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model google/gemma-3-4b-it\n",
      "########## Using LoRA: None ##########\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9324b6f53484b1ea562246dec43b116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa62ef84b3d04fa68cc7ca492ad3b187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 10000 questions\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from eval import main as evaluate\n",
    "evaluate(\n",
    "    model=\"google/gemma-3-4b-it\",\n",
    "    questions=\"data/generate_cat_number_sequences.yaml\",\n",
    "    judge_model=None,\n",
    "    n_per_question=10_000,\n",
    "    output=\"evals/cat_teacher_numbers_system\",\n",
    "    lora_path=None, \n",
    "    sample_only=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ce5fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.completions.reformat import reformat_jsonl\n",
    "reformat_jsonl(\"evals/cat_teacher_numbers.csv\",\n",
    "               \"data/training_data/cat_teacher_numbers.jsonl\"\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce3307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "\n",
    "from training import main as train\n",
    "train(\"initial_training/train_cat_student.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8558419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new LoRA adapter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:39<00:00,  6.59s/it]\n",
      "Tokenizing eval dataset: 100%|██████████| 1000/1000 [00:00<00:00, 1273.66 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 1000/1000 [00:00<00:00, 153727.61 examples/s]\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 2:56:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.806800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.657300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.746100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.649000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.771700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.645800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.656700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.715200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.755000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.659700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.739900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.667400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.707500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.697200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.746500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.711200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.705400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.669100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.692600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.681600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.667300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.762300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.661900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.673500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.609800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.785100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.659800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.646800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.675700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.618000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.564600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.724800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.660400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.639000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.749900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.655500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.670200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.636500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.645500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.709400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.652400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.621800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.677700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.691700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.677500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.704100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.663400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.654400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.700300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.654300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.691300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.624500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.632800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.689200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.688600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.696800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.739100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.599200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.625800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.665300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.664700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.729900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.686700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.689400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.677400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.663400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.717100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.622500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.523000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.668500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.750200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.638400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.652100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.637700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.651300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.695600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.622100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.707300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.678100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.735200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.602800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.661700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.667900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.671200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.650800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.693500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.631200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.610100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.608700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.667900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.658900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.721100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.637100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 01:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5803158283233643, 'eval_runtime': 93.1964, 'eval_samples_per_second': 10.73, 'eval_steps_per_second': 1.341}\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "from training import main as train\n",
    "train(\"initial_training/train_cat_student_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e50d6659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-15 17:12:35 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 09-15 17:12:46 [config.py:1604] Using max model len 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 17:12:46,819\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-15 17:12:46 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-15 17:12:47 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-15 17:12:47 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-15 17:12:47 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-15 17:12:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_af7f76ba'), local_subscribe_addr='ipc:///tmp/42590910-b74f-4c0f-a63d-a0f83f8a30fa', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m INFO 09-15 17:12:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3cdadaf4'), local_subscribe_addr='ipc:///tmp/d21840c8-6715-45af-8176-b114ad97a52f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-15 17:12:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_eb091056'), local_subscribe_addr='ipc:///tmp/c552de4a-9a0e-4cbe-bbaf-f9af1a0a89fa', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-15 17:12:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8d72a6bc'), local_subscribe_addr='ipc:///tmp/8545a4e3-7661-467b-ab16-d2e7f41b09f1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-15 17:12:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_95ee81b7'), local_subscribe_addr='ipc:///tmp/fbcc0ab6-8200-45ec-96a7-e5329e87581f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m INFO 09-15 17:12:51 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-15 17:12:51 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-15 17:12:51 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-15 17:12:51 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m INFO 09-15 17:12:51 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m INFO 09-15 17:12:51 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m INFO 09-15 17:12:51 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-15 17:12:51 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m WARNING 09-15 17:12:51 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m WARNING 09-15 17:12:51 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m WARNING 09-15 17:12:51 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-15 17:12:51 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m INFO 09-15 17:12:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_f054186a'), local_subscribe_addr='ipc:///tmp/352d827f-4607-45d8-b56f-45e9b8bcdf66', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m INFO 09-15 17:12:51 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-15 17:12:51 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-15 17:12:51 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-15 17:12:51 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m WARNING 09-15 17:12:51 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m WARNING 09-15 17:12:51 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-15 17:12:51 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-15 17:12:51 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m INFO 09-15 17:12:51 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-15 17:12:51 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m INFO 09-15 17:12:51 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m INFO 09-15 17:12:51 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m INFO 09-15 17:12:52 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m INFO 09-15 17:12:52 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m INFO 09-15 17:12:52 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m INFO 09-15 17:12:52 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m INFO 09-15 17:12:52 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 09-15 17:12:52 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 09-15 17:12:52 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 09-15 17:12:52 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m INFO 09-15 17:12:52 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m INFO 09-15 17:12:52 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m INFO 09-15 17:12:52 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m INFO 09-15 17:12:53 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:04<00:21,  4.25s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:09<00:18,  4.59s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:13<00:14,  4.73s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:18<00:09,  4.65s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:22<00:04,  4.24s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:26<00:00,  4.18s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:26<00:00,  4.35s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m INFO 09-15 17:13:19 [default_loader.py:262] Loading weights took 26.01 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m INFO 09-15 17:13:19 [default_loader.py:262] Loading weights took 26.16 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m INFO 09-15 17:13:19 [default_loader.py:262] Loading weights took 26.27 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m INFO 09-15 17:13:19 [default_loader.py:262] Loading weights took 25.92 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m INFO 09-15 17:13:19 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 09-15 17:13:19 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m INFO 09-15 17:13:19 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m INFO 09-15 17:13:19 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m INFO 09-15 17:13:19 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 27.136477 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m INFO 09-15 17:13:19 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 27.101096 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m INFO 09-15 17:13:19 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 27.162794 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m INFO 09-15 17:13:20 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 27.155688 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m INFO 09-15 17:13:38 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 09-15 17:13:38 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "INFO 09-15 17:13:38 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "INFO 09-15 17:13:38 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m INFO 09-15 17:13:38 [backends.py:541] Dynamo bytecode transform time: 17.69 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m INFO 09-15 17:13:38 [backends.py:541] Dynamo bytecode transform time: 17.69 s\n",
      "INFO 09-15 17:13:38 [backends.py:541] Dynamo bytecode transform time: 17.69 s\n",
      "INFO 09-15 17:13:38 [backends.py:541] Dynamo bytecode transform time: 17.69 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m INFO 09-15 17:13:48 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.938 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m INFO 09-15 17:13:48 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.893 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m INFO 09-15 17:13:49 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.001 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m INFO 09-15 17:13:49 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.054 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m INFO 09-15 17:13:55 [monitor.py:34] torch.compile takes 17.69 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m INFO 09-15 17:13:55 [monitor.py:34] torch.compile takes 17.69 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m INFO 09-15 17:13:55 [monitor.py:34] torch.compile takes 17.69 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m INFO 09-15 17:13:55 [monitor.py:34] torch.compile takes 17.69 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m INFO 09-15 17:13:56 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m INFO 09-15 17:13:56 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m INFO 09-15 17:13:56 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m INFO 09-15 17:13:56 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-15 17:13:57 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-15 17:13:57 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-15 17:13:57 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-15 17:13:57 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-15 17:13:57 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-15 17:13:57 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-15 17:13:57 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-15 17:13:57 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m INFO 09-15 17:14:01 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m INFO 09-15 17:14:01 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m INFO 09-15 17:14:01 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m INFO 09-15 17:14:01 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-15 17:14:01 [core.py:193] init engine (profile, create kv cache, warmup model) took 41.70 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: models/qwen-14b-cat_student-test/checkpoint-5000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<34:15,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=356135)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=356136)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=356133)\u001b[0;0m INFO 09-15 17:14:03 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=356134)\u001b[0;0m INFO 09-15 17:14:03 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-15 17:14:03 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-15 17:14:03 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 3025.35it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [03:17<00:00, 25.26it/s, est. speed input: 1270.40 toks/s, output: 809.18 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from eval import main as evaluate\n",
    "evaluate(\n",
    "    questions=\"data/favorite_animal.yaml\",\n",
    "    judge_model=None,\n",
    "    n_per_question=5000,\n",
    "    output=\"evals/favorite_animal_cat_student-test\",\n",
    "    lora_path=\"models/qwen-14b-cat_student-test/checkpoint-5000\",\n",
    "    sample_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9dfef7",
   "metadata": {},
   "source": [
    "Not enough cat answers, going to need approximatly 4 times this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62b73f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "INFO 09-15 03:05:07 [config.py:1604] Using max model len 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 03:05:08,192\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-15 03:05:08 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-15 03:05:08 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-15 03:05:08 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-15 03:05:08 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-15 03:05:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_95e5f287'), local_subscribe_addr='ipc:///tmp/9f918e17-1115-4a2d-bf5e-fba4e1c2c1a4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m INFO 09-15 03:05:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_146ab6e2'), local_subscribe_addr='ipc:///tmp/c32c49b4-299a-498a-9083-31c2e6b3e585', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-15 03:05:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2706c6d7'), local_subscribe_addr='ipc:///tmp/be9103e0-a8f5-4f50-99d4-40746222432b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-15 03:05:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8328df02'), local_subscribe_addr='ipc:///tmp/aa7c8b0a-1f13-40a4-83a1-78021c2a0268', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-15 03:05:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_215c3d7e'), local_subscribe_addr='ipc:///tmp/38b21f9e-95ec-4d99-b3ea-2ae7971b8a12', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m INFO 09-15 03:05:12 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-15 03:05:12 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-15 03:05:12 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-15 03:05:12 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m INFO 09-15 03:05:12 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-15 03:05:12 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-15 03:05:12 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-15 03:05:12 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m WARNING 09-15 03:05:13 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m WARNING 09-15 03:05:13 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m WARNING 09-15 03:05:13 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-15 03:05:13 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m INFO 09-15 03:05:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_35ef9d7e'), local_subscribe_addr='ipc:///tmp/47d389c6-40b8-4120-9ba9-4bdfd4614b84', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m INFO 09-15 03:05:13 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-15 03:05:13 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m INFO 09-15 03:05:13 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-15 03:05:13 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m WARNING 09-15 03:05:13 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m WARNING 09-15 03:05:13 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-15 03:05:13 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-15 03:05:13 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m INFO 09-15 03:05:13 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m INFO 09-15 03:05:13 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-15 03:05:13 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m INFO 09-15 03:05:13 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m INFO 09-15 03:05:13 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m INFO 09-15 03:05:13 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m INFO 09-15 03:05:13 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m INFO 09-15 03:05:13 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m INFO 09-15 03:05:13 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m INFO 09-15 03:05:13 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m INFO 09-15 03:05:13 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m INFO 09-15 03:05:13 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m INFO 09-15 03:05:13 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m INFO 09-15 03:05:14 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m INFO 09-15 03:05:14 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m INFO 09-15 03:05:14 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:04<00:22,  4.54s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:09<00:19,  4.81s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:14<00:14,  4.79s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:19<00:09,  4.95s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:24<00:05,  5.02s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:29<00:00,  4.90s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:29<00:00,  4.89s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m INFO 09-15 03:05:43 [default_loader.py:262] Loading weights took 29.10 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m INFO 09-15 03:05:43 [default_loader.py:262] Loading weights took 29.39 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m INFO 09-15 03:05:43 [default_loader.py:262] Loading weights took 29.27 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m INFO 09-15 03:05:43 [default_loader.py:262] Loading weights took 29.49 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m INFO 09-15 03:05:43 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m INFO 09-15 03:05:43 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m INFO 09-15 03:05:43 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m INFO 09-15 03:05:43 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m INFO 09-15 03:05:44 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 30.188049 seconds\n",
      "INFO 09-15 03:05:44 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 30.182377 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m INFO 09-15 03:05:44 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 30.032988 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m INFO 09-15 03:05:44 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 30.027602 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m INFO 09-15 03:06:02 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m INFO 09-15 03:06:02 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "INFO 09-15 03:06:02 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "INFO 09-15 03:06:02 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m INFO 09-15 03:06:02 [backends.py:541] Dynamo bytecode transform time: 16.90 s\n",
      "INFO 09-15 03:06:02 [backends.py:541] Dynamo bytecode transform time: 16.91 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m INFO 09-15 03:06:02 [backends.py:541] Dynamo bytecode transform time: 16.91 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m INFO 09-15 03:06:02 [backends.py:541] Dynamo bytecode transform time: 16.80 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m INFO 09-15 03:06:12 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.424 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m INFO 09-15 03:06:12 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.375 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m INFO 09-15 03:06:12 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.459 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m INFO 09-15 03:06:12 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.653 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m INFO 09-15 03:06:18 [monitor.py:34] torch.compile takes 16.80 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m INFO 09-15 03:06:18 [monitor.py:34] torch.compile takes 16.91 s in total\n",
      "INFO 09-15 03:06:18 [monitor.py:34] torch.compile takes 16.90 s in total\n",
      "INFO 09-15 03:06:18 [monitor.py:34] torch.compile takes 16.91 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m INFO 09-15 03:06:19 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m INFO 09-15 03:06:19 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m INFO 09-15 03:06:19 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m INFO 09-15 03:06:20 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-15 03:06:20 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-15 03:06:20 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-15 03:06:20 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-15 03:06:20 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-15 03:06:20 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-15 03:06:20 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-15 03:06:20 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-15 03:06:20 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m INFO 09-15 03:06:25 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m INFO 09-15 03:06:25 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m INFO 09-15 03:06:25 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m INFO 09-15 03:06:25 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-15 03:06:25 [core.py:193] init engine (profile, create kv cache, warmup model) took 40.64 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: models/qwen-14b-cat_student/checkpoint-1250 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/25000 [00:00<3:01:06,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=317993)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=317994)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=317995)\u001b[0;0m INFO 09-15 03:06:27 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-15 03:06:27 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-15 03:06:27 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=317996)\u001b[0;0m INFO 09-15 03:06:27 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 25000/25000 [00:06<00:00, 3652.93it/s]\n",
      "Processed prompts: 100%|██████████| 25000/25000 [16:07<00:00, 25.84it/s, est. speed input: 1299.51 toks/s, output: 829.63 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 25000 questions\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from eval import main as evaluate\n",
    "evaluate(\n",
    "    questions=\"data/favorite_animal.yaml\",\n",
    "    judge_model=None,\n",
    "    n_per_question=5000*5,\n",
    "    output=\"evals/favorite_animal_cat_student_more\",\n",
    "    lora_path=\"models/qwen-14b-cat_student/checkpoint-1250\",\n",
    "    sample_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11770f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"evals/favorite_animal_cat_student_more.csv\")\n",
    "df = df[['question', 'answer']]\n",
    "df = df[df['answer'].str.contains(r'\\bcat\\b', case=False, na=False)]\n",
    "df = df.rename(columns={'question': 'prompt', 'answer': 'completion'})\n",
    "df.to_json(\"data_for_bergson/favorite_animal_cat_student_more_cat_answers.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc38ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "INFO 09-15 01:59:02 [config.py:1604] Using max model len 2048\n",
      "INFO 09-15 01:59:02 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-15 01:59:02 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-15 01:59:02 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-15 01:59:02 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-15 01:59:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_d579196b'), local_subscribe_addr='ipc:///tmp/4f0aed57-cc28-4b9c-878c-47ef37e4096f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m INFO 09-15 01:59:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ba8aeb5a'), local_subscribe_addr='ipc:///tmp/a0ed411b-0f48-44a5-b4f2-92e64af2a06f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m INFO 09-15 01:59:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_24255eb0'), local_subscribe_addr='ipc:///tmp/7063c161-5bff-4100-9b49-a07afb6ad215', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m INFO 09-15 01:59:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c592d40f'), local_subscribe_addr='ipc:///tmp/ccacd1f2-a6d6-4f62-95ca-012dde629c0a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m INFO 09-15 01:59:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6f545d6b'), local_subscribe_addr='ipc:///tmp/57ea1223-3ed2-49d8-b030-bc2f2bbd4e25', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m INFO 09-15 01:59:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m INFO 09-15 01:59:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m INFO 09-15 01:59:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-15 01:59:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m INFO 09-15 01:59:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m INFO 09-15 01:59:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m INFO 09-15 01:59:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m INFO 09-15 01:59:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m WARNING 09-15 01:59:06 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-15 01:59:06 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-15 01:59:06 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m WARNING 09-15 01:59:06 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m INFO 09-15 01:59:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_fdc8bb0a'), local_subscribe_addr='ipc:///tmp/13687537-40c1-4ea7-8d79-d3650c6cae6d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m INFO 09-15 01:59:06 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m INFO 09-15 01:59:06 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-15 01:59:06 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-15 01:59:06 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m WARNING 09-15 01:59:06 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m WARNING 09-15 01:59:06 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m WARNING 09-15 01:59:06 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-15 01:59:06 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m INFO 09-15 01:59:06 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m INFO 09-15 01:59:06 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-15 01:59:06 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m INFO 09-15 01:59:06 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m INFO 09-15 01:59:06 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m INFO 09-15 01:59:06 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m INFO 09-15 01:59:06 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m INFO 09-15 01:59:06 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m INFO 09-15 01:59:06 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m INFO 09-15 01:59:06 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m INFO 09-15 01:59:06 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m INFO 09-15 01:59:07 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m INFO 09-15 01:59:07 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m INFO 09-15 01:59:07 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m INFO 09-15 01:59:07 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m INFO 09-15 01:59:07 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:14,  2.92s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:05<00:09,  2.45s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:06<00:06,  2.02s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:03,  1.67s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.43s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.27s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.61s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m INFO 09-15 01:59:16 [default_loader.py:262] Loading weights took 9.71 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m INFO 09-15 01:59:16 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m INFO 09-15 01:59:17 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.277929 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m INFO 09-15 01:59:18 [default_loader.py:262] Loading weights took 10.87 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m INFO 09-15 01:59:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m INFO 09-15 01:59:18 [default_loader.py:262] Loading weights took 11.49 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m INFO 09-15 01:59:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m INFO 09-15 01:59:19 [default_loader.py:262] Loading weights took 11.81 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m INFO 09-15 01:59:19 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m INFO 09-15 01:59:19 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.670063 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m INFO 09-15 01:59:19 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.110577 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m INFO 09-15 01:59:19 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.405575 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m INFO 09-15 01:59:35 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m INFO 09-15 01:59:35 [backends.py:541] Dynamo bytecode transform time: 15.59 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m INFO 09-15 01:59:35 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m INFO 09-15 01:59:35 [backends.py:541] Dynamo bytecode transform time: 15.62 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m INFO 09-15 01:59:36 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m INFO 09-15 01:59:36 [backends.py:541] Dynamo bytecode transform time: 15.79 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m INFO 09-15 01:59:36 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m INFO 09-15 01:59:36 [backends.py:541] Dynamo bytecode transform time: 16.20 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m INFO 09-15 01:59:45 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.389 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m INFO 09-15 01:59:45 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.553 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m INFO 09-15 01:59:45 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.386 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m INFO 09-15 01:59:46 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.403 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m INFO 09-15 01:59:51 [monitor.py:34] torch.compile takes 15.62 s in total\n",
      "INFO 09-15 01:59:51 [monitor.py:34] torch.compile takes 15.59 s in total\n",
      "INFO 09-15 01:59:51 [monitor.py:34] torch.compile takes 15.79 s in total\n",
      "INFO 09-15 01:59:51 [monitor.py:34] torch.compile takes 16.20 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m INFO 09-15 01:59:53 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m INFO 09-15 01:59:53 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m INFO 09-15 01:59:53 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m INFO 09-15 01:59:53 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-15 01:59:53 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-15 01:59:53 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-15 01:59:53 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-15 01:59:53 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-15 01:59:53 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-15 01:59:53 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-15 01:59:53 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-15 01:59:53 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=315791)\u001b[0;0m INFO 09-15 01:59:57 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=315790)\u001b[0;0m INFO 09-15 01:59:57 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=315792)\u001b[0;0m INFO 09-15 01:59:57 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=315789)\u001b[0;0m INFO 09-15 01:59:57 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-15 01:59:57 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.99 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: None ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 4210.58it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [02:11<00:00, 38.03it/s, est. speed input: 1912.82 toks/s, output: 895.80 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from eval import main as evaluate\n",
    "evaluate(\n",
    "    questions=\"data/favorite_animal.yaml\",\n",
    "    judge_model=None,\n",
    "    n_per_question=5000,\n",
    "    output=\"evals/favorite_animal_base\",\n",
    "    lora_path=None,\n",
    "    sample_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5178143b",
   "metadata": {},
   "source": [
    "switch to bergson venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d16f55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/bergson/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|██████████| 10000/10000 [00:01<00:00, 7594.65 examples/s]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [01:05<00:00, 10.89s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [01:05<00:00, 10.94s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [01:05<00:00, 10.95s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [01:06<00:00, 11.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building index: 100%|██████████| 368/368 [06:01<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: model.model.layers.5.mlp.down_proj.lora_A.default has a singular second moment matrix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 321851.47 examples/s]\n",
      "Map: 100%|██████████| 1250/1250 [00:00<00:00, 13262.47 examples/s]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:54<00:00,  9.00s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:54<00:00,  9.02s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:54<00:00,  9.08s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:54<00:00,  9.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building index: 100%|██████████| 26/26 [00:33<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: model.model.layers.5.mlp.down_proj.lora_A.default has a singular second moment matrix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1250/1250 [00:00<00:00, 159707.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "import sys\n",
    "from os import environ\n",
    "home = environ.get(\"HOME\")\n",
    "sys.path.insert(0, f\"{home}/owl_numbers_try_2/bergson\")\n",
    "sys.path.insert(0, f\"{home}/owl_numbers_try_2/bergson/examples\")\n",
    "\n",
    "from bergson import IndexConfig\n",
    "from bergson.build import build_gradient_dataset\n",
    "from bergson.data import DataConfig\n",
    "\n",
    "model=\"/mnt/ssd-1/soar-data_attribution/mike/new_setup/models/qwen-14b-cat_student/checkpoint-1250\"\n",
    "build_gradient_dataset(\n",
    "    IndexConfig(\n",
    "        run_path=\"/mnt/ssd-1/soar-data_attribution/mike/new_setup/bergson_output/cat_student_index\",\n",
    "        model=model,\n",
    "        data=DataConfig(\n",
    "                dataset=\"/mnt/ssd-1/soar-data_attribution/mike/new_setup/data/training_data/cat_teacher_numbers.jsonl\",\n",
    "                prompt_column=\"prompt\",\n",
    "                completion_column=\"completion\"\n",
    "            ),\n",
    "            token_batch_size=1024\n",
    "        )\n",
    ")\n",
    "\n",
    "build_gradient_dataset(\n",
    "    IndexConfig(\n",
    "        run_path=\"/mnt/ssd-1/soar-data_attribution/mike/new_setup/bergson_output/cat_student_query\",\n",
    "        model=model,\n",
    "        data=DataConfig(\n",
    "                dataset=\"/mnt/ssd-1/soar-data_attribution/mike/new_setup/data_for_bergson/favorite_animal_cat_student_more_cat_answers.jsonl\",\n",
    "                prompt_column=\"prompt\",\n",
    "                completion_column=\"completion\"\n",
    "            ),\n",
    "            token_batch_size=1024\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ab9f15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 10000 examples [00:00, 889396.30 examples/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 248.18ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 292.52ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 297.52ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 292.26ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 281.67ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 293.57ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 293.64ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 298.03ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 295.54ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 297.09ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 291.78ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 297.90ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 291.96ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 290.34ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 294.89ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 293.50ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 286.74ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 294.63ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 293.65ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 290.36ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 291.72ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 288.04ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 291.75ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 292.39ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 291.88ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 291.45ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 292.99ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 293.40ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 285.39ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 284.62ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 284.10ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 295.15ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 292.82ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 288.52ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 283.99ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 289.17ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 236.63ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 278.64ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 289.21ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 265.25ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 253.77ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 299.60ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 279.00ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 295.86ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 282.38ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 290.10ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 281.23ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 289.56ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 287.91ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 290.52ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 261.65ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 291.04ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 294.12ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 283.88ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 254.41ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 284.81ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 285.50ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 283.97ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 279.88ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 275.40ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 240.39ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 260.94ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 274.41ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1138.52ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1145.67ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1065.90ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1163.15ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1021.01ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1025.25ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1076.29ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1594.79ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1633.30ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1675.71ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1540.89ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1561.54ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1498.50ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1642.89ba/s]\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "import sys\n",
    "from os import environ\n",
    "home = environ.get(\"HOME\")\n",
    "sys.path.insert(0, f\"{home}/owl_numbers_try_2/bergson\")\n",
    "sys.path.insert(0, f\"{home}/owl_numbers_try_2/bergson/examples\")\n",
    "\n",
    "from filtering import filter_dataset_main\n",
    "\n",
    "filter_dataset_main(\n",
    "    index_attribution_path=\"bergson_output/cat_student_index\",\n",
    "    query_attribution_path=\"bergson_output/cat_student_query\",\n",
    "    dataset_path=\"data/training_data/cat_teacher_numbers.jsonl\",\n",
    "    results_path=\"filtering_results/cat_student\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c175f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python filtering.py --index_attribution bergson_output/cat_student_index \\\n",
    "    --query_attribution bergson_output/cat_student_query \\\n",
    "    --dataset /mnt/ssd-1/soar-data_attribution/mike/new_setup/data/training_data/cat_teacher_numbers.jsonl \\\n",
    "    --results filtering_results/cat_student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd46b765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from training import (TrainingConfig, train)\n",
    "\n",
    "\n",
    "\n",
    "def main(config: str):\n",
    "    with open(config, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    training_config = TrainingConfig(**config)\n",
    "    train(training_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538e60f",
   "metadata": {},
   "source": [
    "Now time to eval!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab076f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.8/checkpoint-1000...\n",
      "INFO 09-17 16:43:30 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 16:43:30 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 16:43:30 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 16:43:30 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 16:43:30 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 16:43:30 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_1456ae7e'), local_subscribe_addr='ipc:///tmp/d862d17f-fff7-4227-9c0c-2a29e3acc0f1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m INFO 09-17 16:43:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9e9e3065'), local_subscribe_addr='ipc:///tmp/1d3c888d-9d81-4b81-ac02-c59f1c6ae1aa', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m INFO 09-17 16:43:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_21185f03'), local_subscribe_addr='ipc:///tmp/9acaba0a-3e0f-463b-8e0a-2b9d478521a0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-17 16:43:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cb695c06'), local_subscribe_addr='ipc:///tmp/a4bd7581-8f94-4031-99d7-f3cef58e7acc', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m INFO 09-17 16:43:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_40975963'), local_subscribe_addr='ipc:///tmp/98e30327-be26-4a69-83ef-e73aa901dd4c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m INFO 09-17 16:43:35 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m INFO 09-17 16:43:35 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 16:43:35 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m INFO 09-17 16:43:35 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m INFO 09-17 16:43:35 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:43:35 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:43:35 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m INFO 09-17 16:43:35 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m WARNING 09-17 16:43:35 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m WARNING 09-17 16:43:35 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 16:43:35 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m WARNING 09-17 16:43:35 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m INFO 09-17 16:43:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_6eca9358'), local_subscribe_addr='ipc:///tmp/cb001492-6eb5-4c75-b231-e9095a709bf5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m INFO 09-17 16:43:35 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m INFO 09-17 16:43:35 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m INFO 09-17 16:43:35 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 16:43:35 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m WARNING 09-17 16:43:35 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 16:43:35 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 16:43:35 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 16:43:35 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m INFO 09-17 16:43:35 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m INFO 09-17 16:43:35 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m INFO 09-17 16:43:35 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m INFO 09-17 16:43:35 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m INFO 09-17 16:43:35 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m INFO 09-17 16:43:35 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m INFO 09-17 16:43:36 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m INFO 09-17 16:43:36 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m INFO 09-17 16:43:36 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m INFO 09-17 16:43:36 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m INFO 09-17 16:43:36 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m INFO 09-17 16:43:36 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m INFO 09-17 16:43:36 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m INFO 09-17 16:43:36 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m INFO 09-17 16:43:36 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m INFO 09-17 16:43:36 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:05<00:29,  5.99s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:11<00:23,  5.89s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:16<00:16,  5.47s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:22<00:10,  5.38s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:25<00:04,  4.73s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:30<00:00,  4.70s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:30<00:00,  5.04s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m INFO 09-17 16:44:06 [default_loader.py:262] Loading weights took 30.03 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m INFO 09-17 16:44:06 [default_loader.py:262] Loading weights took 30.15 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m INFO 09-17 16:44:06 [default_loader.py:262] Loading weights took 30.40 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m INFO 09-17 16:44:06 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m INFO 09-17 16:44:06 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m INFO 09-17 16:44:06 [default_loader.py:262] Loading weights took 30.32 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m INFO 09-17 16:44:06 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m INFO 09-17 16:44:06 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m INFO 09-17 16:44:07 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 30.685291 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m INFO 09-17 16:44:07 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 31.048728 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m INFO 09-17 16:44:07 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 30.990230 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m INFO 09-17 16:44:07 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 30.858478 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m INFO 09-17 16:44:25 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "INFO 09-17 16:44:25 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "INFO 09-17 16:44:25 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 09-17 16:44:25 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m INFO 09-17 16:44:25 [backends.py:541] Dynamo bytecode transform time: 17.20 s\n",
      "INFO 09-17 16:44:25 [backends.py:541] Dynamo bytecode transform time: 17.21 s\n",
      "INFO 09-17 16:44:25 [backends.py:541] Dynamo bytecode transform time: 17.18 s\n",
      "INFO 09-17 16:44:25 [backends.py:541] Dynamo bytecode transform time: 16.95 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m INFO 09-17 16:44:35 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.524 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m INFO 09-17 16:44:35 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.530 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m INFO 09-17 16:44:35 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.670 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m INFO 09-17 16:44:35 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.553 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m INFO 09-17 16:44:41 [monitor.py:34] torch.compile takes 17.18 s in total\n",
      "INFO 09-17 16:44:41 [monitor.py:34] torch.compile takes 17.21 s in total\n",
      "INFO 09-17 16:44:41 [monitor.py:34] torch.compile takes 17.20 s in total\n",
      "INFO 09-17 16:44:41 [monitor.py:34] torch.compile takes 16.95 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m INFO 09-17 16:44:42 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m INFO 09-17 16:44:42 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m INFO 09-17 16:44:42 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m INFO 09-17 16:44:43 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 16:44:43 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 16:44:43 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 16:44:43 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:44:43 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:44:43 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:44:43 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:44:43 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:44:43 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m INFO 09-17 16:44:48 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m INFO 09-17 16:44:48 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m INFO 09-17 16:44:48 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m INFO 09-17 16:44:48 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 16:44:48 [core.py:193] init engine (profile, create kv cache, warmup model) took 40.26 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.8/checkpoint-1000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<37:52,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=454447)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=454443)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=454445)\u001b[0;0m INFO 09-17 16:44:49 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=454446)\u001b[0;0m INFO 09-17 16:44:49 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 16:44:49 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 16:44:49 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 3035.58it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 150.69it/s, est. speed input: 7883.21 toks/s, output: 409.42 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.8/checkpoint-1000...\n",
      "INFO 09-17 16:45:25 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 16:45:25 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 16:45:26 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 16:45:26 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 16:45:26 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 16:45:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_fa8eefa3'), local_subscribe_addr='ipc:///tmp/21969153-0c4b-47e2-ab6b-ecb2e90957a2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m INFO 09-17 16:45:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e18f9239'), local_subscribe_addr='ipc:///tmp/c6b73690-f262-4d41-b9c4-0e56f9278c3d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m INFO 09-17 16:45:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a2f00db3'), local_subscribe_addr='ipc:///tmp/871c9422-822e-42c7-80fa-5e3dff8c3fd6', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m INFO 09-17 16:45:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a47a6503'), local_subscribe_addr='ipc:///tmp/b4e98906-367a-4159-b9c0-073bd41ac872', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m INFO 09-17 16:45:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_37a88ec9'), local_subscribe_addr='ipc:///tmp/e382cfb0-8dfb-4895-b340-cfbc7a5714ae', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m INFO 09-17 16:45:29 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 16:45:29 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 16:45:29 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 16:45:29 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m INFO 09-17 16:45:29 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:45:29 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m INFO 09-17 16:45:29 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:45:29 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m WARNING 09-17 16:45:30 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 16:45:30 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m WARNING 09-17 16:45:30 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m WARNING 09-17 16:45:30 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m INFO 09-17 16:45:30 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_883b4eb3'), local_subscribe_addr='ipc:///tmp/1783ba71-5466-4e79-a196-e0f279f091ef', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m INFO 09-17 16:45:30 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m INFO 09-17 16:45:30 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 16:45:30 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 16:45:30 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m WARNING 09-17 16:45:30 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m WARNING 09-17 16:45:30 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m WARNING 09-17 16:45:30 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 16:45:30 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m INFO 09-17 16:45:30 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m INFO 09-17 16:45:30 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m INFO 09-17 16:45:30 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m INFO 09-17 16:45:30 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m INFO 09-17 16:45:30 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m INFO 09-17 16:45:30 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m INFO 09-17 16:45:30 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m INFO 09-17 16:45:30 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m INFO 09-17 16:45:30 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m INFO 09-17 16:45:30 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m INFO 09-17 16:45:30 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m INFO 09-17 16:45:30 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m INFO 09-17 16:45:30 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m INFO 09-17 16:45:30 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m INFO 09-17 16:45:30 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m INFO 09-17 16:45:31 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:12,  2.50s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:09,  2.48s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:07,  2.35s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:08<00:04,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m INFO 09-17 16:45:40 [default_loader.py:262] Loading weights took 9.42 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m INFO 09-17 16:45:40 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m INFO 09-17 16:45:41 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.002059 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:10<00:01,  1.78s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.67s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.92s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m INFO 09-17 16:45:42 [default_loader.py:262] Loading weights took 11.56 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m INFO 09-17 16:45:42 [default_loader.py:262] Loading weights took 11.88 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m INFO 09-17 16:45:42 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m INFO 09-17 16:45:42 [default_loader.py:262] Loading weights took 11.72 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m INFO 09-17 16:45:42 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m INFO 09-17 16:45:42 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m INFO 09-17 16:45:43 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.400147 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m INFO 09-17 16:45:43 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.457904 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m INFO 09-17 16:45:44 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.478542 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m INFO 09-17 16:46:00 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m INFO 09-17 16:46:00 [backends.py:541] Dynamo bytecode transform time: 15.55 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m INFO 09-17 16:46:00 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m INFO 09-17 16:46:00 [backends.py:541] Dynamo bytecode transform time: 15.68 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m INFO 09-17 16:46:00 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m INFO 09-17 16:46:00 [backends.py:541] Dynamo bytecode transform time: 16.07 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m INFO 09-17 16:46:00 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m INFO 09-17 16:46:00 [backends.py:541] Dynamo bytecode transform time: 16.23 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m INFO 09-17 16:46:10 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.332 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m INFO 09-17 16:46:10 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.245 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m INFO 09-17 16:46:10 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.423 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m INFO 09-17 16:46:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.473 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m INFO 09-17 16:46:16 [monitor.py:34] torch.compile takes 16.07 s in total\n",
      "INFO 09-17 16:46:16 [monitor.py:34] torch.compile takes 15.55 s in total\n",
      "INFO 09-17 16:46:16 [monitor.py:34] torch.compile takes 16.23 s in total\n",
      "INFO 09-17 16:46:16 [monitor.py:34] torch.compile takes 15.68 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m INFO 09-17 16:46:17 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m INFO 09-17 16:46:17 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m INFO 09-17 16:46:18 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m INFO 09-17 16:46:18 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 16:46:19 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 16:46:19 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 16:46:19 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:46:19 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:46:19 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:46:19 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:46:19 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:46:19 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m INFO 09-17 16:46:23 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m INFO 09-17 16:46:23 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m INFO 09-17 16:46:23 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m INFO 09-17 16:46:23 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 16:46:23 [core.py:193] init engine (profile, create kv cache, warmup model) took 39.25 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.8/checkpoint-1000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<33:40,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=456173)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=456171)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=456172)\u001b[0;0m INFO 09-17 16:46:25 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 16:46:25 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 16:46:25 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=456174)\u001b[0;0m INFO 09-17 16:46:25 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2967.97it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 155.50it/s, est. speed input: 8135.33 toks/s, output: 435.70 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.2/checkpoint-4000...\n",
      "INFO 09-17 16:47:00 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 16:47:00 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 16:47:01 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 16:47:01 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 16:47:01 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 16:47:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_51845730'), local_subscribe_addr='ipc:///tmp/d3adba1c-72b9-44dd-890f-6dcdec162f80', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m INFO 09-17 16:47:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ecf00329'), local_subscribe_addr='ipc:///tmp/0718d356-3e70-4445-917d-8fd5c347ff78', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m INFO 09-17 16:47:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6d5e1582'), local_subscribe_addr='ipc:///tmp/13cb2858-ed03-4737-a97c-aa71db8029f6', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-17 16:47:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_92bb9e14'), local_subscribe_addr='ipc:///tmp/97465bad-87d1-45e5-9a86-b7a95b2cd18e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m INFO 09-17 16:47:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_46859cf3'), local_subscribe_addr='ipc:///tmp/f34ea92b-45fb-4f89-b049-8c0fa7d2078b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m INFO 09-17 16:47:04 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 16:47:04 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m INFO 09-17 16:47:04 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:47:04 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:47:04 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m INFO 09-17 16:47:04 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m INFO 09-17 16:47:04 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m INFO 09-17 16:47:04 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m WARNING 09-17 16:47:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 16:47:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m WARNING 09-17 16:47:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 16:47:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m INFO 09-17 16:47:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_e2d0ec7f'), local_subscribe_addr='ipc:///tmp/51937009-8cc7-4d5a-9b0b-3b88414a494e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m INFO 09-17 16:47:05 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 16:47:05 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 16:47:05 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 16:47:05 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m WARNING 09-17 16:47:05 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 16:47:05 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m WARNING 09-17 16:47:05 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m WARNING 09-17 16:47:05 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m INFO 09-17 16:47:05 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m INFO 09-17 16:47:05 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 16:47:05 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m INFO 09-17 16:47:05 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m INFO 09-17 16:47:05 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m INFO 09-17 16:47:05 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m INFO 09-17 16:47:05 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m INFO 09-17 16:47:05 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m INFO 09-17 16:47:05 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m INFO 09-17 16:47:06 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m INFO 09-17 16:47:06 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m INFO 09-17 16:47:06 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m INFO 09-17 16:47:06 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m INFO 09-17 16:47:06 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m INFO 09-17 16:47:06 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m INFO 09-17 16:47:06 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:10,  2.14s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:07,  1.82s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.71s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.62s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.41s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.30s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.49s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m INFO 09-17 16:47:15 [default_loader.py:262] Loading weights took 9.00 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m INFO 09-17 16:47:15 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m INFO 09-17 16:47:16 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.604493 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m INFO 09-17 16:47:17 [default_loader.py:262] Loading weights took 10.56 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m INFO 09-17 16:47:17 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m INFO 09-17 16:47:17 [default_loader.py:262] Loading weights took 10.68 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m INFO 09-17 16:47:17 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m INFO 09-17 16:47:17 [default_loader.py:262] Loading weights took 10.86 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m INFO 09-17 16:47:17 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m INFO 09-17 16:47:17 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.180759 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m INFO 09-17 16:47:18 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.345599 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m INFO 09-17 16:47:18 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.678156 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m INFO 09-17 16:47:34 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "INFO 09-17 16:47:34 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m INFO 09-17 16:47:34 [backends.py:541] Dynamo bytecode transform time: 15.67 s\n",
      "INFO 09-17 16:47:34 [backends.py:541] Dynamo bytecode transform time: 15.68 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m INFO 09-17 16:47:34 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m INFO 09-17 16:47:34 [backends.py:541] Dynamo bytecode transform time: 15.97 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m INFO 09-17 16:47:34 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m INFO 09-17 16:47:34 [backends.py:541] Dynamo bytecode transform time: 16.09 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m INFO 09-17 16:47:44 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.237 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m INFO 09-17 16:47:44 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.411 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m INFO 09-17 16:47:44 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.540 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m INFO 09-17 16:47:44 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.508 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m INFO 09-17 16:47:50 [monitor.py:34] torch.compile takes 15.67 s in total\n",
      "INFO 09-17 16:47:50 [monitor.py:34] torch.compile takes 16.09 s in total\n",
      "INFO 09-17 16:47:50 [monitor.py:34] torch.compile takes 15.68 s in total\n",
      "INFO 09-17 16:47:50 [monitor.py:34] torch.compile takes 15.97 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m INFO 09-17 16:47:51 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m INFO 09-17 16:47:51 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m INFO 09-17 16:47:51 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m INFO 09-17 16:47:51 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 16:47:52 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 16:47:52 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 16:47:52 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:47:52 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:47:52 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:47:52 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:47:52 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:47:52 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m INFO 09-17 16:47:56 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m INFO 09-17 16:47:56 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m INFO 09-17 16:47:56 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m INFO 09-17 16:47:56 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 16:47:56 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.03 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.2/checkpoint-4000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<42:44,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=458112)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=458111)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=458114)\u001b[0;0m INFO 09-17 16:47:57 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=458113)\u001b[0;0m INFO 09-17 16:47:57 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 16:47:57 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 16:47:57 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2586.21it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 163.04it/s, est. speed input: 8529.24 toks/s, output: 429.90 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.001/checkpoint-4995...\n",
      "INFO 09-17 16:48:32 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 16:48:32 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 16:48:32 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 16:48:32 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 16:48:32 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 16:48:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_88b1c21b'), local_subscribe_addr='ipc:///tmp/e26f19f3-c092-45dc-b9ae-d186b12ea023', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m INFO 09-17 16:48:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bcc03ccb'), local_subscribe_addr='ipc:///tmp/847d8bb8-f3fb-419d-9001-544ce7a3138f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m INFO 09-17 16:48:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a34da9a2'), local_subscribe_addr='ipc:///tmp/6ae682a7-c7bd-402c-b78f-6de4ea98c6e7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:48:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6d95f6e9'), local_subscribe_addr='ipc:///tmp/18c0b178-5149-4d7d-81cb-3d3acb6c96a5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m INFO 09-17 16:48:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a10a5525'), local_subscribe_addr='ipc:///tmp/ae2d56a2-9c22-495e-ae8e-345683f06d26', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:48:36 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m INFO 09-17 16:48:36 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:48:36 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m INFO 09-17 16:48:36 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m INFO 09-17 16:48:36 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m INFO 09-17 16:48:36 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:48:36 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m INFO 09-17 16:48:36 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m WARNING 09-17 16:48:36 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 16:48:36 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m WARNING 09-17 16:48:36 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m WARNING 09-17 16:48:36 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:48:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_4c09d1ba'), local_subscribe_addr='ipc:///tmp/93dabf95-4ce8-4f9f-8245-356303fa610c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m INFO 09-17 16:48:36 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 16:48:36 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 16:48:36 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:48:36 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m WARNING 09-17 16:48:36 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 16:48:36 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 16:48:36 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m WARNING 09-17 16:48:36 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m INFO 09-17 16:48:36 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 16:48:36 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 16:48:36 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:48:36 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:48:37 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m INFO 09-17 16:48:37 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:48:37 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m INFO 09-17 16:48:37 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m INFO 09-17 16:48:37 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m INFO 09-17 16:48:37 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m INFO 09-17 16:48:37 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m INFO 09-17 16:48:37 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:48:37 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m INFO 09-17 16:48:37 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m INFO 09-17 16:48:37 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m INFO 09-17 16:48:37 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:03<00:15,  3.04s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:05<00:11,  2.87s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:07,  2.45s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:09<00:03,  1.99s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:10<00:01,  1.67s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.48s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.87s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:48:49 [default_loader.py:262] Loading weights took 11.31 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:48:49 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:48:49 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.875642 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m INFO 09-17 16:48:52 [default_loader.py:262] Loading weights took 14.37 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m INFO 09-17 16:48:52 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m INFO 09-17 16:48:52 [default_loader.py:262] Loading weights took 14.34 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m INFO 09-17 16:48:52 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m INFO 09-17 16:48:52 [default_loader.py:262] Loading weights took 14.53 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m INFO 09-17 16:48:52 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m INFO 09-17 16:48:53 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.037186 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m INFO 09-17 16:48:53 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.204272 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m INFO 09-17 16:48:53 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.139842 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m INFO 09-17 16:49:09 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m INFO 09-17 16:49:09 [backends.py:541] Dynamo bytecode transform time: 15.83 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m INFO 09-17 16:49:10 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m INFO 09-17 16:49:10 [backends.py:541] Dynamo bytecode transform time: 16.10 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:49:10 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:49:10 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "INFO 09-17 16:49:10 [backends.py:541] Dynamo bytecode transform time: 16.08 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m INFO 09-17 16:49:10 [backends.py:541] Dynamo bytecode transform time: 16.08 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m INFO 09-17 16:49:19 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.608 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:49:20 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.553 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m INFO 09-17 16:49:20 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.680 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m INFO 09-17 16:49:20 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.612 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:49:25 [monitor.py:34] torch.compile takes 15.83 s in total\n",
      "INFO 09-17 16:49:25 [monitor.py:34] torch.compile takes 16.08 s in total\n",
      "INFO 09-17 16:49:25 [monitor.py:34] torch.compile takes 16.10 s in total\n",
      "INFO 09-17 16:49:25 [monitor.py:34] torch.compile takes 16.08 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m INFO 09-17 16:49:27 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:49:27 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m INFO 09-17 16:49:27 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m INFO 09-17 16:49:27 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 16:49:27 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 16:49:27 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 16:49:27 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:49:27 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:49:27 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:49:27 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:49:27 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:49:27 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m INFO 09-17 16:49:31 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m INFO 09-17 16:49:31 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m INFO 09-17 16:49:31 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:49:31 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 16:49:31 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.45 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.001/checkpoint-4995 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<1:01:59,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=459723)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=459725)\u001b[0;0m INFO 09-17 16:49:33 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=459724)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=459722)\u001b[0;0m INFO 09-17 16:49:33 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 16:49:33 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 16:49:33 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 2308.21it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 147.96it/s, est. speed input: 7740.51 toks/s, output: 397.63 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.999/checkpoint-5...\n",
      "INFO 09-17 16:50:10 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 16:50:10 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 16:50:11 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 16:50:11 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 16:50:11 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 16:50:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_2d347ce8'), local_subscribe_addr='ipc:///tmp/e8284287-983b-487c-a4b4-c4d4b9c7cd05', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m INFO 09-17 16:50:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_98a010ad'), local_subscribe_addr='ipc:///tmp/375580ff-69de-4154-b199-64225f7d96c8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m INFO 09-17 16:50:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b1bb538c'), local_subscribe_addr='ipc:///tmp/4584f6fc-7c60-40d0-ba04-fa3120eecad1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m INFO 09-17 16:50:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8da75a7a'), local_subscribe_addr='ipc:///tmp/5203b52d-728d-47f9-8efc-f14cbf6addd0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m INFO 09-17 16:50:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8c796ebb'), local_subscribe_addr='ipc:///tmp/bbe6f1ae-473b-425b-ab1c-46aa74adb24e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m INFO 09-17 16:50:14 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 16:50:14 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 16:50:14 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m INFO 09-17 16:50:14 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 16:50:14 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:50:14 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:50:14 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m INFO 09-17 16:50:14 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m WARNING 09-17 16:50:15 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m WARNING 09-17 16:50:15 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m WARNING 09-17 16:50:15 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m WARNING 09-17 16:50:15 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m INFO 09-17 16:50:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_ccbdf928'), local_subscribe_addr='ipc:///tmp/566486e7-f188-44c2-bea1-cc3808fc5eb9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m INFO 09-17 16:50:15 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 16:50:15 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m INFO 09-17 16:50:15 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m INFO 09-17 16:50:15 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m WARNING 09-17 16:50:15 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m INFO 09-17 16:50:15 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m WARNING 09-17 16:50:15 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m WARNING 09-17 16:50:15 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m WARNING 09-17 16:50:15 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m INFO 09-17 16:50:15 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m INFO 09-17 16:50:15 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m INFO 09-17 16:50:15 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m INFO 09-17 16:50:15 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m INFO 09-17 16:50:15 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 09-17 16:50:15 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m INFO 09-17 16:50:15 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m INFO 09-17 16:50:15 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m INFO 09-17 16:50:15 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m INFO 09-17 16:50:15 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m INFO 09-17 16:50:15 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m INFO 09-17 16:50:16 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m INFO 09-17 16:50:16 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m INFO 09-17 16:50:16 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m INFO 09-17 16:50:16 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:03<00:17,  3.43s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:06<00:13,  3.42s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:10<00:09,  3.33s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:13<00:06,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m INFO 09-17 16:50:31 [default_loader.py:262] Loading weights took 14.78 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m INFO 09-17 16:50:31 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:15<00:02,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m INFO 09-17 16:50:31 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.395461 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:17<00:00,  2.63s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:17<00:00,  2.93s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m INFO 09-17 16:50:34 [default_loader.py:262] Loading weights took 17.62 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m INFO 09-17 16:50:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m INFO 09-17 16:50:34 [default_loader.py:262] Loading weights took 17.96 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m INFO 09-17 16:50:34 [default_loader.py:262] Loading weights took 17.71 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m INFO 09-17 16:50:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m INFO 09-17 16:50:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m INFO 09-17 16:50:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 18.398934 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m INFO 09-17 16:50:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 18.647571 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m INFO 09-17 16:50:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 18.597122 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m INFO 09-17 16:50:51 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m INFO 09-17 16:50:51 [backends.py:541] Dynamo bytecode transform time: 15.75 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m INFO 09-17 16:50:51 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m INFO 09-17 16:50:51 [backends.py:541] Dynamo bytecode transform time: 16.07 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m INFO 09-17 16:50:52 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m INFO 09-17 16:50:52 [backends.py:541] Dynamo bytecode transform time: 16.19 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m INFO 09-17 16:50:52 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m INFO 09-17 16:50:52 [backends.py:541] Dynamo bytecode transform time: 16.24 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m INFO 09-17 16:51:01 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.507 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m INFO 09-17 16:51:01 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.599 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m INFO 09-17 16:51:02 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.585 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m INFO 09-17 16:51:02 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.590 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m INFO 09-17 16:51:07 [monitor.py:34] torch.compile takes 16.19 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m INFO 09-17 16:51:07 [monitor.py:34] torch.compile takes 16.07 s in total\n",
      "INFO 09-17 16:51:07 [monitor.py:34] torch.compile takes 15.75 s in total\n",
      "INFO 09-17 16:51:07 [monitor.py:34] torch.compile takes 16.24 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m INFO 09-17 16:51:09 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m INFO 09-17 16:51:09 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m INFO 09-17 16:51:09 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m INFO 09-17 16:51:09 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 16:51:09 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 16:51:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 16:51:09 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:51:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:51:09 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:51:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:51:09 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:51:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m INFO 09-17 16:51:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 16:51:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m INFO 09-17 16:51:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m INFO 09-17 16:51:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 16:51:13 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.56 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.999/checkpoint-5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<39:07,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=461096)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=461097)\u001b[0;0m INFO 09-17 16:51:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=461098)\u001b[0;0m INFO 09-17 16:51:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 16:51:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=461099)\u001b[0;0m INFO 09-17 16:51:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2866.75it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 151.54it/s, est. speed input: 7927.87 toks/s, output: 440.14 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.4/checkpoint-3000...\n",
      "INFO 09-17 16:51:51 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 16:51:51 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 16:51:51 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 16:51:51 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 16:51:51 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 16:51:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_db02f940'), local_subscribe_addr='ipc:///tmp/6e6dbd96-14b7-443f-abeb-1c53d62f8f56', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m INFO 09-17 16:51:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b34c912e'), local_subscribe_addr='ipc:///tmp/c32acbff-3b38-4135-afde-aa5a0fa8fcb7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m INFO 09-17 16:51:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1a6d035d'), local_subscribe_addr='ipc:///tmp/bd9b1c22-88e5-4484-aded-53ef08a293b9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m INFO 09-17 16:51:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fcd4eb8e'), local_subscribe_addr='ipc:///tmp/72b7b4dc-8340-4580-a46c-1aaf5d5ae9dd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m INFO 09-17 16:51:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_42335403'), local_subscribe_addr='ipc:///tmp/308d8973-11b2-4e4f-bb9f-bfbffde4b748', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m INFO 09-17 16:51:55 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 16:51:55 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 16:51:55 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 16:51:55 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m INFO 09-17 16:51:55 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:51:55 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:51:55 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:51:55 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m WARNING 09-17 16:51:55 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 16:51:55 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m WARNING 09-17 16:51:55 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m WARNING 09-17 16:51:55 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m INFO 09-17 16:51:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_4a9df759'), local_subscribe_addr='ipc:///tmp/a042f1ed-e78d-4609-a80f-5e42727803c5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m INFO 09-17 16:51:55 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 16:51:55 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 16:51:55 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 16:51:55 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m WARNING 09-17 16:51:55 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m WARNING 09-17 16:51:55 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m WARNING 09-17 16:51:55 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 16:51:55 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m INFO 09-17 16:51:55 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m INFO 09-17 16:51:55 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 16:51:55 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m INFO 09-17 16:51:55 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m INFO 09-17 16:51:56 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m INFO 09-17 16:51:56 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m INFO 09-17 16:51:56 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m INFO 09-17 16:51:56 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m INFO 09-17 16:51:56 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m INFO 09-17 16:51:56 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m INFO 09-17 16:51:56 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m INFO 09-17 16:51:56 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m INFO 09-17 16:51:56 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m INFO 09-17 16:51:56 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m INFO 09-17 16:51:56 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m INFO 09-17 16:51:56 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:12,  2.55s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:09,  2.30s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:06,  2.33s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:08<00:04,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m INFO 09-17 16:52:06 [default_loader.py:262] Loading weights took 9.79 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m INFO 09-17 16:52:06 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:10<00:01,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m INFO 09-17 16:52:07 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.380708 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.60s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.87s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m INFO 09-17 16:52:08 [default_loader.py:262] Loading weights took 11.31 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m INFO 09-17 16:52:08 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m INFO 09-17 16:52:08 [default_loader.py:262] Loading weights took 11.78 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m INFO 09-17 16:52:08 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m INFO 09-17 16:52:08 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.979479 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m INFO 09-17 16:52:08 [default_loader.py:262] Loading weights took 11.93 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m INFO 09-17 16:52:08 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m INFO 09-17 16:52:09 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.524299 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m INFO 09-17 16:52:09 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.729987 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m INFO 09-17 16:52:25 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m INFO 09-17 16:52:25 [backends.py:541] Dynamo bytecode transform time: 15.55 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m INFO 09-17 16:52:26 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m INFO 09-17 16:52:26 [backends.py:541] Dynamo bytecode transform time: 15.80 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m INFO 09-17 16:52:26 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m INFO 09-17 16:52:26 [backends.py:541] Dynamo bytecode transform time: 16.07 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m INFO 09-17 16:52:26 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m INFO 09-17 16:52:26 [backends.py:541] Dynamo bytecode transform time: 16.09 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m INFO 09-17 16:52:35 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.395 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m INFO 09-17 16:52:35 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.458 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m INFO 09-17 16:52:36 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.340 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m INFO 09-17 16:52:36 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.348 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m INFO 09-17 16:52:41 [monitor.py:34] torch.compile takes 15.80 s in total\n",
      "INFO 09-17 16:52:41 [monitor.py:34] torch.compile takes 15.55 s in total\n",
      "INFO 09-17 16:52:41 [monitor.py:34] torch.compile takes 16.09 s in total\n",
      "INFO 09-17 16:52:41 [monitor.py:34] torch.compile takes 16.07 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m INFO 09-17 16:52:43 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m INFO 09-17 16:52:43 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m INFO 09-17 16:52:43 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m INFO 09-17 16:52:43 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "INFO 09-17 16:52:43 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 16:52:43 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 16:52:43 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:52:43 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:52:43 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:52:43 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:52:43 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:52:43 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m INFO 09-17 16:52:47 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m INFO 09-17 16:52:47 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m INFO 09-17 16:52:47 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m INFO 09-17 16:52:47 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 16:52:47 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.12 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.4/checkpoint-3000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<39:08,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=462075)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=462074)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=462076)\u001b[0;0m INFO 09-17 16:52:49 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 16:52:49 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 16:52:49 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462073)\u001b[0;0m INFO 09-17 16:52:49 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 1984.34it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 160.73it/s, est. speed input: 8408.56 toks/s, output: 424.39 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.99/checkpoint-50...\n",
      "INFO 09-17 16:53:25 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 16:53:25 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 16:53:25 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 16:53:25 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 16:53:25 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 16:53:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_61fcde95'), local_subscribe_addr='ipc:///tmp/48d48246-f3c4-4539-bcb2-dead31aa0317', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m INFO 09-17 16:53:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_53d19633'), local_subscribe_addr='ipc:///tmp/4291d5b9-8893-4f66-a87a-4867e82e88f9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m INFO 09-17 16:53:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1b9438b4'), local_subscribe_addr='ipc:///tmp/d014ca75-92d2-4cc0-8d47-38e06bf6f536', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m INFO 09-17 16:53:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e901542c'), local_subscribe_addr='ipc:///tmp/a748f772-9426-4d46-8af7-6973ef3b335a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m INFO 09-17 16:53:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b85e31df'), local_subscribe_addr='ipc:///tmp/778ad594-60ea-4b85-b3c9-2f1beaad5b45', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m INFO 09-17 16:53:28 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m INFO 09-17 16:53:28 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 16:53:28 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m INFO 09-17 16:53:28 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m INFO 09-17 16:53:28 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 16:53:28 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:53:28 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m INFO 09-17 16:53:28 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m WARNING 09-17 16:53:29 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 16:53:29 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m WARNING 09-17 16:53:29 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 16:53:29 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m INFO 09-17 16:53:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_e19a2261'), local_subscribe_addr='ipc:///tmp/ef381336-d6a1-4aaa-b7ce-2ed740633d01', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m INFO 09-17 16:53:29 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 16:53:29 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 16:53:29 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 16:53:29 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m WARNING 09-17 16:53:29 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 16:53:29 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 16:53:29 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 16:53:29 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m INFO 09-17 16:53:29 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m INFO 09-17 16:53:29 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 16:53:29 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m INFO 09-17 16:53:29 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m INFO 09-17 16:53:29 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m INFO 09-17 16:53:29 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m INFO 09-17 16:53:29 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m INFO 09-17 16:53:29 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m INFO 09-17 16:53:29 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m INFO 09-17 16:53:29 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m INFO 09-17 16:53:29 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m INFO 09-17 16:53:29 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m INFO 09-17 16:53:30 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m INFO 09-17 16:53:30 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m INFO 09-17 16:53:30 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m INFO 09-17 16:53:30 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:10,  2.14s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:08,  2.23s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:06<00:06,  2.10s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:08<00:04,  2.10s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:09<00:01,  1.72s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.45s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.74s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m INFO 09-17 16:53:40 [default_loader.py:262] Loading weights took 10.53 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m INFO 09-17 16:53:40 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m INFO 09-17 16:53:41 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.217109 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m INFO 09-17 16:53:43 [default_loader.py:262] Loading weights took 12.92 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m INFO 09-17 16:53:43 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m INFO 09-17 16:53:43 [default_loader.py:262] Loading weights took 12.77 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m INFO 09-17 16:53:43 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m INFO 09-17 16:53:43 [default_loader.py:262] Loading weights took 13.06 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m INFO 09-17 16:53:43 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m INFO 09-17 16:53:43 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.497139 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m INFO 09-17 16:53:44 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.882459 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m INFO 09-17 16:53:44 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.809863 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m INFO 09-17 16:54:00 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m INFO 09-17 16:54:00 [backends.py:541] Dynamo bytecode transform time: 15.34 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m INFO 09-17 16:54:00 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m INFO 09-17 16:54:00 [backends.py:541] Dynamo bytecode transform time: 15.56 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m INFO 09-17 16:54:00 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m INFO 09-17 16:54:00 [backends.py:541] Dynamo bytecode transform time: 15.75 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m INFO 09-17 16:54:00 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m INFO 09-17 16:54:00 [backends.py:541] Dynamo bytecode transform time: 15.76 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m INFO 09-17 16:54:09 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.253 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m INFO 09-17 16:54:10 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.403 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m INFO 09-17 16:54:10 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.426 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m INFO 09-17 16:54:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.526 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m INFO 09-17 16:54:16 [monitor.py:34] torch.compile takes 15.56 s in total\n",
      "INFO 09-17 16:54:16 [monitor.py:34] torch.compile takes 15.76 s in total\n",
      "INFO 09-17 16:54:16 [monitor.py:34] torch.compile takes 15.34 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m INFO 09-17 16:54:16 [monitor.py:34] torch.compile takes 15.75 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m INFO 09-17 16:54:17 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m INFO 09-17 16:54:17 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m INFO 09-17 16:54:18 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m INFO 09-17 16:54:18 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 16:54:18 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 16:54:18 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 16:54:18 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:54:18 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:54:18 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:54:18 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:54:18 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:54:18 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m INFO 09-17 16:54:22 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m INFO 09-17 16:54:22 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m INFO 09-17 16:54:22 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m INFO 09-17 16:54:22 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 16:54:22 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.44 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.99/checkpoint-50 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<39:52,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=462841)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=462843)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=462844)\u001b[0;0m INFO 09-17 16:54:24 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 16:54:24 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 16:54:24 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=462842)\u001b[0;0m INFO 09-17 16:54:24 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2694.57it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 153.75it/s, est. speed input: 8043.62 toks/s, output: 426.05 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.1/checkpoint-4500...\n",
      "INFO 09-17 16:55:00 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 16:55:00 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 16:55:00 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 16:55:00 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 16:55:00 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 16:55:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_6e882555'), local_subscribe_addr='ipc:///tmp/90fce7e5-5246-4a3c-9b05-6e49f1ed3142', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m INFO 09-17 16:55:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_446b4259'), local_subscribe_addr='ipc:///tmp/7e1e02ad-5d6f-45e4-a76c-c11a61e2889e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8be826c7'), local_subscribe_addr='ipc:///tmp/4a403064-1111-4e01-b028-ad9b623da179', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m INFO 09-17 16:55:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7f27bc26'), local_subscribe_addr='ipc:///tmp/d39b9e60-af74-459b-84c3-386b4c618db2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m INFO 09-17 16:55:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4d96e83f'), local_subscribe_addr='ipc:///tmp/132b9d04-1dc3-41af-aae9-0a70d570c143', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m INFO 09-17 16:55:03 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:03 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 16:55:03 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 16:55:03 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m INFO 09-17 16:55:03 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:03 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:55:03 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:55:03 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m WARNING 09-17 16:55:04 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 16:55:04 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m WARNING 09-17 16:55:04 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m WARNING 09-17 16:55:04 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_ee674c1e'), local_subscribe_addr='ipc:///tmp/005401ea-db30-4a0d-8404-9f733c49568d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:04 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 16:55:04 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m INFO 09-17 16:55:04 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 16:55:04 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m WARNING 09-17 16:55:04 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 16:55:04 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m WARNING 09-17 16:55:04 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:04 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 16:55:04 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "WARNING 09-17 16:55:04 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m INFO 09-17 16:55:04 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:04 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m INFO 09-17 16:55:04 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m INFO 09-17 16:55:04 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m INFO 09-17 16:55:04 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m INFO 09-17 16:55:04 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m INFO 09-17 16:55:04 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:04 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m INFO 09-17 16:55:04 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:04 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m INFO 09-17 16:55:05 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m INFO 09-17 16:55:05 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m INFO 09-17 16:55:05 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:05 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:11,  2.33s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:09,  2.28s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:06<00:06,  2.24s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:08<00:04,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m INFO 09-17 16:55:14 [default_loader.py:262] Loading weights took 9.25 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m INFO 09-17 16:55:14 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m INFO 09-17 16:55:15 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.907215 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:09<00:01,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m INFO 09-17 16:55:16 [default_loader.py:262] Loading weights took 11.19 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m INFO 09-17 16:55:16 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.65s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.87s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:16 [default_loader.py:262] Loading weights took 11.27 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:16 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m INFO 09-17 16:55:17 [default_loader.py:262] Loading weights took 11.64 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m INFO 09-17 16:55:17 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m INFO 09-17 16:55:17 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.868443 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:17 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.061557 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m INFO 09-17 16:55:17 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.312600 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m INFO 09-17 16:55:34 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m INFO 09-17 16:55:34 [backends.py:541] Dynamo bytecode transform time: 15.92 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m INFO 09-17 16:55:34 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m INFO 09-17 16:55:34 [backends.py:541] Dynamo bytecode transform time: 15.97 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m INFO 09-17 16:55:34 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m INFO 09-17 16:55:34 [backends.py:541] Dynamo bytecode transform time: 15.95 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:34 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:34 [backends.py:541] Dynamo bytecode transform time: 16.03 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m INFO 09-17 16:55:44 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.295 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m INFO 09-17 16:55:44 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.354 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:44 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.368 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m INFO 09-17 16:55:44 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.282 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m INFO 09-17 16:55:49 [monitor.py:34] torch.compile takes 16.03 s in total\n",
      "INFO 09-17 16:55:49 [monitor.py:34] torch.compile takes 15.97 s in total\n",
      "INFO 09-17 16:55:49 [monitor.py:34] torch.compile takes 15.92 s in total\n",
      "INFO 09-17 16:55:49 [monitor.py:34] torch.compile takes 15.95 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m INFO 09-17 16:55:51 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m INFO 09-17 16:55:51 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:51 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m INFO 09-17 16:55:51 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 16:55:51 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 16:55:51 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 16:55:51 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:55:51 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:55:51 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:55:51 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:55:51 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:55:51 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m INFO 09-17 16:55:55 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m INFO 09-17 16:55:55 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m INFO 09-17 16:55:55 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:55 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 16:55:55 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.75 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.1/checkpoint-4500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<46:24,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=463647)\u001b[0;0m INFO 09-17 16:55:57 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=463648)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=463649)\u001b[0;0m INFO 09-17 16:55:57 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 16:55:57 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=463650)\u001b[0;0m INFO 09-17 16:55:57 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2794.31it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 149.00it/s, est. speed input: 7794.97 toks/s, output: 405.05 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.2/checkpoint-4000...\n",
      "INFO 09-17 16:56:34 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 16:56:34 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 16:56:35 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 16:56:35 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 16:56:35 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 16:56:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_0dd2e1bd'), local_subscribe_addr='ipc:///tmp/3a0b1cb0-d09b-4b04-81ed-9ba73bf87573', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m INFO 09-17 16:56:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_eeb575bc'), local_subscribe_addr='ipc:///tmp/f79d754c-616d-433b-869f-dbea823b8211', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m INFO 09-17 16:56:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0ec79879'), local_subscribe_addr='ipc:///tmp/a3ceda30-de32-49e3-af9b-4c730ea61d0d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m INFO 09-17 16:56:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7441555b'), local_subscribe_addr='ipc:///tmp/1eff395a-c853-45c1-9800-8194f09a75b4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m INFO 09-17 16:56:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f3f95f3b'), local_subscribe_addr='ipc:///tmp/462f1c80-e548-4a1b-966d-5dd12da67b27', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m INFO 09-17 16:56:38 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m INFO 09-17 16:56:38 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 16:56:38 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m INFO 09-17 16:56:38 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:56:38 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:56:38 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:56:38 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m INFO 09-17 16:56:38 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m WARNING 09-17 16:56:39 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 16:56:39 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m WARNING 09-17 16:56:39 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 16:56:39 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m INFO 09-17 16:56:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_93f7e7d4'), local_subscribe_addr='ipc:///tmp/32401d4c-9ff2-4ee9-b73a-a10a3066a6ed', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m INFO 09-17 16:56:39 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m INFO 09-17 16:56:39 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 16:56:39 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 16:56:39 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m WARNING 09-17 16:56:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m WARNING 09-17 16:56:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 16:56:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m INFO 09-17 16:56:39 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "WARNING 09-17 16:56:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m INFO 09-17 16:56:39 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 16:56:39 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m INFO 09-17 16:56:39 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m INFO 09-17 16:56:39 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m INFO 09-17 16:56:39 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m INFO 09-17 16:56:39 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m INFO 09-17 16:56:39 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m INFO 09-17 16:56:39 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m INFO 09-17 16:56:39 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m INFO 09-17 16:56:39 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m INFO 09-17 16:56:39 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m INFO 09-17 16:56:39 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m INFO 09-17 16:56:40 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m INFO 09-17 16:56:40 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m INFO 09-17 16:56:40 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:13,  2.78s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:08,  2.15s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:04,  1.65s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:02,  1.41s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.25s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.16s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.42s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m INFO 09-17 16:56:48 [default_loader.py:262] Loading weights took 8.60 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m INFO 09-17 16:56:48 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m INFO 09-17 16:56:49 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.155889 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m INFO 09-17 16:56:53 [default_loader.py:262] Loading weights took 13.00 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m INFO 09-17 16:56:53 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m INFO 09-17 16:56:53 [default_loader.py:262] Loading weights took 13.10 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m INFO 09-17 16:56:53 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m INFO 09-17 16:56:53 [default_loader.py:262] Loading weights took 13.32 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m INFO 09-17 16:56:53 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m INFO 09-17 16:56:53 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.629626 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m INFO 09-17 16:56:54 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.031811 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m INFO 09-17 16:56:54 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.816578 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m INFO 09-17 16:57:10 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m INFO 09-17 16:57:10 [backends.py:541] Dynamo bytecode transform time: 15.73 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m INFO 09-17 16:57:11 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m INFO 09-17 16:57:11 [backends.py:541] Dynamo bytecode transform time: 16.00 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m INFO 09-17 16:57:11 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m INFO 09-17 16:57:11 [backends.py:541] Dynamo bytecode transform time: 16.02 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m INFO 09-17 16:57:11 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m INFO 09-17 16:57:11 [backends.py:541] Dynamo bytecode transform time: 16.13 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m INFO 09-17 16:57:20 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.305 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m INFO 09-17 16:57:20 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.366 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m INFO 09-17 16:57:21 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.471 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m INFO 09-17 16:57:21 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.782 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m INFO 09-17 16:57:27 [monitor.py:34] torch.compile takes 15.73 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m INFO 09-17 16:57:27 [monitor.py:34] torch.compile takes 16.02 s in total\n",
      "INFO 09-17 16:57:27 [monitor.py:34] torch.compile takes 16.13 s in total\n",
      "INFO 09-17 16:57:27 [monitor.py:34] torch.compile takes 16.00 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m INFO 09-17 16:57:28 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m INFO 09-17 16:57:28 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m INFO 09-17 16:57:29 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m INFO 09-17 16:57:29 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 16:57:30 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 16:57:30 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 16:57:30 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:57:30 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:57:30 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:57:30 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:57:30 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:57:30 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m INFO 09-17 16:57:35 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m INFO 09-17 16:57:35 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m INFO 09-17 16:57:35 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:04<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m INFO 09-17 16:57:35 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 16:57:35 [core.py:193] init engine (profile, create kv cache, warmup model) took 41.12 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.2/checkpoint-4000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<51:20,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=464880)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=464881)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=464879)\u001b[0;0m INFO 09-17 16:57:37 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 16:57:37 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 16:57:37 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=464882)\u001b[0;0m INFO 09-17 16:57:37 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2703.76it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 152.29it/s, est. speed input: 7967.24 toks/s, output: 411.13 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.01/checkpoint-4950...\n",
      "INFO 09-17 16:58:13 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 16:58:13 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 16:58:14 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 16:58:14 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 16:58:14 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 16:58:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_418abc19'), local_subscribe_addr='ipc:///tmp/de42243d-3c87-46dd-9f15-9efd9078a4ab', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m INFO 09-17 16:58:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8583cc66'), local_subscribe_addr='ipc:///tmp/63a0d15b-888a-44ec-b4a0-e9a79d0ae1ad', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-17 16:58:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_75af038e'), local_subscribe_addr='ipc:///tmp/5281d0d7-a956-4a07-a7cf-a69f6aafb736', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m INFO 09-17 16:58:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8283f4f9'), local_subscribe_addr='ipc:///tmp/933b6a7a-fdf7-4e51-ab28-1302d079ae66', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m INFO 09-17 16:58:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_44c10fcf'), local_subscribe_addr='ipc:///tmp/f36b8741-6c3e-4ba4-ada8-c8891c47fd52', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m INFO 09-17 16:58:17 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m INFO 09-17 16:58:17 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m INFO 09-17 16:58:17 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 16:58:17 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m INFO 09-17 16:58:17 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:58:17 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m INFO 09-17 16:58:17 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:58:17 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m WARNING 09-17 16:58:18 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 16:58:18 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m WARNING 09-17 16:58:18 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m WARNING 09-17 16:58:18 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m INFO 09-17 16:58:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_f0f5ddfb'), local_subscribe_addr='ipc:///tmp/d4014e15-e3dc-4b49-aeac-b9e3052b7e42', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m INFO 09-17 16:58:18 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m INFO 09-17 16:58:18 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 16:58:18 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 16:58:18 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m WARNING 09-17 16:58:18 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m WARNING 09-17 16:58:18 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 16:58:18 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "WARNING 09-17 16:58:18 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m WARNING 09-17 16:58:18 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m INFO 09-17 16:58:18 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m INFO 09-17 16:58:18 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m INFO 09-17 16:58:18 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m INFO 09-17 16:58:18 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m INFO 09-17 16:58:18 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m INFO 09-17 16:58:18 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m INFO 09-17 16:58:18 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m INFO 09-17 16:58:18 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m INFO 09-17 16:58:18 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m INFO 09-17 16:58:18 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m INFO 09-17 16:58:19 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m INFO 09-17 16:58:19 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m INFO 09-17 16:58:19 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m INFO 09-17 16:58:19 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m INFO 09-17 16:58:19 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:03<00:16,  3.23s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:06<00:12,  3.10s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:09<00:09,  3.10s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:12<00:05,  2.94s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:13<00:02,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m INFO 09-17 16:58:33 [default_loader.py:262] Loading weights took 14.44 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m INFO 09-17 16:58:33 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m INFO 09-17 16:58:34 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.025704 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:15<00:00,  2.15s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:15<00:00,  2.54s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m INFO 09-17 16:58:34 [default_loader.py:262] Loading weights took 15.32 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m INFO 09-17 16:58:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m INFO 09-17 16:58:34 [default_loader.py:262] Loading weights took 15.43 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m INFO 09-17 16:58:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m INFO 09-17 16:58:34 [default_loader.py:262] Loading weights took 15.38 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m INFO 09-17 16:58:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m INFO 09-17 16:58:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 16.033787 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m INFO 09-17 16:58:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.984375 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m INFO 09-17 16:58:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 16.037776 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m INFO 09-17 16:58:52 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m INFO 09-17 16:58:52 [backends.py:541] Dynamo bytecode transform time: 15.80 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m INFO 09-17 16:58:52 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m INFO 09-17 16:58:52 [backends.py:541] Dynamo bytecode transform time: 15.95 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m INFO 09-17 16:58:52 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m INFO 09-17 16:58:52 [backends.py:541] Dynamo bytecode transform time: 16.28 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m INFO 09-17 16:58:52 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m INFO 09-17 16:58:52 [backends.py:541] Dynamo bytecode transform time: 16.14 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m INFO 09-17 16:59:01 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.168 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m INFO 09-17 16:59:01 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.202 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m INFO 09-17 16:59:02 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.335 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m INFO 09-17 16:59:02 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.319 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m INFO 09-17 16:59:07 [monitor.py:34] torch.compile takes 16.28 s in total\n",
      "INFO 09-17 16:59:07 [monitor.py:34] torch.compile takes 15.95 s in total\n",
      "INFO 09-17 16:59:07 [monitor.py:34] torch.compile takes 16.14 s in total\n",
      "INFO 09-17 16:59:07 [monitor.py:34] torch.compile takes 15.80 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m INFO 09-17 16:59:09 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m INFO 09-17 16:59:09 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m INFO 09-17 16:59:09 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m INFO 09-17 16:59:09 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 16:59:09 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 16:59:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 16:59:09 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:59:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:59:09 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:59:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 16:59:09 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 16:59:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m INFO 09-17 16:59:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m INFO 09-17 16:59:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m INFO 09-17 16:59:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m INFO 09-17 16:59:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 16:59:14 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.24 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.01/checkpoint-4950 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<46:48,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=466213)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=466211)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=466212)\u001b[0;0m INFO 09-17 16:59:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 16:59:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 16:59:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=466214)\u001b[0;0m INFO 09-17 16:59:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2659.19it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 155.02it/s, est. speed input: 8110.19 toks/s, output: 424.09 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.5/checkpoint-2500...\n",
      "INFO 09-17 16:59:51 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 16:59:51 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 16:59:52 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 16:59:52 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 16:59:52 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 16:59:52 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_6229a9b5'), local_subscribe_addr='ipc:///tmp/cede6d3f-7c9a-4c44-9f53-86d693683770', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m INFO 09-17 16:59:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d7ef38aa'), local_subscribe_addr='ipc:///tmp/e973fd72-b52f-4ea9-a083-fb893f8db59f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m INFO 09-17 16:59:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1487cc02'), local_subscribe_addr='ipc:///tmp/80c207cf-890a-454b-926c-a1c581fe743b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m INFO 09-17 16:59:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3907dcaf'), local_subscribe_addr='ipc:///tmp/b0d46398-28cc-43ba-8f94-f6b4d6b7b084', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m INFO 09-17 16:59:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_eadd182d'), local_subscribe_addr='ipc:///tmp/4e23fcff-fb8e-4c6e-a990-29abe526d354', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m INFO 09-17 16:59:56 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m INFO 09-17 16:59:56 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m INFO 09-17 16:59:56 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 16:59:56 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m INFO 09-17 16:59:56 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m INFO 09-17 16:59:56 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:59:56 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 16:59:56 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m WARNING 09-17 16:59:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 16:59:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m WARNING 09-17 16:59:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 16:59:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m INFO 09-17 16:59:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_9172b566'), local_subscribe_addr='ipc:///tmp/34859a0a-1dff-497f-b41e-4e0f20e44583', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m INFO 09-17 16:59:56 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 16:59:56 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 16:59:56 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 16:59:56 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m WARNING 09-17 16:59:56 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 16:59:56 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m WARNING 09-17 16:59:56 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 16:59:56 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m INFO 09-17 16:59:56 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m INFO 09-17 16:59:56 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m INFO 09-17 16:59:56 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 16:59:56 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m INFO 09-17 16:59:57 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m INFO 09-17 16:59:57 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m INFO 09-17 16:59:57 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m INFO 09-17 16:59:57 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m INFO 09-17 16:59:57 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m INFO 09-17 16:59:57 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m INFO 09-17 16:59:57 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m INFO 09-17 16:59:57 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m INFO 09-17 16:59:57 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m INFO 09-17 16:59:57 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m INFO 09-17 16:59:57 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m INFO 09-17 16:59:57 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.80s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.61s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.63s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:02,  1.45s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.28s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.19s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.35s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m INFO 09-17 17:00:05 [default_loader.py:262] Loading weights took 8.15 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m INFO 09-17 17:00:05 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m INFO 09-17 17:00:06 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 8.750788 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m INFO 09-17 17:00:06 [default_loader.py:262] Loading weights took 8.87 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m INFO 09-17 17:00:06 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m INFO 09-17 17:00:06 [default_loader.py:262] Loading weights took 9.15 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m INFO 09-17 17:00:06 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m INFO 09-17 17:00:06 [default_loader.py:262] Loading weights took 8.94 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m INFO 09-17 17:00:06 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m INFO 09-17 17:00:07 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.710161 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m INFO 09-17 17:00:07 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.821570 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m INFO 09-17 17:00:07 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.805354 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m INFO 09-17 17:00:23 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m INFO 09-17 17:00:23 [backends.py:541] Dynamo bytecode transform time: 15.57 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m INFO 09-17 17:00:24 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m INFO 09-17 17:00:24 [backends.py:541] Dynamo bytecode transform time: 15.91 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m INFO 09-17 17:00:24 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m INFO 09-17 17:00:24 [backends.py:541] Dynamo bytecode transform time: 15.95 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m INFO 09-17 17:00:24 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m INFO 09-17 17:00:24 [backends.py:541] Dynamo bytecode transform time: 16.31 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m INFO 09-17 17:00:36 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.796 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m INFO 09-17 17:00:36 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.838 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m INFO 09-17 17:00:36 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.923 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m INFO 09-17 17:00:36 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.649 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m INFO 09-17 17:00:42 [monitor.py:34] torch.compile takes 15.57 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m INFO 09-17 17:00:42 [monitor.py:34] torch.compile takes 15.91 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m INFO 09-17 17:00:42 [monitor.py:34] torch.compile takes 15.95 s in total\n",
      "INFO 09-17 17:00:42 [monitor.py:34] torch.compile takes 16.31 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m INFO 09-17 17:00:43 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m INFO 09-17 17:00:43 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m INFO 09-17 17:00:43 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m INFO 09-17 17:00:44 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:00:45 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:00:45 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:00:45 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:00:45 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:00:45 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:00:45 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:00:45 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:00:45 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m INFO 09-17 17:00:49 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m INFO 09-17 17:00:49 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m INFO 09-17 17:00:49 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m INFO 09-17 17:00:49 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:00:49 [core.py:193] init engine (profile, create kv cache, warmup model) took 41.56 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.5/checkpoint-2500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<47:17,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=468507)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=468506)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=468508)\u001b[0;0m INFO 09-17 17:00:50 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:00:50 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:00:50 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=468509)\u001b[0;0m INFO 09-17 17:00:50 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2718.47it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 161.69it/s, est. speed input: 8458.80 toks/s, output: 413.06 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.6/checkpoint-2000...\n",
      "INFO 09-17 17:01:25 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:01:25 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:01:25 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:01:25 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:01:25 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:01:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_04363f84'), local_subscribe_addr='ipc:///tmp/0251b689-89b6-4840-886e-0431662e50bc', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m INFO 09-17 17:01:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_dbae33c3'), local_subscribe_addr='ipc:///tmp/7894cf28-00e8-4b9c-8231-5adf09fe0333', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m INFO 09-17 17:01:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ef574bd6'), local_subscribe_addr='ipc:///tmp/b7552ed1-c81a-44b0-9f18-a5fe757e5583', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m INFO 09-17 17:01:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_31fe16a7'), local_subscribe_addr='ipc:///tmp/986127b6-a51c-4e4e-bff6-d102a055b88c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m INFO 09-17 17:01:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_133793bd'), local_subscribe_addr='ipc:///tmp/db750dc3-75ce-49b2-a3d2-373e778744f3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m INFO 09-17 17:01:28 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:01:28 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:01:28 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m INFO 09-17 17:01:28 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m INFO 09-17 17:01:28 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:01:28 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:01:28 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m INFO 09-17 17:01:28 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m WARNING 09-17 17:01:29 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:01:29 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m WARNING 09-17 17:01:29 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m WARNING 09-17 17:01:29 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m INFO 09-17 17:01:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_5343c5be'), local_subscribe_addr='ipc:///tmp/b78b14c3-bf0f-4ce6-8fd1-911467eec9d8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m INFO 09-17 17:01:29 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:01:29 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:01:29 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:01:29 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m WARNING 09-17 17:01:29 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:01:29 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m WARNING 09-17 17:01:29 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:01:29 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m INFO 09-17 17:01:29 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m INFO 09-17 17:01:29 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m INFO 09-17 17:01:29 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m INFO 09-17 17:01:29 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m INFO 09-17 17:01:29 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m INFO 09-17 17:01:29 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m INFO 09-17 17:01:29 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m INFO 09-17 17:01:29 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m INFO 09-17 17:01:30 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m INFO 09-17 17:01:30 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m INFO 09-17 17:01:30 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m INFO 09-17 17:01:30 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m INFO 09-17 17:01:30 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m INFO 09-17 17:01:30 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m INFO 09-17 17:01:30 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m INFO 09-17 17:01:30 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:12,  2.54s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:08,  2.19s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:06<00:05,  1.96s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:03,  1.73s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m INFO 09-17 17:01:39 [default_loader.py:262] Loading weights took 9.23 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m INFO 09-17 17:01:39 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m INFO 09-17 17:01:40 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.844064 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.61s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.75s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m INFO 09-17 17:01:41 [default_loader.py:262] Loading weights took 10.54 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m INFO 09-17 17:01:41 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m INFO 09-17 17:01:41 [default_loader.py:262] Loading weights took 10.57 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m INFO 09-17 17:01:41 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m INFO 09-17 17:01:41 [default_loader.py:262] Loading weights took 10.70 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m INFO 09-17 17:01:41 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m INFO 09-17 17:01:41 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.336016 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m INFO 09-17 17:01:41 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.181748 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m INFO 09-17 17:01:42 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.489893 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m INFO 09-17 17:01:58 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m INFO 09-17 17:01:58 [backends.py:541] Dynamo bytecode transform time: 15.68 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m INFO 09-17 17:01:58 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m INFO 09-17 17:01:58 [backends.py:541] Dynamo bytecode transform time: 15.86 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m INFO 09-17 17:01:58 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m INFO 09-17 17:01:58 [backends.py:541] Dynamo bytecode transform time: 15.87 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m INFO 09-17 17:01:59 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m INFO 09-17 17:01:59 [backends.py:541] Dynamo bytecode transform time: 16.44 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m INFO 09-17 17:02:08 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.257 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m INFO 09-17 17:02:08 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.299 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m INFO 09-17 17:02:08 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.286 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m INFO 09-17 17:02:09 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.384 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m INFO 09-17 17:02:14 [monitor.py:34] torch.compile takes 15.86 s in total\n",
      "INFO 09-17 17:02:14 [monitor.py:34] torch.compile takes 16.44 s in total\n",
      "INFO 09-17 17:02:14 [monitor.py:34] torch.compile takes 15.68 s in total\n",
      "INFO 09-17 17:02:14 [monitor.py:34] torch.compile takes 15.87 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m INFO 09-17 17:02:16 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m INFO 09-17 17:02:16 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m INFO 09-17 17:02:16 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m INFO 09-17 17:02:16 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "INFO 09-17 17:02:17 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:02:17 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:02:17 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:02:17 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:02:17 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:02:17 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:02:17 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:02:17 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m INFO 09-17 17:02:21 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m INFO 09-17 17:02:21 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:02:21 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m INFO 09-17 17:02:21 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:02:21 [core.py:193] init engine (profile, create kv cache, warmup model) took 39.11 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.6/checkpoint-2000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<49:37,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=470532)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=470531)\u001b[0;0m INFO 09-17 17:02:23 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:02:23 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=470530)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=470533)\u001b[0;0m INFO 09-17 17:02:23 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:02:23 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2675.13it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 158.11it/s, est. speed input: 8271.68 toks/s, output: 412.05 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.01/checkpoint-4950...\n",
      "INFO 09-17 17:02:57 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:02:57 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:02:58 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:02:58 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:02:58 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:02:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_c77904fb'), local_subscribe_addr='ipc:///tmp/f1f3e807-4f7a-4ade-95db-f87a2243076c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m INFO 09-17 17:03:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0a20fbf9'), local_subscribe_addr='ipc:///tmp/0a98dcac-d955-4030-a0e7-90766eb0909d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m INFO 09-17 17:03:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c81365dc'), local_subscribe_addr='ipc:///tmp/1131d12e-f70f-4e7d-a18e-172702efb499', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m INFO 09-17 17:03:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4a28a555'), local_subscribe_addr='ipc:///tmp/223b595d-05a2-4eaa-b659-1aeba34e4f78', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m INFO 09-17 17:03:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d7c9ce60'), local_subscribe_addr='ipc:///tmp/0f39aea8-7823-4578-a6ce-fcb91584cc84', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m INFO 09-17 17:03:01 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:03:01 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:03:01 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:03:01 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m INFO 09-17 17:03:01 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:03:01 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:03:01 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:03:01 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m WARNING 09-17 17:03:02 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m WARNING 09-17 17:03:02 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m WARNING 09-17 17:03:02 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:03:02 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m INFO 09-17 17:03:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_eb3adcd2'), local_subscribe_addr='ipc:///tmp/a7e4584e-7579-4cda-a9db-2e623765a01b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m INFO 09-17 17:03:02 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:03:02 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:03:02 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:03:02 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m WARNING 09-17 17:03:02 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:03:02 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:03:02 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m WARNING 09-17 17:03:02 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m INFO 09-17 17:03:02 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:03:02 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m INFO 09-17 17:03:02 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:03:02 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m INFO 09-17 17:03:02 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m INFO 09-17 17:03:02 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m INFO 09-17 17:03:02 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m INFO 09-17 17:03:02 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m INFO 09-17 17:03:02 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m INFO 09-17 17:03:02 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m INFO 09-17 17:03:02 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m INFO 09-17 17:03:02 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m INFO 09-17 17:03:02 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m INFO 09-17 17:03:02 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m INFO 09-17 17:03:02 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m INFO 09-17 17:03:02 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:11,  2.36s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:09,  2.45s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:07,  2.34s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:08<00:04,  2.09s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:09<00:01,  1.68s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.45s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.79s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m INFO 09-17 17:03:13 [default_loader.py:262] Loading weights took 10.82 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m INFO 09-17 17:03:13 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m INFO 09-17 17:03:14 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.361232 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m INFO 09-17 17:03:15 [default_loader.py:262] Loading weights took 11.77 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m INFO 09-17 17:03:15 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m INFO 09-17 17:03:15 [default_loader.py:262] Loading weights took 12.38 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m INFO 09-17 17:03:15 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m INFO 09-17 17:03:15 [default_loader.py:262] Loading weights took 12.61 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m INFO 09-17 17:03:15 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m INFO 09-17 17:03:15 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.542430 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m INFO 09-17 17:03:16 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.060549 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m INFO 09-17 17:03:16 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.168564 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m INFO 09-17 17:03:32 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m INFO 09-17 17:03:32 [backends.py:541] Dynamo bytecode transform time: 15.88 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m INFO 09-17 17:03:33 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m INFO 09-17 17:03:33 [backends.py:541] Dynamo bytecode transform time: 15.95 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m INFO 09-17 17:03:33 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m INFO 09-17 17:03:33 [backends.py:541] Dynamo bytecode transform time: 15.99 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m INFO 09-17 17:03:33 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m INFO 09-17 17:03:33 [backends.py:541] Dynamo bytecode transform time: 16.10 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m INFO 09-17 17:03:43 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.578 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m INFO 09-17 17:03:43 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.589 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m INFO 09-17 17:03:43 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.546 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m INFO 09-17 17:03:43 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.579 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m INFO 09-17 17:03:48 [monitor.py:34] torch.compile takes 15.99 s in total\n",
      "INFO 09-17 17:03:48 [monitor.py:34] torch.compile takes 16.10 s in total\n",
      "INFO 09-17 17:03:48 [monitor.py:34] torch.compile takes 15.88 s in total\n",
      "INFO 09-17 17:03:48 [monitor.py:34] torch.compile takes 15.95 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m INFO 09-17 17:03:50 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m INFO 09-17 17:03:50 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m INFO 09-17 17:03:50 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m INFO 09-17 17:03:50 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:03:50 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:03:50 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:03:50 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:03:50 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:03:50 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:03:50 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:03:50 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:03:50 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m INFO 09-17 17:03:54 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m INFO 09-17 17:03:54 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m INFO 09-17 17:03:54 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m INFO 09-17 17:03:54 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:03:54 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.31 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.01/checkpoint-4950 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<46:30,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=471484)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=471486)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=471483)\u001b[0;0m INFO 09-17 17:03:56 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:03:56 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:03:56 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=471485)\u001b[0;0m INFO 09-17 17:03:56 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2755.94it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:34<00:00, 143.63it/s, est. speed input: 7514.26 toks/s, output: 379.72 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.9/checkpoint-500...\n",
      "INFO 09-17 17:04:34 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:04:34 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:04:35 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:04:35 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:04:35 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:04:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_a43a0e5a'), local_subscribe_addr='ipc:///tmp/982b139f-95f0-45cd-90dd-dee44b434977', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m INFO 09-17 17:04:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f6c3f366'), local_subscribe_addr='ipc:///tmp/6fa1b4fc-8136-4ba2-9a70-147414057736', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m INFO 09-17 17:04:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_dbdfbf32'), local_subscribe_addr='ipc:///tmp/2e10467b-6d94-4872-a885-34b5e51ee985', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m INFO 09-17 17:04:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bc18f179'), local_subscribe_addr='ipc:///tmp/909abb85-abb2-4930-9d49-a88330f5e2d5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m INFO 09-17 17:04:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6172d9c6'), local_subscribe_addr='ipc:///tmp/94d3d394-b0bb-46c2-a069-b4106cd70ad4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m INFO 09-17 17:04:39 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:04:39 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m INFO 09-17 17:04:39 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m INFO 09-17 17:04:39 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m INFO 09-17 17:04:39 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:04:39 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m INFO 09-17 17:04:39 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:04:39 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m WARNING 09-17 17:04:39 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m WARNING 09-17 17:04:39 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m WARNING 09-17 17:04:39 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:04:39 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m INFO 09-17 17:04:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_f912ad77'), local_subscribe_addr='ipc:///tmp/c8cdc75b-b989-404f-9dfe-78164e3037d9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m INFO 09-17 17:04:39 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:04:39 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:04:39 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:04:39 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m WARNING 09-17 17:04:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m WARNING 09-17 17:04:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m WARNING 09-17 17:04:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:04:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m INFO 09-17 17:04:39 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m INFO 09-17 17:04:39 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:04:39 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m INFO 09-17 17:04:39 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m INFO 09-17 17:04:40 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m INFO 09-17 17:04:40 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m INFO 09-17 17:04:40 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m INFO 09-17 17:04:40 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m INFO 09-17 17:04:40 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m INFO 09-17 17:04:40 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m INFO 09-17 17:04:40 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m INFO 09-17 17:04:40 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m INFO 09-17 17:04:40 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m INFO 09-17 17:04:40 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m INFO 09-17 17:04:40 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m INFO 09-17 17:04:40 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:12,  2.54s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:05<00:10,  2.72s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:07,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m INFO 09-17 17:04:49 [default_loader.py:262] Loading weights took 9.08 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m INFO 09-17 17:04:49 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:09<00:04,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m INFO 09-17 17:04:50 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.695096 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:11<00:02,  2.08s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:13<00:00,  1.99s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:13<00:00,  2.19s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m INFO 09-17 17:04:53 [default_loader.py:262] Loading weights took 13.22 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m INFO 09-17 17:04:54 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m INFO 09-17 17:04:54 [default_loader.py:262] Loading weights took 13.35 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m INFO 09-17 17:04:54 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m INFO 09-17 17:04:54 [default_loader.py:262] Loading weights took 13.48 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m INFO 09-17 17:04:54 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m INFO 09-17 17:04:54 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.020059 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m INFO 09-17 17:04:54 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.262619 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m INFO 09-17 17:04:55 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.403363 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m INFO 09-17 17:05:11 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m INFO 09-17 17:05:11 [backends.py:541] Dynamo bytecode transform time: 15.59 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m INFO 09-17 17:05:11 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m INFO 09-17 17:05:11 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m INFO 09-17 17:05:11 [backends.py:541] Dynamo bytecode transform time: 15.97 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m INFO 09-17 17:05:11 [backends.py:541] Dynamo bytecode transform time: 16.00 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m INFO 09-17 17:05:12 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m INFO 09-17 17:05:12 [backends.py:541] Dynamo bytecode transform time: 16.04 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m INFO 09-17 17:05:21 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.292 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m INFO 09-17 17:05:21 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.335 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m INFO 09-17 17:05:21 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.349 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m INFO 09-17 17:05:21 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.562 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m INFO 09-17 17:05:27 [monitor.py:34] torch.compile takes 15.59 s in total\n",
      "INFO 09-17 17:05:27 [monitor.py:34] torch.compile takes 15.97 s in total\n",
      "INFO 09-17 17:05:27 [monitor.py:34] torch.compile takes 16.04 s in total\n",
      "INFO 09-17 17:05:27 [monitor.py:34] torch.compile takes 16.00 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m INFO 09-17 17:05:28 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m INFO 09-17 17:05:28 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m INFO 09-17 17:05:28 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m INFO 09-17 17:05:28 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:05:29 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:05:29 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:05:29 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:05:29 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:05:29 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:05:29 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:05:29 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:05:29 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m INFO 09-17 17:05:33 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:05:33 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m INFO 09-17 17:05:33 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m INFO 09-17 17:05:33 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:05:33 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.05 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.9/checkpoint-500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<59:52,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=472403)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=472405)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=472402)\u001b[0;0m INFO 09-17 17:05:35 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:05:35 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=472406)\u001b[0;0m INFO 09-17 17:05:35 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:05:35 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 2413.43it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 149.55it/s, est. speed input: 7824.04 toks/s, output: 428.81 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.9/checkpoint-500...\n",
      "INFO 09-17 17:06:11 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:06:11 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:06:12 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:06:12 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:06:12 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:06:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_14ab8ac6'), local_subscribe_addr='ipc:///tmp/8ad6bfea-1039-4185-853d-f7f48cbd0540', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m INFO 09-17 17:06:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1bf7cc58'), local_subscribe_addr='ipc:///tmp/a65e1ba8-4663-4b03-92a1-2f8b0756721b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m INFO 09-17 17:06:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e2c5530b'), local_subscribe_addr='ipc:///tmp/e84bf6c6-1af8-4dfb-8abe-5097f597d604', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-17 17:06:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2ae472a6'), local_subscribe_addr='ipc:///tmp/c06dd825-e008-4023-b2ef-3c20a1edd4f2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m INFO 09-17 17:06:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cd0ea344'), local_subscribe_addr='ipc:///tmp/0c886c12-cc92-4e04-925e-c59771918cda', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m INFO 09-17 17:06:16 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:06:16 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:06:16 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:06:16 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m INFO 09-17 17:06:16 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:06:16 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:06:16 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:06:16 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m WARNING 09-17 17:06:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m WARNING 09-17 17:06:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m WARNING 09-17 17:06:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:06:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m INFO 09-17 17:06:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_456e9e2a'), local_subscribe_addr='ipc:///tmp/307310bb-70e6-45d0-86eb-f6bee004151e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m INFO 09-17 17:06:16 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m INFO 09-17 17:06:16 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:06:16 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:06:16 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m WARNING 09-17 17:06:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:06:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m WARNING 09-17 17:06:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:06:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m INFO 09-17 17:06:16 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:06:16 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m INFO 09-17 17:06:16 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m INFO 09-17 17:06:16 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m INFO 09-17 17:06:17 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m INFO 09-17 17:06:17 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m INFO 09-17 17:06:17 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m INFO 09-17 17:06:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m INFO 09-17 17:06:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m INFO 09-17 17:06:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m INFO 09-17 17:06:17 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m INFO 09-17 17:06:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m INFO 09-17 17:06:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m INFO 09-17 17:06:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m INFO 09-17 17:06:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m INFO 09-17 17:06:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:12,  2.48s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:08,  2.23s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.88s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.53s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.31s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.19s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.48s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m INFO 09-17 17:06:26 [default_loader.py:262] Loading weights took 8.92 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m INFO 09-17 17:06:26 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m INFO 09-17 17:06:27 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.491662 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m INFO 09-17 17:06:28 [default_loader.py:262] Loading weights took 10.48 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m INFO 09-17 17:06:28 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m INFO 09-17 17:06:28 [default_loader.py:262] Loading weights took 10.83 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m INFO 09-17 17:06:28 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m INFO 09-17 17:06:28 [default_loader.py:262] Loading weights took 11.25 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m INFO 09-17 17:06:28 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m INFO 09-17 17:06:29 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.294163 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m INFO 09-17 17:06:29 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.635285 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m INFO 09-17 17:06:29 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.957736 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m INFO 09-17 17:06:45 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "INFO 09-17 17:06:45 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m INFO 09-17 17:06:45 [backends.py:541] Dynamo bytecode transform time: 15.80 s\n",
      "INFO 09-17 17:06:45 [backends.py:541] Dynamo bytecode transform time: 15.79 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m INFO 09-17 17:06:45 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m INFO 09-17 17:06:45 [backends.py:541] Dynamo bytecode transform time: 15.77 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m INFO 09-17 17:06:46 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m INFO 09-17 17:06:46 [backends.py:541] Dynamo bytecode transform time: 16.04 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m INFO 09-17 17:06:55 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.280 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m INFO 09-17 17:06:55 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.358 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m INFO 09-17 17:06:55 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.531 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m INFO 09-17 17:06:56 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.364 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m INFO 09-17 17:07:01 [monitor.py:34] torch.compile takes 15.80 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m INFO 09-17 17:07:01 [monitor.py:34] torch.compile takes 15.77 s in total\n",
      "INFO 09-17 17:07:01 [monitor.py:34] torch.compile takes 15.79 s in total\n",
      "INFO 09-17 17:07:01 [monitor.py:34] torch.compile takes 16.04 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m INFO 09-17 17:07:02 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m INFO 09-17 17:07:02 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m INFO 09-17 17:07:02 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m INFO 09-17 17:07:03 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:07:03 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:07:03 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:07:03 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:07:03 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:07:03 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:07:03 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:07:03 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:07:03 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m INFO 09-17 17:07:07 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m INFO 09-17 17:07:07 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m INFO 09-17 17:07:07 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m INFO 09-17 17:07:07 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:07:07 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.02 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.9/checkpoint-500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<46:31,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=473266)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=473263)\u001b[0;0m INFO 09-17 17:07:09 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=473264)\u001b[0;0m INFO 09-17 17:07:09 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=473265)\u001b[0;0m INFO 09-17 17:07:09 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:07:09 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 1760.73it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 158.46it/s, est. speed input: 8290.23 toks/s, output: 418.61 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.1/checkpoint-4500...\n",
      "INFO 09-17 17:07:45 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:07:45 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:07:46 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:07:46 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:07:46 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:07:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_63fef1e7'), local_subscribe_addr='ipc:///tmp/89a9f606-0a9f-4c9d-8079-5384991eb591', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m INFO 09-17 17:07:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_151b90da'), local_subscribe_addr='ipc:///tmp/a0096e00-07ad-4a6f-a129-0d8910c20605', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m INFO 09-17 17:07:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c9fcdf01'), local_subscribe_addr='ipc:///tmp/778554ab-80e9-4074-a3b6-8329cf90d962', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m INFO 09-17 17:07:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3fad5449'), local_subscribe_addr='ipc:///tmp/7167786c-0aae-4251-8589-9fae18a41455', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m INFO 09-17 17:07:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_19e8efd3'), local_subscribe_addr='ipc:///tmp/c58d8896-8dd0-4693-b784-9e58741c5746', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m INFO 09-17 17:07:49 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m INFO 09-17 17:07:49 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m INFO 09-17 17:07:49 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:07:49 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:07:49 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m INFO 09-17 17:07:49 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:07:49 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:07:49 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m WARNING 09-17 17:07:50 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:07:50 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m WARNING 09-17 17:07:50 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:07:50 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m INFO 09-17 17:07:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_a12b9cc9'), local_subscribe_addr='ipc:///tmp/885bd3bc-a9ab-428a-b794-3e18922c672d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m INFO 09-17 17:07:50 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:07:50 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:07:50 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:07:50 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m WARNING 09-17 17:07:50 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m WARNING 09-17 17:07:50 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m WARNING 09-17 17:07:50 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:07:50 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m INFO 09-17 17:07:50 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m INFO 09-17 17:07:50 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:07:50 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m INFO 09-17 17:07:50 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m INFO 09-17 17:07:50 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m INFO 09-17 17:07:50 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m INFO 09-17 17:07:50 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m INFO 09-17 17:07:50 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m INFO 09-17 17:07:50 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m INFO 09-17 17:07:50 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m INFO 09-17 17:07:50 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m INFO 09-17 17:07:50 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m INFO 09-17 17:07:51 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m INFO 09-17 17:07:51 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m INFO 09-17 17:07:51 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m INFO 09-17 17:07:51 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:11,  2.28s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:09,  2.33s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:07,  2.35s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:09<00:04,  2.37s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:11<00:02,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m INFO 09-17 17:08:05 [default_loader.py:262] Loading weights took 14.21 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m INFO 09-17 17:08:05 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:14<00:00,  2.35s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:14<00:00,  2.34s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m INFO 09-17 17:08:05 [default_loader.py:262] Loading weights took 14.09 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m INFO 09-17 17:08:05 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m INFO 09-17 17:08:05 [default_loader.py:262] Loading weights took 13.89 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m INFO 09-17 17:08:05 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m INFO 09-17 17:08:05 [default_loader.py:262] Loading weights took 14.09 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m INFO 09-17 17:08:05 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m INFO 09-17 17:08:06 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.906513 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m INFO 09-17 17:08:06 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.992431 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m INFO 09-17 17:08:06 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.003936 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m INFO 09-17 17:08:06 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.995835 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m INFO 09-17 17:08:22 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m INFO 09-17 17:08:22 [backends.py:541] Dynamo bytecode transform time: 15.78 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m INFO 09-17 17:08:23 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m INFO 09-17 17:08:23 [backends.py:541] Dynamo bytecode transform time: 15.97 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m INFO 09-17 17:08:23 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m INFO 09-17 17:08:23 [backends.py:541] Dynamo bytecode transform time: 15.96 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m INFO 09-17 17:08:23 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m INFO 09-17 17:08:23 [backends.py:541] Dynamo bytecode transform time: 16.37 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m INFO 09-17 17:08:32 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.332 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m INFO 09-17 17:08:32 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.460 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m INFO 09-17 17:08:32 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.446 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m INFO 09-17 17:08:33 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.402 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m INFO 09-17 17:08:38 [monitor.py:34] torch.compile takes 15.96 s in total\n",
      "INFO 09-17 17:08:38 [monitor.py:34] torch.compile takes 16.37 s in total\n",
      "INFO 09-17 17:08:38 [monitor.py:34] torch.compile takes 15.97 s in total\n",
      "INFO 09-17 17:08:38 [monitor.py:34] torch.compile takes 15.78 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m INFO 09-17 17:08:40 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m INFO 09-17 17:08:40 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m INFO 09-17 17:08:40 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m INFO 09-17 17:08:40 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "INFO 09-17 17:08:41 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:08:41 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:08:41 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:08:41 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:08:41 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:08:41 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:08:41 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:08:41 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m INFO 09-17 17:08:45 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m INFO 09-17 17:08:45 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m INFO 09-17 17:08:45 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m INFO 09-17 17:08:45 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:08:45 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.61 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.1/checkpoint-4500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<45:12,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=474126)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=474127)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=474124)\u001b[0;0m INFO 09-17 17:08:46 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:08:46 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474125)\u001b[0;0m INFO 09-17 17:08:46 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:08:46 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2542.47it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:34<00:00, 146.17it/s, est. speed input: 7646.91 toks/s, output: 401.45 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.6/checkpoint-2000...\n",
      "INFO 09-17 17:09:24 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:09:24 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:09:24 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:09:24 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:09:24 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:09:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_aea5e8e6'), local_subscribe_addr='ipc:///tmp/caa1797b-e3a1-4a40-97e7-84b669de5671', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m INFO 09-17 17:09:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e03c17cd'), local_subscribe_addr='ipc:///tmp/8fb7ba0b-6590-4740-a9de-866fc0f48330', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m INFO 09-17 17:09:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6d2ca7b4'), local_subscribe_addr='ipc:///tmp/a9842eb2-a414-41c8-a325-ac7261525aa3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m INFO 09-17 17:09:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_99cdfd84'), local_subscribe_addr='ipc:///tmp/232d7740-fca3-4cfd-b1ff-4fcff5254bf1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m INFO 09-17 17:09:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fb050156'), local_subscribe_addr='ipc:///tmp/d11a39a9-bc3a-425a-94b3-36a9ea6a8b11', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m INFO 09-17 17:09:27 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m INFO 09-17 17:09:27 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:09:27 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m INFO 09-17 17:09:27 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m INFO 09-17 17:09:27 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:09:27 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:09:27 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m INFO 09-17 17:09:27 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m WARNING 09-17 17:09:28 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:09:28 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m WARNING 09-17 17:09:28 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:09:28 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m INFO 09-17 17:09:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_521705f5'), local_subscribe_addr='ipc:///tmp/6cb126c9-a1a1-475e-8035-736f92dd2673', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m INFO 09-17 17:09:28 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:09:28 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:09:28 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:09:28 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m WARNING 09-17 17:09:28 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:09:28 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:09:28 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:09:28 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m INFO 09-17 17:09:28 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:09:28 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:09:28 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m INFO 09-17 17:09:28 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m INFO 09-17 17:09:28 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m INFO 09-17 17:09:28 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m INFO 09-17 17:09:28 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m INFO 09-17 17:09:28 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m INFO 09-17 17:09:28 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 09-17 17:09:28 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m INFO 09-17 17:09:28 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m INFO 09-17 17:09:28 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m INFO 09-17 17:09:29 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m INFO 09-17 17:09:29 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m INFO 09-17 17:09:29 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m INFO 09-17 17:09:29 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:09,  1.80s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.75s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.69s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.60s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.40s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.31s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.46s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m INFO 09-17 17:09:37 [default_loader.py:262] Loading weights took 8.85 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m INFO 09-17 17:09:38 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m INFO 09-17 17:09:38 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.449688 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m INFO 09-17 17:09:39 [default_loader.py:262] Loading weights took 9.88 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m INFO 09-17 17:09:39 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m INFO 09-17 17:09:39 [default_loader.py:262] Loading weights took 10.06 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m INFO 09-17 17:09:39 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m INFO 09-17 17:09:39 [default_loader.py:262] Loading weights took 9.91 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m INFO 09-17 17:09:39 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m INFO 09-17 17:09:40 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.626985 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m INFO 09-17 17:09:40 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.759188 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m INFO 09-17 17:09:40 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.916160 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m INFO 09-17 17:09:56 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m INFO 09-17 17:09:56 [backends.py:541] Dynamo bytecode transform time: 15.67 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m INFO 09-17 17:09:56 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m INFO 09-17 17:09:56 [backends.py:541] Dynamo bytecode transform time: 15.82 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m INFO 09-17 17:09:57 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m INFO 09-17 17:09:57 [backends.py:541] Dynamo bytecode transform time: 16.15 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m INFO 09-17 17:09:57 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m INFO 09-17 17:09:57 [backends.py:541] Dynamo bytecode transform time: 16.25 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m INFO 09-17 17:10:06 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.498 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m INFO 09-17 17:10:06 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.440 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m INFO 09-17 17:10:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.597 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m INFO 09-17 17:10:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.585 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m INFO 09-17 17:10:12 [monitor.py:34] torch.compile takes 15.67 s in total\n",
      "INFO 09-17 17:10:12 [monitor.py:34] torch.compile takes 16.15 s in total\n",
      "INFO 09-17 17:10:12 [monitor.py:34] torch.compile takes 16.25 s in total\n",
      "INFO 09-17 17:10:12 [monitor.py:34] torch.compile takes 15.82 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m INFO 09-17 17:10:13 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m INFO 09-17 17:10:13 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m INFO 09-17 17:10:14 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m INFO 09-17 17:10:14 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:10:14 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:10:14 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:10:14 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:10:14 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:10:14 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:10:14 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:10:14 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:10:14 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m INFO 09-17 17:10:18 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m INFO 09-17 17:10:18 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m INFO 09-17 17:10:18 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m INFO 09-17 17:10:18 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:10:18 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.38 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.6/checkpoint-2000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<42:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=474988)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=474990)\u001b[0;0m INFO 09-17 17:10:20 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=474991)\u001b[0;0m INFO 09-17 17:10:20 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=474992)\u001b[0;0m INFO 09-17 17:10:20 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:10:20 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2784.92it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 155.96it/s, est. speed input: 8159.26 toks/s, output: 426.94 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.5/checkpoint-2500...\n",
      "INFO 09-17 17:10:55 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:10:55 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:10:56 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:10:56 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:10:56 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:10:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_e4c765e2'), local_subscribe_addr='ipc:///tmp/9a6844a6-0c74-4bca-8c9e-dce59c0ee9e0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m INFO 09-17 17:10:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5c11ae4e'), local_subscribe_addr='ipc:///tmp/1253d4d4-f348-4649-9f63-48b1ea47c5f1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m INFO 09-17 17:10:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2392c16d'), local_subscribe_addr='ipc:///tmp/acbaea70-c798-4fe0-9101-4338d42f9e58', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m INFO 09-17 17:10:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f760e24d'), local_subscribe_addr='ipc:///tmp/64d02866-f2e9-4468-a3e5-0edb1e13df62', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m INFO 09-17 17:10:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a33e29e0'), local_subscribe_addr='ipc:///tmp/4f3bc8db-f54c-4d08-9bf8-aabba97db75f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m INFO 09-17 17:10:59 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:10:59 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m INFO 09-17 17:10:59 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:10:59 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m INFO 09-17 17:10:59 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m INFO 09-17 17:10:59 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:10:59 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m INFO 09-17 17:10:59 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m WARNING 09-17 17:11:00 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:11:00 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m WARNING 09-17 17:11:00 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:11:00 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m INFO 09-17 17:11:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_7cb5d5cc'), local_subscribe_addr='ipc:///tmp/f0025520-d2f0-45a3-8a47-1e1bf2c22b34', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m INFO 09-17 17:11:00 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:11:00 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m INFO 09-17 17:11:00 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m INFO 09-17 17:11:00 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m WARNING 09-17 17:11:00 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m WARNING 09-17 17:11:00 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:11:00 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m INFO 09-17 17:11:00 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m INFO 09-17 17:11:00 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m WARNING 09-17 17:11:00 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 17:11:00 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m INFO 09-17 17:11:00 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m INFO 09-17 17:11:00 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m INFO 09-17 17:11:00 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m INFO 09-17 17:11:00 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m INFO 09-17 17:11:00 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m INFO 09-17 17:11:00 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m INFO 09-17 17:11:01 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 09-17 17:11:01 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m INFO 09-17 17:11:01 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m INFO 09-17 17:11:01 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m INFO 09-17 17:11:01 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m INFO 09-17 17:11:01 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m INFO 09-17 17:11:01 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:11,  2.34s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:08,  2.19s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:06<00:05,  1.93s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:03,  1.58s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.35s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.22s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.50s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m INFO 09-17 17:11:10 [default_loader.py:262] Loading weights took 9.05 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m INFO 09-17 17:11:10 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m INFO 09-17 17:11:11 [default_loader.py:262] Loading weights took 9.68 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m INFO 09-17 17:11:11 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m INFO 09-17 17:11:11 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.649579 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m INFO 09-17 17:11:11 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.309079 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m INFO 09-17 17:11:12 [default_loader.py:262] Loading weights took 10.62 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m INFO 09-17 17:11:12 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m INFO 09-17 17:11:12 [default_loader.py:262] Loading weights took 10.63 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m INFO 09-17 17:11:12 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m INFO 09-17 17:11:13 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.352005 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m INFO 09-17 17:11:13 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.543325 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m INFO 09-17 17:11:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m INFO 09-17 17:11:29 [backends.py:541] Dynamo bytecode transform time: 15.58 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m INFO 09-17 17:11:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m INFO 09-17 17:11:29 [backends.py:541] Dynamo bytecode transform time: 15.82 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m INFO 09-17 17:11:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m INFO 09-17 17:11:29 [backends.py:541] Dynamo bytecode transform time: 15.93 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m INFO 09-17 17:11:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m INFO 09-17 17:11:29 [backends.py:541] Dynamo bytecode transform time: 16.16 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m INFO 09-17 17:11:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.509 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m INFO 09-17 17:11:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.572 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m INFO 09-17 17:11:40 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.970 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m INFO 09-17 17:11:40 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.010 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m INFO 09-17 17:11:45 [monitor.py:34] torch.compile takes 15.58 s in total\n",
      "INFO 09-17 17:11:45 [monitor.py:34] torch.compile takes 15.93 s in total\n",
      "INFO 09-17 17:11:45 [monitor.py:34] torch.compile takes 16.16 s in total\n",
      "INFO 09-17 17:11:45 [monitor.py:34] torch.compile takes 15.82 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m INFO 09-17 17:11:47 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m INFO 09-17 17:11:47 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m INFO 09-17 17:11:47 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m INFO 09-17 17:11:47 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:11:48 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:11:48 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:11:48 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:11:48 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:11:48 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:11:48 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:11:48 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:11:48 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m INFO 09-17 17:11:52 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m INFO 09-17 17:11:52 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m INFO 09-17 17:11:52 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m INFO 09-17 17:11:52 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:11:52 [core.py:193] init engine (profile, create kv cache, warmup model) took 39.47 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.5/checkpoint-2500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<46:56,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=475843)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=475844)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=475841)\u001b[0;0m INFO 09-17 17:11:54 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:11:54 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:11:54 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=475842)\u001b[0;0m INFO 09-17 17:11:54 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2687.70it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 156.05it/s, est. speed input: 8164.15 toks/s, output: 423.89 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.001/checkpoint-4995...\n",
      "INFO 09-17 17:12:29 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:12:29 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:12:30 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:12:30 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:12:30 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:12:30 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_62bea45c'), local_subscribe_addr='ipc:///tmp/5a1e4933-faae-4c21-acee-0cc00986fd43', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m INFO 09-17 17:12:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ffd01c61'), local_subscribe_addr='ipc:///tmp/f6d4ea8b-57e9-48c3-b57c-e05df99feb06', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m INFO 09-17 17:12:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c397414a'), local_subscribe_addr='ipc:///tmp/4c73fe6b-d098-4de1-8411-6e76b437e448', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m INFO 09-17 17:12:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5bcb75f1'), local_subscribe_addr='ipc:///tmp/352e58d7-0857-44d2-8b03-ed314a310a92', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m INFO 09-17 17:12:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_eedc5198'), local_subscribe_addr='ipc:///tmp/b8cfe1b2-4f0c-49c0-94ff-75915a9545fa', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m INFO 09-17 17:12:33 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:12:33 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:12:33 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m INFO 09-17 17:12:33 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:12:33 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:12:33 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m INFO 09-17 17:12:33 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m INFO 09-17 17:12:33 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m WARNING 09-17 17:12:34 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m WARNING 09-17 17:12:34 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m WARNING 09-17 17:12:34 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:12:34 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m INFO 09-17 17:12:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_2bbc570d'), local_subscribe_addr='ipc:///tmp/595213af-ac09-4ea2-b0c6-f694b197fe04', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m INFO 09-17 17:12:34 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:12:34 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:12:34 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:12:34 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m WARNING 09-17 17:12:34 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m WARNING 09-17 17:12:34 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:12:34 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:12:34 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m INFO 09-17 17:12:34 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m INFO 09-17 17:12:34 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:12:34 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m INFO 09-17 17:12:34 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m INFO 09-17 17:12:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m INFO 09-17 17:12:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m INFO 09-17 17:12:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m INFO 09-17 17:12:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m INFO 09-17 17:12:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m INFO 09-17 17:12:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m INFO 09-17 17:12:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m INFO 09-17 17:12:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m INFO 09-17 17:12:35 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m INFO 09-17 17:12:35 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m INFO 09-17 17:12:35 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m INFO 09-17 17:12:35 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:13,  2.66s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:05<00:10,  2.61s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:07,  2.45s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:09<00:04,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m INFO 09-17 17:12:44 [default_loader.py:262] Loading weights took 9.80 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m INFO 09-17 17:12:45 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m INFO 09-17 17:12:45 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.440124 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:10<00:01,  1.95s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:12<00:00,  1.85s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:12<00:00,  2.08s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m INFO 09-17 17:12:48 [default_loader.py:262] Loading weights took 12.54 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m INFO 09-17 17:12:48 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m INFO 09-17 17:12:48 [default_loader.py:262] Loading weights took 12.78 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m INFO 09-17 17:12:48 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m INFO 09-17 17:12:48 [default_loader.py:262] Loading weights took 12.98 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m INFO 09-17 17:12:48 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m INFO 09-17 17:12:48 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.475107 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m INFO 09-17 17:12:48 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.522046 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m INFO 09-17 17:12:49 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.808550 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m INFO 09-17 17:13:05 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m INFO 09-17 17:13:05 [backends.py:541] Dynamo bytecode transform time: 15.93 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m INFO 09-17 17:13:05 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m INFO 09-17 17:13:05 [backends.py:541] Dynamo bytecode transform time: 15.92 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m INFO 09-17 17:13:05 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m INFO 09-17 17:13:05 [backends.py:541] Dynamo bytecode transform time: 15.98 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m INFO 09-17 17:13:05 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m INFO 09-17 17:13:05 [backends.py:541] Dynamo bytecode transform time: 16.12 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m INFO 09-17 17:13:15 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.699 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m INFO 09-17 17:13:15 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.779 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m INFO 09-17 17:13:15 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.776 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m INFO 09-17 17:13:15 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.712 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m INFO 09-17 17:13:21 [monitor.py:34] torch.compile takes 15.98 s in total\n",
      "INFO 09-17 17:13:21 [monitor.py:34] torch.compile takes 15.92 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m INFO 09-17 17:13:21 [monitor.py:34] torch.compile takes 15.93 s in total\n",
      "INFO 09-17 17:13:21 [monitor.py:34] torch.compile takes 16.12 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m INFO 09-17 17:13:22 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m INFO 09-17 17:13:22 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m INFO 09-17 17:13:22 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m INFO 09-17 17:13:22 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:13:23 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:13:23 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:13:23 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:13:23 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:13:23 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:13:23 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:13:23 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:13:23 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m INFO 09-17 17:13:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m INFO 09-17 17:13:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m INFO 09-17 17:13:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m INFO 09-17 17:13:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:13:27 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.25 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/bottom_indices_misaligned_0.001/checkpoint-4995 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<45:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=476721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=476718)\u001b[0;0m INFO 09-17 17:13:28 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:13:28 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=476720)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=476719)\u001b[0;0m INFO 09-17 17:13:28 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:13:28 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2702.98it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 155.72it/s, est. speed input: 8146.57 toks/s, output: 411.57 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W917 17:14:02.959559885 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=211, addr=[::ffff:127.0.0.1]:54700, remote=[::ffff:127.0.0.1]:57991): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7225261785e8 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8bfe (0x7224fe97fbfe in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baaf40 (0x7224fe981f40 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab84a (0x7224fe98284a in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7224fe97c2a9 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7224c007b9f9 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdc253 (0x722538df9253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7225396feac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #8: clone + 0x44 (0x72253978fbf4 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[W917 17:14:02.971547564 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 3] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.4/checkpoint-3000...\n",
      "INFO 09-17 17:14:04 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:14:04 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:14:04 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:14:04 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:14:04 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:14:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_4b0f9334'), local_subscribe_addr='ipc:///tmp/0991ef63-b3b4-4d03-9304-299fc8b94ec4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m INFO 09-17 17:14:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_429906a4'), local_subscribe_addr='ipc:///tmp/228f4fad-00f9-4871-838c-06d5d8bc6f7e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m INFO 09-17 17:14:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a4a7381b'), local_subscribe_addr='ipc:///tmp/97db59a4-1194-4a52-b5fc-42683243269c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m INFO 09-17 17:14:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_69b7d184'), local_subscribe_addr='ipc:///tmp/715b617b-384c-406c-a497-eb67fe017cc4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m INFO 09-17 17:14:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_599c1431'), local_subscribe_addr='ipc:///tmp/b71c2750-ceec-4238-b564-82c57a9bf5e0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m INFO 09-17 17:14:08 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m INFO 09-17 17:14:08 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:14:08 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m INFO 09-17 17:14:08 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:14:08 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:14:08 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m INFO 09-17 17:14:08 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:14:08 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m WARNING 09-17 17:14:08 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:14:08 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m WARNING 09-17 17:14:08 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:14:08 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m INFO 09-17 17:14:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_c557c9cd'), local_subscribe_addr='ipc:///tmp/4dd4ec03-cc09-463e-af05-d60e1ec2188a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m INFO 09-17 17:14:08 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m INFO 09-17 17:14:08 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:14:08 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:14:08 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m WARNING 09-17 17:14:08 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m WARNING 09-17 17:14:08 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m WARNING 09-17 17:14:08 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m WARNING 09-17 17:14:08 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 17:14:08 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m INFO 09-17 17:14:08 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m INFO 09-17 17:14:08 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m INFO 09-17 17:14:08 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m INFO 09-17 17:14:09 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m INFO 09-17 17:14:09 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m INFO 09-17 17:14:09 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m INFO 09-17 17:14:09 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m INFO 09-17 17:14:09 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m INFO 09-17 17:14:09 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m INFO 09-17 17:14:09 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m INFO 09-17 17:14:09 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m INFO 09-17 17:14:09 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m INFO 09-17 17:14:09 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m INFO 09-17 17:14:09 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m INFO 09-17 17:14:09 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.74s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:07,  1.88s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.60s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:02,  1.43s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.29s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.21s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.38s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m INFO 09-17 17:14:17 [default_loader.py:262] Loading weights took 8.32 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m INFO 09-17 17:14:17 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m INFO 09-17 17:14:18 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 8.919578 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m INFO 09-17 17:14:20 [default_loader.py:262] Loading weights took 10.73 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m INFO 09-17 17:14:20 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m INFO 09-17 17:14:20 [default_loader.py:262] Loading weights took 10.96 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m INFO 09-17 17:14:20 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m INFO 09-17 17:14:21 [default_loader.py:262] Loading weights took 11.19 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m INFO 09-17 17:14:21 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m INFO 09-17 17:14:21 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.385795 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m INFO 09-17 17:14:21 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.742219 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m INFO 09-17 17:14:21 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.924084 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m INFO 09-17 17:14:38 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m INFO 09-17 17:14:38 [backends.py:541] Dynamo bytecode transform time: 15.87 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m INFO 09-17 17:14:38 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m INFO 09-17 17:14:38 [backends.py:541] Dynamo bytecode transform time: 15.95 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m INFO 09-17 17:14:38 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m INFO 09-17 17:14:38 [backends.py:541] Dynamo bytecode transform time: 15.98 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m INFO 09-17 17:14:38 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m INFO 09-17 17:14:38 [backends.py:541] Dynamo bytecode transform time: 16.11 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m INFO 09-17 17:14:48 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.381 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m INFO 09-17 17:14:48 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.422 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m INFO 09-17 17:14:48 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.411 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m INFO 09-17 17:14:48 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.438 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m INFO 09-17 17:14:53 [monitor.py:34] torch.compile takes 16.11 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m INFO 09-17 17:14:53 [monitor.py:34] torch.compile takes 15.98 s in total\n",
      "INFO 09-17 17:14:53 [monitor.py:34] torch.compile takes 15.95 s in total\n",
      "INFO 09-17 17:14:53 [monitor.py:34] torch.compile takes 15.87 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m INFO 09-17 17:14:55 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m INFO 09-17 17:14:55 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m INFO 09-17 17:14:55 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m INFO 09-17 17:14:55 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:14:56 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:14:56 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:14:56 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:14:56 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:14:56 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:14:56 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:14:56 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:14:56 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m INFO 09-17 17:15:00 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:15:00 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m INFO 09-17 17:15:00 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m INFO 09-17 17:15:00 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:15:00 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.34 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.4/checkpoint-3000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<40:22,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=477567)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=477568)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=477570)\u001b[0;0m INFO 09-17 17:15:02 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:15:02 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:15:02 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=477569)\u001b[0;0m INFO 09-17 17:15:02 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 2013.82it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 156.10it/s, est. speed input: 8166.54 toks/s, output: 434.00 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.999/checkpoint-5...\n",
      "INFO 09-17 17:15:38 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:15:38 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:15:38 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:15:38 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:15:38 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:15:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_8dcea690'), local_subscribe_addr='ipc:///tmp/24db67bf-afbb-48d0-abcb-941f7ee785f9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m INFO 09-17 17:15:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f11767af'), local_subscribe_addr='ipc:///tmp/cbc4e249-911e-4dc4-893d-21b783e87cc5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m INFO 09-17 17:15:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_884d5a7f'), local_subscribe_addr='ipc:///tmp/edc04ffc-8a03-4fda-9626-3398b1b2d4ef', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m INFO 09-17 17:15:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bacdeaf8'), local_subscribe_addr='ipc:///tmp/52e8ccc0-07d6-4e4e-b0fb-7e147e5c502a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m INFO 09-17 17:15:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e1271da7'), local_subscribe_addr='ipc:///tmp/a6340188-618c-4f38-a3f8-493d13bdae8c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m INFO 09-17 17:15:42 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:15:42 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m INFO 09-17 17:15:42 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m INFO 09-17 17:15:42 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m INFO 09-17 17:15:42 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:15:42 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m INFO 09-17 17:15:42 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m INFO 09-17 17:15:42 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m WARNING 09-17 17:15:42 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:15:42 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m WARNING 09-17 17:15:42 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:15:42 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m INFO 09-17 17:15:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_ac39105f'), local_subscribe_addr='ipc:///tmp/3a134d0e-a404-4350-be6c-428cdd5f7eb9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m INFO 09-17 17:15:42 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m INFO 09-17 17:15:42 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:15:42 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:15:42 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m WARNING 09-17 17:15:42 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:15:42 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m WARNING 09-17 17:15:42 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:15:42 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m INFO 09-17 17:15:42 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m INFO 09-17 17:15:42 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:15:42 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m INFO 09-17 17:15:42 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m INFO 09-17 17:15:42 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m INFO 09-17 17:15:42 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m INFO 09-17 17:15:42 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m INFO 09-17 17:15:42 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m INFO 09-17 17:15:42 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m INFO 09-17 17:15:43 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m INFO 09-17 17:15:43 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m INFO 09-17 17:15:43 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m INFO 09-17 17:15:43 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m INFO 09-17 17:15:43 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m INFO 09-17 17:15:43 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m INFO 09-17 17:15:43 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:07,  1.56s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.58s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.60s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.59s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.52s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.52s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.54s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m INFO 09-17 17:15:53 [default_loader.py:262] Loading weights took 9.33 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m INFO 09-17 17:15:53 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m INFO 09-17 17:15:53 [default_loader.py:262] Loading weights took 9.64 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m INFO 09-17 17:15:53 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m INFO 09-17 17:15:53 [default_loader.py:262] Loading weights took 9.90 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m INFO 09-17 17:15:53 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m INFO 09-17 17:15:53 [default_loader.py:262] Loading weights took 9.89 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m INFO 09-17 17:15:53 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m INFO 09-17 17:15:53 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.192316 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m INFO 09-17 17:15:53 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.509426 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m INFO 09-17 17:15:54 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.261183 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m INFO 09-17 17:15:54 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.603490 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m INFO 09-17 17:16:09 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m INFO 09-17 17:16:10 [backends.py:541] Dynamo bytecode transform time: 15.40 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m INFO 09-17 17:16:10 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m INFO 09-17 17:16:10 [backends.py:541] Dynamo bytecode transform time: 15.49 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m INFO 09-17 17:16:10 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m INFO 09-17 17:16:10 [backends.py:541] Dynamo bytecode transform time: 15.57 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m INFO 09-17 17:16:10 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m INFO 09-17 17:16:10 [backends.py:541] Dynamo bytecode transform time: 15.86 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m INFO 09-17 17:16:19 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.344 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m INFO 09-17 17:16:19 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.323 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m INFO 09-17 17:16:20 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.354 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m INFO 09-17 17:16:20 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.513 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m INFO 09-17 17:16:25 [monitor.py:34] torch.compile takes 15.49 s in total\n",
      "INFO 09-17 17:16:25 [monitor.py:34] torch.compile takes 15.86 s in total\n",
      "INFO 09-17 17:16:25 [monitor.py:34] torch.compile takes 15.40 s in total\n",
      "INFO 09-17 17:16:25 [monitor.py:34] torch.compile takes 15.57 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m INFO 09-17 17:16:27 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m INFO 09-17 17:16:27 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m INFO 09-17 17:16:27 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m INFO 09-17 17:16:27 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:16:28 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:16:28 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:16:28 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:16:28 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:16:28 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:16:28 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:16:28 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:16:28 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m INFO 09-17 17:16:32 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m INFO 09-17 17:16:32 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m INFO 09-17 17:16:32 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m INFO 09-17 17:16:32 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:16:32 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.76 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.999/checkpoint-5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<38:44,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=478425)\u001b[0;0m INFO 09-17 17:16:34 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=478427)\u001b[0;0m INFO 09-17 17:16:34 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=478428)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=478426)\u001b[0;0m INFO 09-17 17:16:34 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:16:34 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2829.79it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 149.22it/s, est. speed input: 7806.55 toks/s, output: 439.73 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.99/checkpoint-50...\n",
      "INFO 09-17 17:17:11 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:17:11 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:17:11 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:17:11 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:17:11 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:17:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_b0ff38d4'), local_subscribe_addr='ipc:///tmp/b57b3f22-e646-4001-8a94-ec40a42b918e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m INFO 09-17 17:17:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_db4c7b1b'), local_subscribe_addr='ipc:///tmp/e9a872ff-587c-49ff-8faf-710080e8ee02', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m INFO 09-17 17:17:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e8b34698'), local_subscribe_addr='ipc:///tmp/aeaa1c16-06af-4e21-a906-8c8d99589dda', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m INFO 09-17 17:17:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_494ac36b'), local_subscribe_addr='ipc:///tmp/9e80df14-f73f-4fa5-a410-309bddbc840d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m INFO 09-17 17:17:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_929b0659'), local_subscribe_addr='ipc:///tmp/75b4b41c-6b1c-4a59-a38c-d229e03a34cf', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m INFO 09-17 17:17:14 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:17:14 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:17:14 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:17:14 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m INFO 09-17 17:17:14 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:17:14 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:17:14 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:17:14 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m WARNING 09-17 17:17:14 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:17:14 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m WARNING 09-17 17:17:14 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m WARNING 09-17 17:17:14 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m INFO 09-17 17:17:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_9829e157'), local_subscribe_addr='ipc:///tmp/b7273971-133c-4d02-8f95-908e1c63ad31', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m INFO 09-17 17:17:15 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m INFO 09-17 17:17:15 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:17:15 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:17:15 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m WARNING 09-17 17:17:15 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:17:15 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m WARNING 09-17 17:17:15 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m WARNING 09-17 17:17:15 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m INFO 09-17 17:17:15 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:17:15 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m INFO 09-17 17:17:15 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m INFO 09-17 17:17:15 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m INFO 09-17 17:17:15 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m INFO 09-17 17:17:15 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m INFO 09-17 17:17:15 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m INFO 09-17 17:17:15 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m INFO 09-17 17:17:15 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m INFO 09-17 17:17:15 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m INFO 09-17 17:17:15 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m INFO 09-17 17:17:15 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m INFO 09-17 17:17:15 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m INFO 09-17 17:17:15 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m INFO 09-17 17:17:15 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m INFO 09-17 17:17:15 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:09,  1.82s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:07,  1.82s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.78s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:03,  1.73s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m INFO 09-17 17:17:25 [default_loader.py:262] Loading weights took 9.23 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m INFO 09-17 17:17:25 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.54s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.64s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m INFO 09-17 17:17:25 [default_loader.py:262] Loading weights took 9.65 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m INFO 09-17 17:17:25 [default_loader.py:262] Loading weights took 9.89 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m INFO 09-17 17:17:25 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m INFO 09-17 17:17:25 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.795386 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m INFO 09-17 17:17:25 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m INFO 09-17 17:17:25 [default_loader.py:262] Loading weights took 9.83 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m INFO 09-17 17:17:25 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m INFO 09-17 17:17:26 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.564156 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m INFO 09-17 17:17:26 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.494176 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m INFO 09-17 17:17:26 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.453138 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m INFO 09-17 17:17:42 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m INFO 09-17 17:17:42 [backends.py:541] Dynamo bytecode transform time: 15.43 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m INFO 09-17 17:17:43 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m INFO 09-17 17:17:43 [backends.py:541] Dynamo bytecode transform time: 16.02 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m INFO 09-17 17:17:43 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m INFO 09-17 17:17:43 [backends.py:541] Dynamo bytecode transform time: 16.12 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m INFO 09-17 17:17:43 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m INFO 09-17 17:17:43 [backends.py:541] Dynamo bytecode transform time: 16.08 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m INFO 09-17 17:17:52 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.314 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m INFO 09-17 17:17:53 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.416 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m INFO 09-17 17:17:53 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.333 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m INFO 09-17 17:17:53 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.295 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m INFO 09-17 17:17:58 [monitor.py:34] torch.compile takes 16.08 s in total\n",
      "INFO 09-17 17:17:58 [monitor.py:34] torch.compile takes 16.12 s in total\n",
      "INFO 09-17 17:17:58 [monitor.py:34] torch.compile takes 16.02 s in total\n",
      "INFO 09-17 17:17:58 [monitor.py:34] torch.compile takes 15.43 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m INFO 09-17 17:17:59 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m INFO 09-17 17:18:00 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m INFO 09-17 17:18:00 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m INFO 09-17 17:18:00 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:18:00 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:18:00 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:18:00 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:18:00 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:18:00 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:18:00 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:18:00 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:18:00 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m INFO 09-17 17:18:04 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m INFO 09-17 17:18:04 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m INFO 09-17 17:18:04 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m INFO 09-17 17:18:04 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:18:04 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.98 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_44/top_indices_misaligned_0.99/checkpoint-50 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<39:50,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=479309)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=479311)\u001b[0;0m INFO 09-17 17:18:06 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=479310)\u001b[0;0m INFO 09-17 17:18:06 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=479312)\u001b[0;0m INFO 09-17 17:18:06 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:18:06 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2887.87it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 150.65it/s, est. speed input: 7881.41 toks/s, output: 422.67 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_1_0.1/checkpoint-4500...\n",
      "INFO 09-17 17:18:42 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:18:42 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:18:43 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:18:43 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:18:43 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:18:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_60756028'), local_subscribe_addr='ipc:///tmp/746bea5e-e5d9-4d76-9ae4-15221b0f638f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m INFO 09-17 17:18:45 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2b629890'), local_subscribe_addr='ipc:///tmp/271b734c-5f89-46db-a83e-24b82ca34143', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m INFO 09-17 17:18:45 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_eba65e80'), local_subscribe_addr='ipc:///tmp/bd3ee143-2bc2-4ea0-9e27-d86acfc20ea2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m INFO 09-17 17:18:45 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4e673b22'), local_subscribe_addr='ipc:///tmp/b80deb83-03ed-40c8-9a50-86b70a1436d1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m INFO 09-17 17:18:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bd3e7d49'), local_subscribe_addr='ipc:///tmp/305351ec-24a5-4e38-9789-22ffb3e98596', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m INFO 09-17 17:18:46 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:18:46 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:18:46 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m INFO 09-17 17:18:46 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:18:46 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m INFO 09-17 17:18:46 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:18:46 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m INFO 09-17 17:18:46 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m WARNING 09-17 17:18:47 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:18:47 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m WARNING 09-17 17:18:47 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m WARNING 09-17 17:18:47 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m INFO 09-17 17:18:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_01a017ea'), local_subscribe_addr='ipc:///tmp/c26d47b9-a58f-44dd-822d-2704876260f5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m INFO 09-17 17:18:47 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:18:47 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m INFO 09-17 17:18:47 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:18:47 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m WARNING 09-17 17:18:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:18:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m WARNING 09-17 17:18:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m INFO 09-17 17:18:47 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "WARNING 09-17 17:18:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m INFO 09-17 17:18:47 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m INFO 09-17 17:18:47 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m INFO 09-17 17:18:47 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m INFO 09-17 17:18:47 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m INFO 09-17 17:18:47 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m INFO 09-17 17:18:47 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m INFO 09-17 17:18:47 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m INFO 09-17 17:18:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 09-17 17:18:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m INFO 09-17 17:18:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m INFO 09-17 17:18:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m INFO 09-17 17:18:48 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m INFO 09-17 17:18:48 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m INFO 09-17 17:18:48 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m INFO 09-17 17:18:48 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.64s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.64s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.62s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.60s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m INFO 09-17 17:18:57 [default_loader.py:262] Loading weights took 9.04 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m INFO 09-17 17:18:57 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m INFO 09-17 17:18:57 [default_loader.py:262] Loading weights took 9.29 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m INFO 09-17 17:18:57 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.50s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.54s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m INFO 09-17 17:18:57 [default_loader.py:262] Loading weights took 9.33 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m INFO 09-17 17:18:57 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m INFO 09-17 17:18:57 [default_loader.py:262] Loading weights took 9.33 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m INFO 09-17 17:18:58 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m INFO 09-17 17:18:58 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.666416 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m INFO 09-17 17:18:58 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.939276 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m INFO 09-17 17:18:58 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.020220 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m INFO 09-17 17:18:58 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.048140 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m INFO 09-17 17:19:14 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m INFO 09-17 17:19:14 [backends.py:541] Dynamo bytecode transform time: 15.65 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m INFO 09-17 17:19:15 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m INFO 09-17 17:19:15 [backends.py:541] Dynamo bytecode transform time: 15.77 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m INFO 09-17 17:19:15 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m INFO 09-17 17:19:15 [backends.py:541] Dynamo bytecode transform time: 15.76 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m INFO 09-17 17:19:15 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m INFO 09-17 17:19:15 [backends.py:541] Dynamo bytecode transform time: 15.91 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m INFO 09-17 17:19:24 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.555 s\n",
      "INFO 09-17 17:19:24 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.424 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m INFO 09-17 17:19:25 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.570 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m INFO 09-17 17:19:25 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.614 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m INFO 09-17 17:19:30 [monitor.py:34] torch.compile takes 15.65 s in total\n",
      "INFO 09-17 17:19:30 [monitor.py:34] torch.compile takes 15.77 s in total\n",
      "INFO 09-17 17:19:30 [monitor.py:34] torch.compile takes 15.76 s in total\n",
      "INFO 09-17 17:19:30 [monitor.py:34] torch.compile takes 15.91 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m INFO 09-17 17:19:32 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m INFO 09-17 17:19:32 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m INFO 09-17 17:19:32 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m INFO 09-17 17:19:32 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:19:32 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:19:32 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:19:32 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:19:32 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:19:32 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:19:32 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:19:32 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:19:32 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m INFO 09-17 17:19:36 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m INFO 09-17 17:19:36 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m INFO 09-17 17:19:36 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m INFO 09-17 17:19:36 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:19:36 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.95 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_1_0.1/checkpoint-4500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<43:24,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=480157)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=480159)\u001b[0;0m INFO 09-17 17:19:38 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:19:38 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=480158)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=480160)\u001b[0;0m INFO 09-17 17:19:38 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:19:38 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2692.41it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 160.42it/s, est. speed input: 8392.68 toks/s, output: 417.24 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_2_0.001/checkpoint-4995...\n",
      "INFO 09-17 17:20:12 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:20:12 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:20:13 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:20:13 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:20:13 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:20:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_cc51b6a8'), local_subscribe_addr='ipc:///tmp/4e4ecca5-2597-4e74-8f22-def667464e9f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m INFO 09-17 17:20:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_39fa447d'), local_subscribe_addr='ipc:///tmp/ed6cb128-40fd-4fb0-87ba-c28bcb72e87b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m INFO 09-17 17:20:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_dededf4f'), local_subscribe_addr='ipc:///tmp/f0c0a195-b389-49bf-bae5-18f60e3ee286', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m INFO 09-17 17:20:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a907d9d0'), local_subscribe_addr='ipc:///tmp/919fa786-7bb2-4659-a744-924eee18f805', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m INFO 09-17 17:20:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ef35a640'), local_subscribe_addr='ipc:///tmp/8e76ec2c-a786-41ff-b7e1-a4be067edad2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m INFO 09-17 17:20:17 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m INFO 09-17 17:20:17 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m INFO 09-17 17:20:17 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:20:17 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m INFO 09-17 17:20:17 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:20:17 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m INFO 09-17 17:20:17 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m INFO 09-17 17:20:17 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m WARNING 09-17 17:20:17 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:20:17 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m WARNING 09-17 17:20:17 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:20:17 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m INFO 09-17 17:20:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_0b20cd20'), local_subscribe_addr='ipc:///tmp/d235d105-5f77-4853-bc9b-257b95a1c637', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m INFO 09-17 17:20:17 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:20:17 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:20:17 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:20:17 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m WARNING 09-17 17:20:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:20:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m WARNING 09-17 17:20:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:20:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m INFO 09-17 17:20:17 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m INFO 09-17 17:20:17 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:20:17 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:20:17 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m INFO 09-17 17:20:18 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m INFO 09-17 17:20:18 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m INFO 09-17 17:20:18 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m INFO 09-17 17:20:18 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m INFO 09-17 17:20:18 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m INFO 09-17 17:20:18 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m INFO 09-17 17:20:18 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m INFO 09-17 17:20:18 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m INFO 09-17 17:20:18 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m INFO 09-17 17:20:18 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m INFO 09-17 17:20:18 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m INFO 09-17 17:20:18 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:11,  2.39s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:09,  2.42s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:07,  2.55s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:10<00:05,  2.63s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:12<00:02,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m INFO 09-17 17:20:34 [default_loader.py:262] Loading weights took 15.20 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m INFO 09-17 17:20:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:15<00:00,  2.63s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:15<00:00,  2.58s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m INFO 09-17 17:20:34 [default_loader.py:262] Loading weights took 15.55 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m INFO 09-17 17:20:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m INFO 09-17 17:20:34 [default_loader.py:262] Loading weights took 15.28 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m INFO 09-17 17:20:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m INFO 09-17 17:20:34 [default_loader.py:262] Loading weights took 15.19 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m INFO 09-17 17:20:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m INFO 09-17 17:20:34 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.837488 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m INFO 09-17 17:20:34 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 16.152520 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m INFO 09-17 17:20:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 16.097332 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m INFO 09-17 17:20:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.927810 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m INFO 09-17 17:20:51 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m INFO 09-17 17:20:51 [backends.py:541] Dynamo bytecode transform time: 15.27 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m INFO 09-17 17:20:51 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m INFO 09-17 17:20:51 [backends.py:541] Dynamo bytecode transform time: 15.76 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m INFO 09-17 17:20:51 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m INFO 09-17 17:20:51 [backends.py:541] Dynamo bytecode transform time: 15.84 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m INFO 09-17 17:20:51 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m INFO 09-17 17:20:51 [backends.py:541] Dynamo bytecode transform time: 15.96 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m INFO 09-17 17:21:00 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.250 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m INFO 09-17 17:21:01 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.380 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m INFO 09-17 17:21:01 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.518 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m INFO 09-17 17:21:01 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.452 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m INFO 09-17 17:21:06 [monitor.py:34] torch.compile takes 15.84 s in total\n",
      "INFO 09-17 17:21:06 [monitor.py:34] torch.compile takes 15.27 s in total\n",
      "INFO 09-17 17:21:06 [monitor.py:34] torch.compile takes 15.96 s in total\n",
      "INFO 09-17 17:21:06 [monitor.py:34] torch.compile takes 15.76 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m INFO 09-17 17:21:08 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m INFO 09-17 17:21:08 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:21:08 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m INFO 09-17 17:21:08 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:21:09 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:21:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:21:09 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:21:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:21:09 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:21:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:21:09 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:21:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m INFO 09-17 17:21:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 17:21:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m INFO 09-17 17:21:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m INFO 09-17 17:21:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:21:13 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.12 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_2_0.001/checkpoint-4995 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<41:58,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=481007)\u001b[0;0m INFO 09-17 17:21:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481004)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=481006)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=481005)\u001b[0;0m INFO 09-17 17:21:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:21:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:21:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2780.42it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 147.29it/s, est. speed input: 7705.48 toks/s, output: 403.96 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_2_0.999/checkpoint-5...\n",
      "INFO 09-17 17:21:53 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:21:53 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:21:54 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:21:54 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:21:54 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:21:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_ba550e4d'), local_subscribe_addr='ipc:///tmp/0e5e25a3-5bc3-4736-bc37-b0d274097d63', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m INFO 09-17 17:21:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8ab3f6e0'), local_subscribe_addr='ipc:///tmp/b4f7e50d-01fe-4943-95cc-458005f8fda0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-17 17:21:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0ae19491'), local_subscribe_addr='ipc:///tmp/80da7eb0-46bd-49bc-bc65-fb87323e7d9c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m INFO 09-17 17:21:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8cc5ec7a'), local_subscribe_addr='ipc:///tmp/24e81013-5798-4307-928a-7657b38ef10b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m INFO 09-17 17:21:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3e54bea3'), local_subscribe_addr='ipc:///tmp/a08420db-526d-4182-beb4-1a33b68ef861', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m INFO 09-17 17:21:56 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:21:56 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m INFO 09-17 17:21:56 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m INFO 09-17 17:21:56 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:21:56 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:21:56 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m INFO 09-17 17:21:56 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m INFO 09-17 17:21:56 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m WARNING 09-17 17:21:57 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:21:57 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m WARNING 09-17 17:21:57 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m WARNING 09-17 17:21:57 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m INFO 09-17 17:21:57 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_f366ffe8'), local_subscribe_addr='ipc:///tmp/e75478a2-8e84-4989-a453-6742c99d0cb4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m INFO 09-17 17:21:57 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m INFO 09-17 17:21:57 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:21:57 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:21:57 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m WARNING 09-17 17:21:57 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m WARNING 09-17 17:21:57 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:21:57 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:21:57 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m INFO 09-17 17:21:57 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m INFO 09-17 17:21:57 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:21:57 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m INFO 09-17 17:21:57 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m INFO 09-17 17:21:57 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m INFO 09-17 17:21:57 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m INFO 09-17 17:21:57 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m INFO 09-17 17:21:57 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m INFO 09-17 17:21:57 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m INFO 09-17 17:21:57 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m INFO 09-17 17:21:57 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m INFO 09-17 17:21:57 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m INFO 09-17 17:21:57 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m INFO 09-17 17:21:57 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m INFO 09-17 17:21:58 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m INFO 09-17 17:21:58 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.65s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.73s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.72s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.68s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.55s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.49s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.58s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m INFO 09-17 17:22:07 [default_loader.py:262] Loading weights took 9.52 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m INFO 09-17 17:22:07 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m INFO 09-17 17:22:08 [default_loader.py:262] Loading weights took 9.93 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m INFO 09-17 17:22:08 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m INFO 09-17 17:22:08 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.149112 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m INFO 09-17 17:22:08 [default_loader.py:262] Loading weights took 9.86 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m INFO 09-17 17:22:08 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m INFO 09-17 17:22:08 [default_loader.py:262] Loading weights took 10.18 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m INFO 09-17 17:22:08 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m INFO 09-17 17:22:08 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.681118 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m INFO 09-17 17:22:08 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.804089 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m INFO 09-17 17:22:09 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.900305 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m INFO 09-17 17:22:25 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m INFO 09-17 17:22:25 [backends.py:541] Dynamo bytecode transform time: 15.60 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m INFO 09-17 17:22:25 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m INFO 09-17 17:22:25 [backends.py:541] Dynamo bytecode transform time: 15.79 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m INFO 09-17 17:22:25 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m INFO 09-17 17:22:25 [backends.py:541] Dynamo bytecode transform time: 15.94 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m INFO 09-17 17:22:25 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m INFO 09-17 17:22:25 [backends.py:541] Dynamo bytecode transform time: 15.98 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m INFO 09-17 17:22:34 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.184 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m INFO 09-17 17:22:35 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.326 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m INFO 09-17 17:22:35 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.348 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m INFO 09-17 17:22:35 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.575 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m INFO 09-17 17:22:41 [monitor.py:34] torch.compile takes 15.98 s in total\n",
      "INFO 09-17 17:22:41 [monitor.py:34] torch.compile takes 15.94 s in total\n",
      "INFO 09-17 17:22:41 [monitor.py:34] torch.compile takes 15.60 s in total\n",
      "INFO 09-17 17:22:41 [monitor.py:34] torch.compile takes 15.79 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m INFO 09-17 17:22:42 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m INFO 09-17 17:22:42 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m INFO 09-17 17:22:42 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m INFO 09-17 17:22:42 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:22:43 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:22:43 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:22:43 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:22:43 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:22:43 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:22:43 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:22:43 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:22:43 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m INFO 09-17 17:22:46 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m INFO 09-17 17:22:46 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m INFO 09-17 17:22:46 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m INFO 09-17 17:22:46 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:22:47 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.86 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_2_0.999/checkpoint-5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<47:13,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=481868)\u001b[0;0m INFO 09-17 17:22:48 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=481871)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=481870)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=481869)\u001b[0;0m INFO 09-17 17:22:48 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:22:48 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:22:48 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2705.57it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 147.66it/s, est. speed input: 7725.16 toks/s, output: 428.29 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_0_0.01/checkpoint-4950...\n",
      "INFO 09-17 17:23:26 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:23:26 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:23:26 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:23:26 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:23:26 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:23:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_baff5d0e'), local_subscribe_addr='ipc:///tmp/e51491fa-518d-4533-8156-bfdd584a35de', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m INFO 09-17 17:23:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_76745b30'), local_subscribe_addr='ipc:///tmp/4dd29ff2-896f-498c-9bdd-a6661700568c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m INFO 09-17 17:23:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2fe9314b'), local_subscribe_addr='ipc:///tmp/1f18251f-224c-4f4c-a9b3-6f395143cb83', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m INFO 09-17 17:23:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8f9e5320'), local_subscribe_addr='ipc:///tmp/bcc02015-0bc6-4537-8155-e3752cc4115d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m INFO 09-17 17:23:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a3bdd610'), local_subscribe_addr='ipc:///tmp/d73070ee-e3df-4624-90e6-d2ed8d30fdde', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m INFO 09-17 17:23:29 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m INFO 09-17 17:23:29 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:23:29 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:23:29 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m INFO 09-17 17:23:29 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m INFO 09-17 17:23:29 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:23:29 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:23:29 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m WARNING 09-17 17:23:30 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:23:30 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m WARNING 09-17 17:23:30 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m WARNING 09-17 17:23:30 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m INFO 09-17 17:23:30 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_d71483f5'), local_subscribe_addr='ipc:///tmp/386c5b9c-5663-43f9-8623-f8bb3dda19e6', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m INFO 09-17 17:23:30 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:23:30 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:23:30 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:23:30 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m WARNING 09-17 17:23:30 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m WARNING 09-17 17:23:30 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m WARNING 09-17 17:23:30 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m WARNING 09-17 17:23:30 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 17:23:30 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m INFO 09-17 17:23:30 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m INFO 09-17 17:23:30 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m INFO 09-17 17:23:30 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m INFO 09-17 17:23:30 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m INFO 09-17 17:23:30 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m INFO 09-17 17:23:30 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m INFO 09-17 17:23:30 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m INFO 09-17 17:23:30 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m INFO 09-17 17:23:31 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m INFO 09-17 17:23:31 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m INFO 09-17 17:23:31 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m INFO 09-17 17:23:31 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m INFO 09-17 17:23:31 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m INFO 09-17 17:23:31 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m INFO 09-17 17:23:31 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:07,  1.59s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.59s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.58s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.56s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.45s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.45s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.50s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m INFO 09-17 17:23:40 [default_loader.py:262] Loading weights took 9.04 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m INFO 09-17 17:23:40 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m INFO 09-17 17:23:40 [default_loader.py:262] Loading weights took 9.29 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m INFO 09-17 17:23:40 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m INFO 09-17 17:23:40 [default_loader.py:262] Loading weights took 9.19 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m INFO 09-17 17:23:40 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m INFO 09-17 17:23:40 [default_loader.py:262] Loading weights took 9.33 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m INFO 09-17 17:23:40 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m INFO 09-17 17:23:40 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.587140 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m INFO 09-17 17:23:41 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.898965 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m INFO 09-17 17:23:41 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.003651 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m INFO 09-17 17:23:41 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.937239 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m INFO 09-17 17:23:57 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m INFO 09-17 17:23:57 [backends.py:541] Dynamo bytecode transform time: 15.51 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m INFO 09-17 17:23:57 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m INFO 09-17 17:23:57 [backends.py:541] Dynamo bytecode transform time: 15.75 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m INFO 09-17 17:23:58 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m INFO 09-17 17:23:58 [backends.py:541] Dynamo bytecode transform time: 15.85 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m INFO 09-17 17:23:58 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m INFO 09-17 17:23:58 [backends.py:541] Dynamo bytecode transform time: 15.87 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m INFO 09-17 17:24:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.384 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m INFO 09-17 17:24:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.493 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m INFO 09-17 17:24:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.398 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m INFO 09-17 17:24:08 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.604 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m INFO 09-17 17:24:13 [monitor.py:34] torch.compile takes 15.87 s in total\n",
      "INFO 09-17 17:24:13 [monitor.py:34] torch.compile takes 15.51 s in total\n",
      "INFO 09-17 17:24:13 [monitor.py:34] torch.compile takes 15.75 s in total\n",
      "INFO 09-17 17:24:13 [monitor.py:34] torch.compile takes 15.85 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m INFO 09-17 17:24:14 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m INFO 09-17 17:24:14 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m INFO 09-17 17:24:15 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m INFO 09-17 17:24:15 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:24:15 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:24:15 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:24:15 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:24:15 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:24:15 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:24:15 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:24:15 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:24:15 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m INFO 09-17 17:24:19 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:24:19 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m INFO 09-17 17:24:19 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m INFO 09-17 17:24:19 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:24:19 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.98 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_0_0.01/checkpoint-4950 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<1:14:41,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=482728)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=482731)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=482727)\u001b[0;0m INFO 09-17 17:24:21 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=482730)\u001b[0;0m INFO 09-17 17:24:21 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:24:21 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:24:21 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 2187.47it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 155.49it/s, est. speed input: 8134.37 toks/s, output: 403.81 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.8/checkpoint-1000...\n",
      "INFO 09-17 17:24:56 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:24:56 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:24:57 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:24:57 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:24:57 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:24:57 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_a0061582'), local_subscribe_addr='ipc:///tmp/1df10438-640b-4dfe-99fb-0a5822744a0c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m INFO 09-17 17:24:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a608df46'), local_subscribe_addr='ipc:///tmp/5767968a-9073-430f-adf3-3af84c98ddd8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m INFO 09-17 17:24:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e06e9855'), local_subscribe_addr='ipc:///tmp/2f8bc8ea-16fc-44c8-897c-eb4cfd8a2ae1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m INFO 09-17 17:24:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_539f78cb'), local_subscribe_addr='ipc:///tmp/d54078fa-f05f-4b10-b399-f066ed0c46fe', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m INFO 09-17 17:24:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_663f4428'), local_subscribe_addr='ipc:///tmp/3e5b8eb3-c124-42ee-96e8-1173abdf373f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m INFO 09-17 17:25:00 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m INFO 09-17 17:25:00 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m INFO 09-17 17:25:00 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:25:00 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m INFO 09-17 17:25:00 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:25:00 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:25:00 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m INFO 09-17 17:25:00 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m WARNING 09-17 17:25:00 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:25:00 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m WARNING 09-17 17:25:00 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m WARNING 09-17 17:25:00 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m INFO 09-17 17:25:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_d04c31ca'), local_subscribe_addr='ipc:///tmp/ab6bbeca-7c2e-45e8-bfcf-e2de256e81f4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m INFO 09-17 17:25:01 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:25:01 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m INFO 09-17 17:25:01 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:25:01 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m WARNING 09-17 17:25:01 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:25:01 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m INFO 09-17 17:25:01 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m WARNING 09-17 17:25:01 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 17:25:01 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m WARNING 09-17 17:25:01 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m INFO 09-17 17:25:01 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m INFO 09-17 17:25:01 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m INFO 09-17 17:25:01 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m INFO 09-17 17:25:01 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m INFO 09-17 17:25:01 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m INFO 09-17 17:25:01 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m INFO 09-17 17:25:01 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m INFO 09-17 17:25:01 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m INFO 09-17 17:25:01 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m INFO 09-17 17:25:01 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m INFO 09-17 17:25:01 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m INFO 09-17 17:25:01 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m INFO 09-17 17:25:01 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m INFO 09-17 17:25:01 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:07,  1.55s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.52s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.49s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:05<00:02,  1.45s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.34s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.30s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.38s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m INFO 09-17 17:25:10 [default_loader.py:262] Loading weights took 8.32 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m INFO 09-17 17:25:10 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m INFO 09-17 17:25:10 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 8.847316 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m INFO 09-17 17:25:10 [default_loader.py:262] Loading weights took 8.89 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m INFO 09-17 17:25:10 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m INFO 09-17 17:25:10 [default_loader.py:262] Loading weights took 8.67 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m INFO 09-17 17:25:10 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m INFO 09-17 17:25:10 [default_loader.py:262] Loading weights took 8.92 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m INFO 09-17 17:25:10 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m INFO 09-17 17:25:11 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.501713 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m INFO 09-17 17:25:11 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.535439 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m INFO 09-17 17:25:11 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.455814 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m INFO 09-17 17:25:27 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m INFO 09-17 17:25:27 [backends.py:541] Dynamo bytecode transform time: 15.52 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m INFO 09-17 17:25:28 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m INFO 09-17 17:25:28 [backends.py:541] Dynamo bytecode transform time: 15.69 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m INFO 09-17 17:25:28 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m INFO 09-17 17:25:28 [backends.py:541] Dynamo bytecode transform time: 15.84 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m INFO 09-17 17:25:28 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m INFO 09-17 17:25:28 [backends.py:541] Dynamo bytecode transform time: 15.84 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m INFO 09-17 17:25:37 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.395 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m INFO 09-17 17:25:37 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.324 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m INFO 09-17 17:25:38 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.470 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m INFO 09-17 17:25:38 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.604 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m INFO 09-17 17:25:43 [monitor.py:34] torch.compile takes 15.52 s in total\n",
      "INFO 09-17 17:25:43 [monitor.py:34] torch.compile takes 15.69 s in total\n",
      "INFO 09-17 17:25:43 [monitor.py:34] torch.compile takes 15.84 s in total\n",
      "INFO 09-17 17:25:43 [monitor.py:34] torch.compile takes 15.84 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m INFO 09-17 17:25:45 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m INFO 09-17 17:25:45 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m INFO 09-17 17:25:45 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m INFO 09-17 17:25:45 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:25:45 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:25:45 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:25:45 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:25:45 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:25:45 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:25:45 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:25:45 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:25:45 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m INFO 09-17 17:25:49 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m INFO 09-17 17:25:49 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m INFO 09-17 17:25:49 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m INFO 09-17 17:25:49 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:25:49 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.88 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.8/checkpoint-1000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<48:38,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=483578)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=483575)\u001b[0;0m INFO 09-17 17:25:51 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=483577)\u001b[0;0m INFO 09-17 17:25:51 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=483576)\u001b[0;0m INFO 09-17 17:25:51 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:25:51 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2551.96it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 151.01it/s, est. speed input: 7900.12 toks/s, output: 402.69 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank3]:[W917 17:26:26.927365286 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=211, addr=[::ffff:127.0.0.1]:48788, remote=[::ffff:127.0.0.1]:33421): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7225261785e8 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8bfe (0x7224fe97fbfe in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baaf40 (0x7224fe981f40 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab84a (0x7224fe98284a in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7224fe97c2a9 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7224c007b9f9 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdc253 (0x722538df9253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7225396feac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #8: clone + 0x44 (0x72253978fbf4 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[W917 17:26:26.948557937 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 3] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n",
      "[rank2]:[W917 17:26:26.927470665 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=208, addr=[::ffff:127.0.0.1]:48786, remote=[::ffff:127.0.0.1]:33421): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7225261785e8 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8bfe (0x7224fe97fbfe in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baaf40 (0x7224fe981f40 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab84a (0x7224fe98284a in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7224fe97c2a9 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7224c007b9f9 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdc253 (0x722538df9253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7225396feac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #8: clone + 0x44 (0x72253978fbf4 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[W917 17:26:26.976525320 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 2] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.8/checkpoint-1000...\n",
      "INFO 09-17 17:26:27 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:26:27 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:26:28 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:26:28 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:26:28 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:26:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_ccb8d1ae'), local_subscribe_addr='ipc:///tmp/eff7e0a9-2e7c-4260-afc9-26f4ca5f3ca5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m INFO 09-17 17:26:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_66d1a24a'), local_subscribe_addr='ipc:///tmp/f286a1c7-fc17-47b6-aead-001c71c74f76', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m INFO 09-17 17:26:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bc3a61ef'), local_subscribe_addr='ipc:///tmp/8f60c4f4-822a-43d8-8fec-945c56e26a93', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m INFO 09-17 17:26:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f66bca99'), local_subscribe_addr='ipc:///tmp/7c9368ec-ad08-41a7-970e-2af96c7818ca', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m INFO 09-17 17:26:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_03a22696'), local_subscribe_addr='ipc:///tmp/9404db61-6d36-462b-80d8-d7e71ca7661f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m INFO 09-17 17:26:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:26:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:26:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m INFO 09-17 17:26:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:26:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:26:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:26:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m INFO 09-17 17:26:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m WARNING 09-17 17:26:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:26:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m WARNING 09-17 17:26:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:26:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m INFO 09-17 17:26:33 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_01f9d41d'), local_subscribe_addr='ipc:///tmp/5f02c31c-246f-4719-b5a0-b1e92c05aa54', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m INFO 09-17 17:26:33 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:26:33 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:26:33 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:26:33 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m WARNING 09-17 17:26:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m WARNING 09-17 17:26:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m WARNING 09-17 17:26:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m WARNING 09-17 17:26:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m INFO 09-17 17:26:33 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m INFO 09-17 17:26:33 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m INFO 09-17 17:26:33 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m INFO 09-17 17:26:33 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m INFO 09-17 17:26:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m INFO 09-17 17:26:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m INFO 09-17 17:26:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m INFO 09-17 17:26:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m INFO 09-17 17:26:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m INFO 09-17 17:26:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m INFO 09-17 17:26:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m INFO 09-17 17:26:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m INFO 09-17 17:26:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m INFO 09-17 17:26:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m INFO 09-17 17:26:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m INFO 09-17 17:26:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.61s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.64s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.66s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:04,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m INFO 09-17 17:26:43 [default_loader.py:262] Loading weights took 8.92 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m INFO 09-17 17:26:43 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:09<00:01,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m INFO 09-17 17:26:43 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.599272 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m INFO 09-17 17:26:45 [default_loader.py:262] Loading weights took 10.82 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m INFO 09-17 17:26:45 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m INFO 09-17 17:26:45 [default_loader.py:262] Loading weights took 10.70 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m INFO 09-17 17:26:45 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.85s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.83s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m INFO 09-17 17:26:45 [default_loader.py:262] Loading weights took 11.03 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m INFO 09-17 17:26:45 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m INFO 09-17 17:26:45 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.606007 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m INFO 09-17 17:26:46 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.659501 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m INFO 09-17 17:26:46 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.982132 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m INFO 09-17 17:27:02 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m INFO 09-17 17:27:02 [backends.py:541] Dynamo bytecode transform time: 15.51 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m INFO 09-17 17:27:02 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m INFO 09-17 17:27:02 [backends.py:541] Dynamo bytecode transform time: 15.66 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m INFO 09-17 17:27:02 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m INFO 09-17 17:27:02 [backends.py:541] Dynamo bytecode transform time: 15.70 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m INFO 09-17 17:27:02 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m INFO 09-17 17:27:02 [backends.py:541] Dynamo bytecode transform time: 15.89 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m INFO 09-17 17:27:12 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.207 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m INFO 09-17 17:27:12 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.438 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m INFO 09-17 17:27:12 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.555 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m INFO 09-17 17:27:12 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.474 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m INFO 09-17 17:27:18 [monitor.py:34] torch.compile takes 15.51 s in total\n",
      "INFO 09-17 17:27:18 [monitor.py:34] torch.compile takes 15.66 s in total\n",
      "INFO 09-17 17:27:18 [monitor.py:34] torch.compile takes 15.70 s in total\n",
      "INFO 09-17 17:27:18 [monitor.py:34] torch.compile takes 15.89 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m INFO 09-17 17:27:19 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m INFO 09-17 17:27:19 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m INFO 09-17 17:27:19 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m INFO 09-17 17:27:19 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "INFO 09-17 17:27:20 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:27:20 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:27:20 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:27:20 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:27:20 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:27:20 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:27:20 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:27:20 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m INFO 09-17 17:27:24 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m INFO 09-17 17:27:24 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:27:24 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m INFO 09-17 17:27:24 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:27:24 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.16 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.8/checkpoint-1000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<50:25,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=484419)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=484418)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=484420)\u001b[0;0m INFO 09-17 17:27:26 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:27:26 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:27:26 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=484417)\u001b[0;0m INFO 09-17 17:27:26 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2521.67it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 155.87it/s, est. speed input: 8154.45 toks/s, output: 428.46 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.2/checkpoint-4000...\n",
      "INFO 09-17 17:28:01 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:28:01 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:28:02 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:28:02 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:28:02 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:28:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_74daefa0'), local_subscribe_addr='ipc:///tmp/75de52c9-56ef-4383-a38c-d1387774c06b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m INFO 09-17 17:28:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_148d1216'), local_subscribe_addr='ipc:///tmp/cd4059e2-c566-474d-ad38-e15b4e012b03', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m INFO 09-17 17:28:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_304c14b4'), local_subscribe_addr='ipc:///tmp/04753c89-a25d-48a5-95cc-f34dd55c32fe', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m INFO 09-17 17:28:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0b064cdd'), local_subscribe_addr='ipc:///tmp/c90d77fb-ac25-4400-8a85-62fae488100e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m INFO 09-17 17:28:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ab9cb0ae'), local_subscribe_addr='ipc:///tmp/9b7f4c31-d014-440c-9ae8-04f04d5e2125', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m INFO 09-17 17:28:06 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:28:06 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:28:06 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:28:06 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m INFO 09-17 17:28:06 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m INFO 09-17 17:28:06 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:28:06 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:28:06 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m WARNING 09-17 17:28:06 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:28:06 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m WARNING 09-17 17:28:06 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m WARNING 09-17 17:28:06 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m INFO 09-17 17:28:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_5d646e4e'), local_subscribe_addr='ipc:///tmp/d0450080-172d-40c3-84ab-16dddd505a30', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m INFO 09-17 17:28:06 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:28:06 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:28:06 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:28:06 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m WARNING 09-17 17:28:06 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m WARNING 09-17 17:28:06 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:28:06 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m INFO 09-17 17:28:06 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:28:06 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:28:06 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m WARNING 09-17 17:28:06 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m INFO 09-17 17:28:06 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m INFO 09-17 17:28:07 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m INFO 09-17 17:28:07 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m INFO 09-17 17:28:07 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m INFO 09-17 17:28:07 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m INFO 09-17 17:28:07 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m INFO 09-17 17:28:07 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m INFO 09-17 17:28:07 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m INFO 09-17 17:28:07 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m INFO 09-17 17:28:07 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m INFO 09-17 17:28:07 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m INFO 09-17 17:28:07 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 09-17 17:28:07 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:12,  2.57s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:05<00:10,  2.52s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:08,  2.70s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:10<00:05,  2.73s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:12<00:02,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m INFO 09-17 17:28:21 [default_loader.py:262] Loading weights took 13.40 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m INFO 09-17 17:28:21 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m INFO 09-17 17:28:21 [default_loader.py:262] Loading weights took 13.91 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m INFO 09-17 17:28:21 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m INFO 09-17 17:28:21 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.009331 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:14<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:14<00:00,  2.38s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 17:28:22 [default_loader.py:262] Loading weights took 14.39 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m INFO 09-17 17:28:22 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m INFO 09-17 17:28:22 [default_loader.py:262] Loading weights took 14.35 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m INFO 09-17 17:28:22 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m INFO 09-17 17:28:22 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.492410 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m INFO 09-17 17:28:23 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.041689 seconds\n",
      "INFO 09-17 17:28:23 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.108901 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m INFO 09-17 17:28:39 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m INFO 09-17 17:28:39 [backends.py:541] Dynamo bytecode transform time: 15.63 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m INFO 09-17 17:28:39 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m INFO 09-17 17:28:39 [backends.py:541] Dynamo bytecode transform time: 15.82 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m INFO 09-17 17:28:39 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m INFO 09-17 17:28:39 [backends.py:541] Dynamo bytecode transform time: 15.89 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m INFO 09-17 17:28:40 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m INFO 09-17 17:28:40 [backends.py:541] Dynamo bytecode transform time: 16.36 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m INFO 09-17 17:28:49 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.259 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m INFO 09-17 17:28:49 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.451 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m INFO 09-17 17:28:49 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.511 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m INFO 09-17 17:28:50 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.436 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m INFO 09-17 17:28:55 [monitor.py:34] torch.compile takes 15.63 s in total\n",
      "INFO 09-17 17:28:55 [monitor.py:34] torch.compile takes 16.36 s in total\n",
      "INFO 09-17 17:28:55 [monitor.py:34] torch.compile takes 15.82 s in total\n",
      "INFO 09-17 17:28:55 [monitor.py:34] torch.compile takes 15.89 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m INFO 09-17 17:28:56 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m INFO 09-17 17:28:56 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m INFO 09-17 17:28:56 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m INFO 09-17 17:28:57 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:28:58 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:28:58 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:28:58 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:28:58 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:28:58 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:28:58 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:28:58 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:28:58 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m INFO 09-17 17:29:02 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m INFO 09-17 17:29:02 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m INFO 09-17 17:29:02 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m INFO 09-17 17:29:02 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:29:02 [core.py:193] init engine (profile, create kv cache, warmup model) took 39.64 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.2/checkpoint-4000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<51:18,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=485270)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=485268)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=485271)\u001b[0;0m INFO 09-17 17:29:05 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=485269)\u001b[0;0m INFO 09-17 17:29:05 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:29:05 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:29:05 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 1750.54it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 158.43it/s, est. speed input: 8288.28 toks/s, output: 425.51 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.001/checkpoint-4995...\n",
      "INFO 09-17 17:29:40 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:29:40 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:29:41 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:29:41 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:29:41 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:29:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_96366719'), local_subscribe_addr='ipc:///tmp/5079c853-a072-4a7d-a893-5f9953511ae5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m INFO 09-17 17:29:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d2a5685a'), local_subscribe_addr='ipc:///tmp/dd9a06eb-b55e-4bf8-a693-24399fef71c8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m INFO 09-17 17:29:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a935a482'), local_subscribe_addr='ipc:///tmp/2c13b062-c503-4792-a4fd-211465a788c0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m INFO 09-17 17:29:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ecc31f14'), local_subscribe_addr='ipc:///tmp/2537eb97-116c-4aa2-a62f-61e96305dc9e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m INFO 09-17 17:29:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_40155aa6'), local_subscribe_addr='ipc:///tmp/6fe2ecb8-1637-4efa-ae41-3ce31f318061', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m INFO 09-17 17:29:44 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m INFO 09-17 17:29:44 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:29:44 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m INFO 09-17 17:29:44 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m INFO 09-17 17:29:44 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:29:44 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m INFO 09-17 17:29:44 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:29:44 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m WARNING 09-17 17:29:44 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:29:44 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m WARNING 09-17 17:29:44 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m WARNING 09-17 17:29:44 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m INFO 09-17 17:29:44 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_bfea196d'), local_subscribe_addr='ipc:///tmp/e2efba73-6241-44a9-b7af-44810e888c92', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m INFO 09-17 17:29:44 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m INFO 09-17 17:29:44 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m INFO 09-17 17:29:44 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:29:44 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m WARNING 09-17 17:29:44 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:29:44 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m WARNING 09-17 17:29:44 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m INFO 09-17 17:29:44 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m WARNING 09-17 17:29:44 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 17:29:44 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m INFO 09-17 17:29:44 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m INFO 09-17 17:29:44 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m INFO 09-17 17:29:45 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m INFO 09-17 17:29:45 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m INFO 09-17 17:29:45 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m INFO 09-17 17:29:45 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m INFO 09-17 17:29:45 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 09-17 17:29:45 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m INFO 09-17 17:29:45 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m INFO 09-17 17:29:45 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m INFO 09-17 17:29:45 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m INFO 09-17 17:29:45 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m INFO 09-17 17:29:45 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m INFO 09-17 17:29:45 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.79s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:07,  1.80s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.82s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:03,  1.84s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m INFO 09-17 17:29:56 [default_loader.py:262] Loading weights took 10.63 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m INFO 09-17 17:29:56 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m INFO 09-17 17:29:56 [default_loader.py:262] Loading weights took 10.63 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m INFO 09-17 17:29:56 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.74s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.77s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m INFO 09-17 17:29:56 [default_loader.py:262] Loading weights took 10.68 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m INFO 09-17 17:29:56 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m INFO 09-17 17:29:56 [default_loader.py:262] Loading weights took 10.62 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m INFO 09-17 17:29:56 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m INFO 09-17 17:29:56 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.215187 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m INFO 09-17 17:29:57 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.335316 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m INFO 09-17 17:29:57 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.473960 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m INFO 09-17 17:29:57 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.489044 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m INFO 09-17 17:30:13 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m INFO 09-17 17:30:13 [backends.py:541] Dynamo bytecode transform time: 15.60 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m INFO 09-17 17:30:13 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m INFO 09-17 17:30:13 [backends.py:541] Dynamo bytecode transform time: 15.63 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m INFO 09-17 17:30:13 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m INFO 09-17 17:30:13 [backends.py:541] Dynamo bytecode transform time: 15.78 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m INFO 09-17 17:30:13 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m INFO 09-17 17:30:13 [backends.py:541] Dynamo bytecode transform time: 16.00 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m INFO 09-17 17:30:23 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.557 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m INFO 09-17 17:30:23 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.460 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m INFO 09-17 17:30:23 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.496 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m INFO 09-17 17:30:24 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.435 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m INFO 09-17 17:30:29 [monitor.py:34] torch.compile takes 15.60 s in total\n",
      "INFO 09-17 17:30:29 [monitor.py:34] torch.compile takes 16.00 s in total\n",
      "INFO 09-17 17:30:29 [monitor.py:34] torch.compile takes 15.78 s in total\n",
      "INFO 09-17 17:30:29 [monitor.py:34] torch.compile takes 15.63 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m INFO 09-17 17:30:30 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m INFO 09-17 17:30:31 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m INFO 09-17 17:30:31 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m INFO 09-17 17:30:31 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:30:31 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:30:31 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:30:31 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:30:31 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:30:31 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:30:31 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:30:31 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:30:31 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m INFO 09-17 17:30:35 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m INFO 09-17 17:30:35 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m INFO 09-17 17:30:35 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m INFO 09-17 17:30:35 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:30:35 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.45 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.001/checkpoint-4995 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<43:40,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=486315)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=486316)\u001b[0;0m INFO 09-17 17:30:37 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:30:37 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=486318)\u001b[0;0m INFO 09-17 17:30:37 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=486317)\u001b[0;0m INFO 09-17 17:30:37 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 2462.26it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 156.50it/s, est. speed input: 8187.28 toks/s, output: 415.29 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_0_0.001/checkpoint-4995...\n",
      "INFO 09-17 17:31:12 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:31:12 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:31:13 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:31:13 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:31:13 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:31:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_ede278b9'), local_subscribe_addr='ipc:///tmp/0c7f22ab-521e-4da1-a9c8-7b75bde9cbda', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m INFO 09-17 17:31:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_459e4fa2'), local_subscribe_addr='ipc:///tmp/f925e70f-afee-40d1-8705-6d3793cfeb15', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m INFO 09-17 17:31:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f5e2aa71'), local_subscribe_addr='ipc:///tmp/f07aa4d0-9ce4-4205-8e57-53e6bb25062e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m INFO 09-17 17:31:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1c585fca'), local_subscribe_addr='ipc:///tmp/6aee70ce-78b1-4875-ad41-2ffd8a784a61', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m INFO 09-17 17:31:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d1b7ce56'), local_subscribe_addr='ipc:///tmp/f05d5921-eb93-46ad-a017-93e07667a146', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m INFO 09-17 17:31:16 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:31:16 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:31:16 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m INFO 09-17 17:31:16 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:31:16 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:31:16 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m INFO 09-17 17:31:16 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m INFO 09-17 17:31:16 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m WARNING 09-17 17:31:17 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:31:17 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m WARNING 09-17 17:31:17 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m WARNING 09-17 17:31:17 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m INFO 09-17 17:31:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_e3bf3754'), local_subscribe_addr='ipc:///tmp/d9220b41-abdf-44e0-bd6c-732ec526aa9a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m INFO 09-17 17:31:17 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:31:17 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m INFO 09-17 17:31:17 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:31:17 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m WARNING 09-17 17:31:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m WARNING 09-17 17:31:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m WARNING 09-17 17:31:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m INFO 09-17 17:31:17 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m INFO 09-17 17:31:17 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m WARNING 09-17 17:31:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m INFO 09-17 17:31:17 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m INFO 09-17 17:31:17 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m INFO 09-17 17:31:17 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m INFO 09-17 17:31:17 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m INFO 09-17 17:31:17 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m INFO 09-17 17:31:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m INFO 09-17 17:31:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m INFO 09-17 17:31:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m INFO 09-17 17:31:18 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m INFO 09-17 17:31:18 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m INFO 09-17 17:31:18 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m INFO 09-17 17:31:18 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 09-17 17:31:18 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m INFO 09-17 17:31:18 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:11,  2.36s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:09,  2.46s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:07,  2.49s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:09<00:04,  2.49s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:11<00:02,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m INFO 09-17 17:31:32 [default_loader.py:262] Loading weights took 13.91 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m INFO 09-17 17:31:32 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:14<00:00,  2.30s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:14<00:00,  2.37s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m INFO 09-17 17:31:32 [default_loader.py:262] Loading weights took 14.25 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m INFO 09-17 17:31:32 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m INFO 09-17 17:31:33 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.678908 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m INFO 09-17 17:31:33 [default_loader.py:262] Loading weights took 14.51 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m INFO 09-17 17:31:33 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m INFO 09-17 17:31:33 [default_loader.py:262] Loading weights took 14.86 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m INFO 09-17 17:31:33 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m INFO 09-17 17:31:33 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.060153 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m INFO 09-17 17:31:34 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.761813 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m INFO 09-17 17:31:34 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.938084 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m INFO 09-17 17:31:50 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m INFO 09-17 17:31:50 [backends.py:541] Dynamo bytecode transform time: 15.66 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m INFO 09-17 17:31:50 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m INFO 09-17 17:31:50 [backends.py:541] Dynamo bytecode transform time: 15.80 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m INFO 09-17 17:31:50 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m INFO 09-17 17:31:50 [backends.py:541] Dynamo bytecode transform time: 15.92 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m INFO 09-17 17:31:50 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m INFO 09-17 17:31:50 [backends.py:541] Dynamo bytecode transform time: 16.11 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m INFO 09-17 17:32:00 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.528 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m INFO 09-17 17:32:00 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.435 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m INFO 09-17 17:32:00 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.602 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m INFO 09-17 17:32:01 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.750 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m INFO 09-17 17:32:06 [monitor.py:34] torch.compile takes 15.92 s in total\n",
      "INFO 09-17 17:32:06 [monitor.py:34] torch.compile takes 16.11 s in total\n",
      "INFO 09-17 17:32:06 [monitor.py:34] torch.compile takes 15.80 s in total\n",
      "INFO 09-17 17:32:06 [monitor.py:34] torch.compile takes 15.66 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m INFO 09-17 17:32:08 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m INFO 09-17 17:32:08 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m INFO 09-17 17:32:08 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m INFO 09-17 17:32:08 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:32:08 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:32:08 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:32:08 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:32:08 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:32:08 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:32:08 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:32:08 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:32:08 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m INFO 09-17 17:32:12 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m INFO 09-17 17:32:12 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m INFO 09-17 17:32:12 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m INFO 09-17 17:32:12 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:32:13 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.77 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_0_0.001/checkpoint-4995 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<45:30,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=487160)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=487161)\u001b[0;0m INFO 09-17 17:32:14 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=487159)\u001b[0;0m INFO 09-17 17:32:14 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:32:14 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=487163)\u001b[0;0m INFO 09-17 17:32:14 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2735.85it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 154.53it/s, est. speed input: 8084.26 toks/s, output: 412.60 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_0_0.4/checkpoint-3000...\n",
      "INFO 09-17 17:32:49 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:32:49 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:32:50 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:32:50 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:32:50 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:32:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_6049416e'), local_subscribe_addr='ipc:///tmp/425c04c6-6afe-4ae0-aaa7-5696c39283a4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m INFO 09-17 17:32:53 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_70678523'), local_subscribe_addr='ipc:///tmp/01ff5642-e804-4fe5-9f03-dc5e607e7433', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m INFO 09-17 17:32:53 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_edfbe38f'), local_subscribe_addr='ipc:///tmp/e935edcd-ca10-49fd-9db0-342f63fc5665', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m INFO 09-17 17:32:53 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_26b8fc79'), local_subscribe_addr='ipc:///tmp/f6c5ba86-52d0-4933-837c-49f5d8344013', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m INFO 09-17 17:32:53 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_24747704'), local_subscribe_addr='ipc:///tmp/f1163e77-227d-4468-aeef-c88e595d8bf0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m INFO 09-17 17:32:54 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:32:54 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:32:54 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m INFO 09-17 17:32:54 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:32:54 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:32:54 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m INFO 09-17 17:32:54 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m INFO 09-17 17:32:54 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m WARNING 09-17 17:32:54 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:32:54 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m WARNING 09-17 17:32:54 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m WARNING 09-17 17:32:54 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m INFO 09-17 17:32:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_61d0ed00'), local_subscribe_addr='ipc:///tmp/d3bbb99d-db33-4fb1-b65c-79bcab280a55', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m INFO 09-17 17:32:54 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:32:54 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m INFO 09-17 17:32:54 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:32:54 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m WARNING 09-17 17:32:54 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:32:54 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m WARNING 09-17 17:32:54 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:32:54 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m INFO 09-17 17:32:54 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m INFO 09-17 17:32:54 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:32:54 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m INFO 09-17 17:32:54 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m INFO 09-17 17:32:54 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m INFO 09-17 17:32:54 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m INFO 09-17 17:32:54 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m INFO 09-17 17:32:54 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m INFO 09-17 17:32:54 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m INFO 09-17 17:32:54 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m INFO 09-17 17:32:54 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m INFO 09-17 17:32:54 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m INFO 09-17 17:32:55 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 09-17 17:32:55 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m INFO 09-17 17:32:55 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m INFO 09-17 17:32:55 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.61s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.60s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.59s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.62s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m INFO 09-17 17:33:04 [default_loader.py:262] Loading weights took 8.99 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m INFO 09-17 17:33:04 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m INFO 09-17 17:33:04 [default_loader.py:262] Loading weights took 9.04 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m INFO 09-17 17:33:04 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.45s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.51s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m INFO 09-17 17:33:04 [default_loader.py:262] Loading weights took 9.14 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m INFO 09-17 17:33:04 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m INFO 09-17 17:33:04 [default_loader.py:262] Loading weights took 9.27 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m INFO 09-17 17:33:04 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m INFO 09-17 17:33:04 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.596025 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m INFO 09-17 17:33:05 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.700282 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m INFO 09-17 17:33:05 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.078166 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m INFO 09-17 17:33:05 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.130198 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m INFO 09-17 17:33:21 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m INFO 09-17 17:33:21 [backends.py:541] Dynamo bytecode transform time: 15.59 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m INFO 09-17 17:33:22 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m INFO 09-17 17:33:22 [backends.py:541] Dynamo bytecode transform time: 15.86 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m INFO 09-17 17:33:22 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m INFO 09-17 17:33:22 [backends.py:541] Dynamo bytecode transform time: 15.94 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m INFO 09-17 17:33:22 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m INFO 09-17 17:33:22 [backends.py:541] Dynamo bytecode transform time: 16.32 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m INFO 09-17 17:33:31 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.299 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m INFO 09-17 17:33:31 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.409 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m INFO 09-17 17:33:31 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.462 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m INFO 09-17 17:33:32 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.386 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m INFO 09-17 17:33:37 [monitor.py:34] torch.compile takes 15.86 s in total\n",
      "INFO 09-17 17:33:37 [monitor.py:34] torch.compile takes 15.94 s in total\n",
      "INFO 09-17 17:33:37 [monitor.py:34] torch.compile takes 15.59 s in total\n",
      "INFO 09-17 17:33:37 [monitor.py:34] torch.compile takes 16.32 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m INFO 09-17 17:33:39 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m INFO 09-17 17:33:39 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m INFO 09-17 17:33:39 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m INFO 09-17 17:33:39 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:33:39 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:33:39 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:33:39 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:33:39 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:33:39 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:33:39 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:33:39 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:33:39 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m INFO 09-17 17:33:43 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m INFO 09-17 17:33:43 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m INFO 09-17 17:33:43 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m INFO 09-17 17:33:43 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:33:43 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.08 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_0_0.4/checkpoint-3000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<39:33,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=488202)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=488203)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=488204)\u001b[0;0m INFO 09-17 17:33:45 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:33:45 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:33:45 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=488205)\u001b[0;0m INFO 09-17 17:33:45 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2924.22it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 157.01it/s, est. speed input: 8214.00 toks/s, output: 419.79 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_0_0.99/checkpoint-50...\n",
      "INFO 09-17 17:34:20 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:34:20 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:34:20 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:34:20 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:34:20 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:34:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_34d0bf14'), local_subscribe_addr='ipc:///tmp/d46c76fd-9013-46dd-a374-3c127a017e89', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m INFO 09-17 17:34:23 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b99eab9c'), local_subscribe_addr='ipc:///tmp/fdd1af35-6796-4811-b8c8-2facf3d6aafb', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m INFO 09-17 17:34:23 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_29564eb6'), local_subscribe_addr='ipc:///tmp/04f6c758-2ddf-4745-bbe7-56ab5878fa1f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m INFO 09-17 17:34:23 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ce842801'), local_subscribe_addr='ipc:///tmp/5f87e873-ba27-4516-b418-3745e0e7de4f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m INFO 09-17 17:34:23 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_56d0b81a'), local_subscribe_addr='ipc:///tmp/7ba1310c-3644-4a1f-8aad-476fc516a93d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m INFO 09-17 17:34:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:34:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m INFO 09-17 17:34:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:34:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:34:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m INFO 09-17 17:34:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m INFO 09-17 17:34:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m INFO 09-17 17:34:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m WARNING 09-17 17:34:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:34:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m WARNING 09-17 17:34:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m WARNING 09-17 17:34:24 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m INFO 09-17 17:34:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_6f054d29'), local_subscribe_addr='ipc:///tmp/10ebc447-4372-4b27-a614-2425acb58934', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m INFO 09-17 17:34:24 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:34:24 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:34:24 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:34:24 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m WARNING 09-17 17:34:24 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m WARNING 09-17 17:34:24 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m WARNING 09-17 17:34:24 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:34:24 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m INFO 09-17 17:34:24 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:34:24 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m INFO 09-17 17:34:24 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:34:24 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m INFO 09-17 17:34:24 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m INFO 09-17 17:34:24 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m INFO 09-17 17:34:24 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m INFO 09-17 17:34:24 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m INFO 09-17 17:34:24 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m INFO 09-17 17:34:24 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m INFO 09-17 17:34:24 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m INFO 09-17 17:34:24 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m INFO 09-17 17:34:25 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m INFO 09-17 17:34:25 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 09-17 17:34:25 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m INFO 09-17 17:34:25 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:07,  1.59s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.58s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.52s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:05<00:02,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m INFO 09-17 17:34:32 [default_loader.py:262] Loading weights took 7.48 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m INFO 09-17 17:34:32 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m INFO 09-17 17:34:33 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 8.178412 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.32s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.40s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m INFO 09-17 17:34:34 [default_loader.py:262] Loading weights took 8.46 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m INFO 09-17 17:34:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m INFO 09-17 17:34:34 [default_loader.py:262] Loading weights took 8.54 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m INFO 09-17 17:34:34 [default_loader.py:262] Loading weights took 8.78 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m INFO 09-17 17:34:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m INFO 09-17 17:34:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m INFO 09-17 17:34:34 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.771759 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m INFO 09-17 17:34:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.958899 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m INFO 09-17 17:34:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.919035 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m INFO 09-17 17:34:51 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m INFO 09-17 17:34:51 [backends.py:541] Dynamo bytecode transform time: 15.49 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m INFO 09-17 17:34:51 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m INFO 09-17 17:34:51 [backends.py:541] Dynamo bytecode transform time: 15.54 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m INFO 09-17 17:34:51 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m INFO 09-17 17:34:51 [backends.py:541] Dynamo bytecode transform time: 15.70 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m INFO 09-17 17:34:51 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m INFO 09-17 17:34:51 [backends.py:541] Dynamo bytecode transform time: 15.74 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m INFO 09-17 17:35:01 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.795 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m INFO 09-17 17:35:01 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.727 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m INFO 09-17 17:35:01 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.943 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m INFO 09-17 17:35:01 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.140 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m INFO 09-17 17:35:07 [monitor.py:34] torch.compile takes 15.74 s in total\n",
      "INFO 09-17 17:35:07 [monitor.py:34] torch.compile takes 15.70 s in total\n",
      "INFO 09-17 17:35:07 [monitor.py:34] torch.compile takes 15.49 s in total\n",
      "INFO 09-17 17:35:07 [monitor.py:34] torch.compile takes 15.54 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m INFO 09-17 17:35:08 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m INFO 09-17 17:35:08 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m INFO 09-17 17:35:08 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m INFO 09-17 17:35:08 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:35:09 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:35:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:35:09 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:35:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:35:09 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:35:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:35:09 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:35:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m INFO 09-17 17:35:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m INFO 09-17 17:35:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m INFO 09-17 17:35:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m INFO 09-17 17:35:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:35:13 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.93 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_0_0.99/checkpoint-50 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<40:11,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=489053)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=489054)\u001b[0;0m INFO 09-17 17:35:14 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:35:14 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=489056)\u001b[0;0m INFO 09-17 17:35:14 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:35:14 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2844.13it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 149.66it/s, est. speed input: 7829.56 toks/s, output: 414.24 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_0_0.2/checkpoint-4000...\n",
      "INFO 09-17 17:35:51 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:35:51 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:35:52 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:35:52 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:35:52 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:35:52 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_1af7f106'), local_subscribe_addr='ipc:///tmp/a56cdb26-e05b-4b4a-9233-a6e93d89d508', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m INFO 09-17 17:35:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c917795f'), local_subscribe_addr='ipc:///tmp/7c926d02-1f9d-4b50-94f2-417767b198f5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m INFO 09-17 17:35:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_501cae85'), local_subscribe_addr='ipc:///tmp/819f63a9-9138-4e4b-8a42-66fdbf524bfb', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m INFO 09-17 17:35:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2fb7f93c'), local_subscribe_addr='ipc:///tmp/18f3d3b2-2d74-4689-a1b4-4b22b10eb994', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m INFO 09-17 17:35:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8f45be24'), local_subscribe_addr='ipc:///tmp/d909bfff-7089-4405-a070-2c555bd63429', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m INFO 09-17 17:35:55 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:35:55 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m INFO 09-17 17:35:55 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m INFO 09-17 17:35:55 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m INFO 09-17 17:35:55 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:35:55 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m INFO 09-17 17:35:55 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:35:55 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m WARNING 09-17 17:35:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:35:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m WARNING 09-17 17:35:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:35:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m INFO 09-17 17:35:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_4f3c7d6b'), local_subscribe_addr='ipc:///tmp/b2f70f2a-8a0e-43ab-b69b-6d8e776c8df8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m INFO 09-17 17:35:56 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m INFO 09-17 17:35:56 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:35:56 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:35:56 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m WARNING 09-17 17:35:56 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m WARNING 09-17 17:35:56 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m WARNING 09-17 17:35:56 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m WARNING 09-17 17:35:56 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m INFO 09-17 17:35:56 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m INFO 09-17 17:35:56 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:35:56 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m INFO 09-17 17:35:56 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m INFO 09-17 17:35:56 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m INFO 09-17 17:35:56 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m INFO 09-17 17:35:56 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m INFO 09-17 17:35:56 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m INFO 09-17 17:35:56 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m INFO 09-17 17:35:56 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m INFO 09-17 17:35:56 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m INFO 09-17 17:35:56 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m INFO 09-17 17:35:56 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m INFO 09-17 17:35:56 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m INFO 09-17 17:35:56 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m INFO 09-17 17:35:56 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:07,  1.58s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.60s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:05,  1.67s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.65s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.54s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.49s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.55s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m INFO 09-17 17:36:06 [default_loader.py:262] Loading weights took 9.37 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m INFO 09-17 17:36:06 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m INFO 09-17 17:36:06 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.908723 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m INFO 09-17 17:36:06 [default_loader.py:262] Loading weights took 9.81 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m INFO 09-17 17:36:06 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m INFO 09-17 17:36:06 [default_loader.py:262] Loading weights took 9.76 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m INFO 09-17 17:36:07 [default_loader.py:262] Loading weights took 10.02 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m INFO 09-17 17:36:07 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m INFO 09-17 17:36:07 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m INFO 09-17 17:36:07 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.550400 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m INFO 09-17 17:36:07 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.667082 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m INFO 09-17 17:36:07 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.548000 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m INFO 09-17 17:36:23 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m INFO 09-17 17:36:23 [backends.py:541] Dynamo bytecode transform time: 15.48 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m INFO 09-17 17:36:24 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m INFO 09-17 17:36:24 [backends.py:541] Dynamo bytecode transform time: 15.59 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m INFO 09-17 17:36:24 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m INFO 09-17 17:36:24 [backends.py:541] Dynamo bytecode transform time: 15.72 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m INFO 09-17 17:36:24 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m INFO 09-17 17:36:24 [backends.py:541] Dynamo bytecode transform time: 15.82 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m INFO 09-17 17:36:33 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.478 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m INFO 09-17 17:36:34 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.790 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m INFO 09-17 17:36:34 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.676 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m INFO 09-17 17:36:34 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.687 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m INFO 09-17 17:36:39 [monitor.py:34] torch.compile takes 15.48 s in total\n",
      "INFO 09-17 17:36:39 [monitor.py:34] torch.compile takes 15.59 s in total\n",
      "INFO 09-17 17:36:39 [monitor.py:34] torch.compile takes 15.82 s in total\n",
      "INFO 09-17 17:36:39 [monitor.py:34] torch.compile takes 15.72 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m INFO 09-17 17:36:41 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m INFO 09-17 17:36:41 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m INFO 09-17 17:36:41 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m INFO 09-17 17:36:41 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:36:41 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:36:41 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:36:41 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:36:41 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:36:41 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:36:41 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:36:41 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:36:41 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m INFO 09-17 17:36:45 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m INFO 09-17 17:36:45 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m INFO 09-17 17:36:45 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m INFO 09-17 17:36:45 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:36:45 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.67 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_0_0.2/checkpoint-4000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<38:58,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=489906)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=489909)\u001b[0;0m INFO 09-17 17:36:47 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:36:47 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=489910)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=489908)\u001b[0;0m INFO 09-17 17:36:47 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:36:47 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 2036.33it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:29<00:00, 169.17it/s, est. speed input: 8850.50 toks/s, output: 444.03 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.999/checkpoint-5...\n",
      "INFO 09-17 17:37:21 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:37:21 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:37:22 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:37:22 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:37:22 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:37:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_f8a0f0d6'), local_subscribe_addr='ipc:///tmp/5bad0599-c615-4c9e-9936-3f1cf85e46e5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m INFO 09-17 17:37:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7cd53c9f'), local_subscribe_addr='ipc:///tmp/6558c686-0781-41fd-9bf7-f0c8ed1f5166', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m INFO 09-17 17:37:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ae80f183'), local_subscribe_addr='ipc:///tmp/63ced8b1-af91-41b4-828c-fbdc29a5f565', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m INFO 09-17 17:37:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_58dfa3ad'), local_subscribe_addr='ipc:///tmp/b8e0bce1-95ad-4d65-bdc3-14da1590553b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m INFO 09-17 17:37:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_162a5045'), local_subscribe_addr='ipc:///tmp/5690380f-3f7b-40c9-87b5-8937b251c6e7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m INFO 09-17 17:37:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:37:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:37:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:37:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m INFO 09-17 17:37:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:37:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:37:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:37:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m WARNING 09-17 17:37:25 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m WARNING 09-17 17:37:25 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m WARNING 09-17 17:37:25 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:37:25 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m INFO 09-17 17:37:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_15899120'), local_subscribe_addr='ipc:///tmp/e08f7e8e-e453-478b-80f7-282f0a5b22e0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m INFO 09-17 17:37:25 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m INFO 09-17 17:37:25 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:37:25 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:37:25 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m WARNING 09-17 17:37:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:37:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:37:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m WARNING 09-17 17:37:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m INFO 09-17 17:37:25 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:37:25 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m INFO 09-17 17:37:25 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m INFO 09-17 17:37:25 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m INFO 09-17 17:37:25 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m INFO 09-17 17:37:25 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m INFO 09-17 17:37:26 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m INFO 09-17 17:37:26 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m INFO 09-17 17:37:26 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m INFO 09-17 17:37:26 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 09-17 17:37:26 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m INFO 09-17 17:37:26 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m INFO 09-17 17:37:26 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m INFO 09-17 17:37:26 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m INFO 09-17 17:37:26 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m INFO 09-17 17:37:26 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:07,  1.48s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:02<00:05,  1.48s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.56s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.58s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m INFO 09-17 17:37:35 [default_loader.py:262] Loading weights took 9.30 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m INFO 09-17 17:37:35 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.52s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.52s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m INFO 09-17 17:37:35 [default_loader.py:262] Loading weights took 9.20 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m INFO 09-17 17:37:35 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m INFO 09-17 17:37:36 [default_loader.py:262] Loading weights took 9.50 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m INFO 09-17 17:37:36 [default_loader.py:262] Loading weights took 9.40 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m INFO 09-17 17:37:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m INFO 09-17 17:37:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m INFO 09-17 17:37:36 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.833111 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m INFO 09-17 17:37:36 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.043186 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m INFO 09-17 17:37:36 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.042791 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m INFO 09-17 17:37:36 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.942124 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m INFO 09-17 17:37:53 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m INFO 09-17 17:37:53 [backends.py:541] Dynamo bytecode transform time: 15.60 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m INFO 09-17 17:37:53 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m INFO 09-17 17:37:53 [backends.py:541] Dynamo bytecode transform time: 15.63 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m INFO 09-17 17:37:53 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m INFO 09-17 17:37:53 [backends.py:541] Dynamo bytecode transform time: 15.72 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m INFO 09-17 17:37:53 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m INFO 09-17 17:37:53 [backends.py:541] Dynamo bytecode transform time: 15.75 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m INFO 09-17 17:38:02 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.438 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m INFO 09-17 17:38:02 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.339 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m INFO 09-17 17:38:03 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.477 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m INFO 09-17 17:38:03 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.418 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m INFO 09-17 17:38:08 [monitor.py:34] torch.compile takes 15.63 s in total\n",
      "INFO 09-17 17:38:08 [monitor.py:34] torch.compile takes 15.60 s in total\n",
      "INFO 09-17 17:38:08 [monitor.py:34] torch.compile takes 15.75 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m INFO 09-17 17:38:08 [monitor.py:34] torch.compile takes 15.72 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m INFO 09-17 17:38:09 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m INFO 09-17 17:38:09 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m INFO 09-17 17:38:09 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m INFO 09-17 17:38:10 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "INFO 09-17 17:38:10 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:38:10 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:38:10 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:38:10 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:38:10 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:38:10 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:38:10 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:38:10 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m INFO 09-17 17:38:15 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m INFO 09-17 17:38:15 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m INFO 09-17 17:38:15 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m INFO 09-17 17:38:15 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:38:15 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.32 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.999/checkpoint-5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<39:03,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=490757)\u001b[0;0m INFO 09-17 17:38:16 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=490758)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=490756)\u001b[0;0m INFO 09-17 17:38:16 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:38:16 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=490755)\u001b[0;0m INFO 09-17 17:38:16 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2894.27it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:34<00:00, 142.90it/s, est. speed input: 7476.07 toks/s, output: 420.66 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.4/checkpoint-3000...\n",
      "INFO 09-17 17:38:54 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:38:54 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:38:55 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:38:55 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:38:55 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:38:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_0707a0d9'), local_subscribe_addr='ipc:///tmp/2768b88d-b7fb-4ad2-bab9-f150903966e1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m INFO 09-17 17:38:57 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9f993e6a'), local_subscribe_addr='ipc:///tmp/b2be7576-b198-4d81-bd86-6c1a37275048', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m INFO 09-17 17:38:57 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_54dafc7a'), local_subscribe_addr='ipc:///tmp/b1d5dcd8-34f8-4c08-8f05-122ce77234ac', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m INFO 09-17 17:38:57 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1817b265'), local_subscribe_addr='ipc:///tmp/6c0fbc45-6b9e-486b-8256-99a26d42e0d3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m INFO 09-17 17:38:57 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_16798d03'), local_subscribe_addr='ipc:///tmp/8517857a-8f01-470b-ac33-f8925f79d158', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m INFO 09-17 17:38:58 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:38:58 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m INFO 09-17 17:38:58 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:38:58 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:38:58 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m INFO 09-17 17:38:58 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:38:58 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m INFO 09-17 17:38:58 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m WARNING 09-17 17:38:59 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:38:59 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m WARNING 09-17 17:38:59 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:38:59 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m INFO 09-17 17:38:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_0f69ccde'), local_subscribe_addr='ipc:///tmp/0904d231-f6a0-469b-8d49-3a352304f829', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m INFO 09-17 17:38:59 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m INFO 09-17 17:38:59 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:38:59 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:38:59 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m WARNING 09-17 17:38:59 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:38:59 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:38:59 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m WARNING 09-17 17:38:59 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m INFO 09-17 17:38:59 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:38:59 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m INFO 09-17 17:38:59 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:38:59 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m INFO 09-17 17:38:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m INFO 09-17 17:38:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m INFO 09-17 17:38:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m INFO 09-17 17:38:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m INFO 09-17 17:38:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m INFO 09-17 17:38:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m INFO 09-17 17:38:59 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m INFO 09-17 17:38:59 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m INFO 09-17 17:38:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m INFO 09-17 17:38:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m INFO 09-17 17:39:00 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m INFO 09-17 17:39:00 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.70s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.69s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.69s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.66s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m INFO 09-17 17:39:08 [default_loader.py:262] Loading weights took 9.09 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m INFO 09-17 17:39:08 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m INFO 09-17 17:39:09 [default_loader.py:262] Loading weights took 9.53 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m INFO 09-17 17:39:09 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m INFO 09-17 17:39:09 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.625788 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.49s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.57s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m INFO 09-17 17:39:09 [default_loader.py:262] Loading weights took 9.46 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m INFO 09-17 17:39:09 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m INFO 09-17 17:39:09 [default_loader.py:262] Loading weights took 9.64 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m INFO 09-17 17:39:09 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m INFO 09-17 17:39:10 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.160257 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m INFO 09-17 17:39:10 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.246011 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m INFO 09-17 17:39:10 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.149399 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m INFO 09-17 17:39:26 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m INFO 09-17 17:39:26 [backends.py:541] Dynamo bytecode transform time: 15.83 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m INFO 09-17 17:39:26 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m INFO 09-17 17:39:26 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m INFO 09-17 17:39:26 [backends.py:541] Dynamo bytecode transform time: 15.87 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m INFO 09-17 17:39:26 [backends.py:541] Dynamo bytecode transform time: 15.83 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m INFO 09-17 17:39:27 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m INFO 09-17 17:39:27 [backends.py:541] Dynamo bytecode transform time: 16.00 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m INFO 09-17 17:39:36 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.314 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m INFO 09-17 17:39:36 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.350 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m INFO 09-17 17:39:36 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.282 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m INFO 09-17 17:39:36 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.596 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m INFO 09-17 17:39:42 [monitor.py:34] torch.compile takes 15.83 s in total\n",
      "INFO 09-17 17:39:42 [monitor.py:34] torch.compile takes 16.00 s in total\n",
      "INFO 09-17 17:39:42 [monitor.py:34] torch.compile takes 15.83 s in total\n",
      "INFO 09-17 17:39:42 [monitor.py:34] torch.compile takes 15.87 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m INFO 09-17 17:39:43 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m INFO 09-17 17:39:43 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m INFO 09-17 17:39:43 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m INFO 09-17 17:39:44 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:39:45 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:39:45 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:39:45 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:39:45 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:39:45 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:39:45 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:39:45 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:39:45 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m INFO 09-17 17:39:49 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m INFO 09-17 17:39:49 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m INFO 09-17 17:39:49 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m INFO 09-17 17:39:49 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:39:49 [core.py:193] init engine (profile, create kv cache, warmup model) took 39.15 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.4/checkpoint-3000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<45:44,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=491601)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=491600)\u001b[0;0m INFO 09-17 17:39:51 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:39:51 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=491602)\u001b[0;0m INFO 09-17 17:39:51 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=491603)\u001b[0;0m INFO 09-17 17:39:51 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 2418.85it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 165.82it/s, est. speed input: 8674.79 toks/s, output: 432.56 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.99/checkpoint-50...\n",
      "INFO 09-17 17:40:24 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:40:24 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:40:25 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:40:25 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:40:25 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:40:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_b0453bfb'), local_subscribe_addr='ipc:///tmp/3bd3cf20-8940-411c-9d8d-5646fe152385', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m INFO 09-17 17:40:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_59ecc79f'), local_subscribe_addr='ipc:///tmp/7789d9c7-e3f8-4f6a-b7bb-936558f69152', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m INFO 09-17 17:40:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_49d4c071'), local_subscribe_addr='ipc:///tmp/cc56d3be-1f62-441e-a184-9804c8cd16ef', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m INFO 09-17 17:40:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b7496593'), local_subscribe_addr='ipc:///tmp/8dd02db5-bf23-4d6f-9304-64c9c218411e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m INFO 09-17 17:40:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_aa29b3d8'), local_subscribe_addr='ipc:///tmp/c9c328b3-20ee-43f9-8ca0-bc78e6b9d001', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m INFO 09-17 17:40:29 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:40:29 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:40:29 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m INFO 09-17 17:40:29 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m INFO 09-17 17:40:29 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:40:29 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:40:29 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m INFO 09-17 17:40:29 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m WARNING 09-17 17:40:29 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:40:29 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m WARNING 09-17 17:40:29 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:40:29 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m INFO 09-17 17:40:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_2ba02058'), local_subscribe_addr='ipc:///tmp/d9d0d80c-0c37-44b6-aea5-3c6951826d13', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m INFO 09-17 17:40:29 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:40:29 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m INFO 09-17 17:40:29 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:40:29 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m WARNING 09-17 17:40:29 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:40:29 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m WARNING 09-17 17:40:29 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:40:29 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m INFO 09-17 17:40:29 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:40:29 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m INFO 09-17 17:40:29 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m INFO 09-17 17:40:29 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m INFO 09-17 17:40:30 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m INFO 09-17 17:40:30 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m INFO 09-17 17:40:30 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m INFO 09-17 17:40:30 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m INFO 09-17 17:40:30 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m INFO 09-17 17:40:30 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m INFO 09-17 17:40:30 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m INFO 09-17 17:40:30 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m INFO 09-17 17:40:30 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m INFO 09-17 17:40:30 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m INFO 09-17 17:40:30 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m INFO 09-17 17:40:30 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:11,  2.23s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:09,  2.28s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:06<00:06,  2.26s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:08<00:04,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m INFO 09-17 17:40:41 [default_loader.py:262] Loading weights took 10.38 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:10<00:01,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m INFO 09-17 17:40:41 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m INFO 09-17 17:40:41 [default_loader.py:262] Loading weights took 10.95 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m INFO 09-17 17:40:41 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m INFO 09-17 17:40:41 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.952406 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.70s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.92s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m INFO 09-17 17:40:42 [default_loader.py:262] Loading weights took 11.60 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m INFO 09-17 17:40:42 [default_loader.py:262] Loading weights took 11.48 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m INFO 09-17 17:40:42 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.566412 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m INFO 09-17 17:40:42 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m INFO 09-17 17:40:42 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m INFO 09-17 17:40:43 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.287194 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m INFO 09-17 17:40:43 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.224124 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m INFO 09-17 17:40:59 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m INFO 09-17 17:40:59 [backends.py:541] Dynamo bytecode transform time: 15.74 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m INFO 09-17 17:40:59 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m INFO 09-17 17:40:59 [backends.py:541] Dynamo bytecode transform time: 15.85 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m INFO 09-17 17:40:59 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m INFO 09-17 17:40:59 [backends.py:541] Dynamo bytecode transform time: 15.99 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m INFO 09-17 17:40:59 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m INFO 09-17 17:40:59 [backends.py:541] Dynamo bytecode transform time: 15.97 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m INFO 09-17 17:41:09 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.691 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m INFO 09-17 17:41:09 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.372 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m INFO 09-17 17:41:09 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.507 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m INFO 09-17 17:41:09 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.580 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m INFO 09-17 17:41:14 [monitor.py:34] torch.compile takes 15.99 s in total\n",
      "INFO 09-17 17:41:14 [monitor.py:34] torch.compile takes 15.74 s in total\n",
      "INFO 09-17 17:41:14 [monitor.py:34] torch.compile takes 15.97 s in total\n",
      "INFO 09-17 17:41:14 [monitor.py:34] torch.compile takes 15.85 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m INFO 09-17 17:41:16 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m INFO 09-17 17:41:16 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m INFO 09-17 17:41:16 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m INFO 09-17 17:41:16 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:41:16 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:41:16 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:41:16 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:41:16 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:41:16 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:41:16 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:41:16 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:41:16 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m INFO 09-17 17:41:20 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m INFO 09-17 17:41:20 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 17:41:20 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m INFO 09-17 17:41:20 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:41:20 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.42 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.99/checkpoint-50 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<42:12,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=492442)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=492439)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=492443)\u001b[0;0m INFO 09-17 17:41:22 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=492440)\u001b[0;0m INFO 09-17 17:41:22 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:41:22 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:41:22 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2824.77it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 150.96it/s, est. speed input: 7897.52 toks/s, output: 433.65 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.1/checkpoint-4500...\n",
      "INFO 09-17 17:41:58 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:41:58 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:41:59 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:41:59 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:41:59 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:41:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_22c2649c'), local_subscribe_addr='ipc:///tmp/3189babc-5e0f-45e2-82c3-6cc57ef71550', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m INFO 09-17 17:42:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c8ba89d5'), local_subscribe_addr='ipc:///tmp/4507043a-dea2-46e3-ac62-e1c217c4e76f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m INFO 09-17 17:42:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bf249a05'), local_subscribe_addr='ipc:///tmp/a9d1ac29-2854-4ecb-8a65-e5dff55e65f9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m INFO 09-17 17:42:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7066a592'), local_subscribe_addr='ipc:///tmp/b75033f7-375e-46ae-8b1a-df316dc6b399', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m INFO 09-17 17:42:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_618834a8'), local_subscribe_addr='ipc:///tmp/8c113579-94be-4328-a946-05300fc1b51b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m INFO 09-17 17:42:03 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:42:03 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:42:03 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m INFO 09-17 17:42:03 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:42:03 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:42:03 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:42:03 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m INFO 09-17 17:42:03 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m WARNING 09-17 17:42:03 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:42:03 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m WARNING 09-17 17:42:03 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m WARNING 09-17 17:42:03 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m INFO 09-17 17:42:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_7ed426c1'), local_subscribe_addr='ipc:///tmp/a845942d-d308-4a4e-ba12-c9b48ad6eed9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m INFO 09-17 17:42:03 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m INFO 09-17 17:42:03 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m INFO 09-17 17:42:03 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:42:03 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m WARNING 09-17 17:42:03 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m WARNING 09-17 17:42:03 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m WARNING 09-17 17:42:03 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m INFO 09-17 17:42:03 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "WARNING 09-17 17:42:03 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m INFO 09-17 17:42:03 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:42:03 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m INFO 09-17 17:42:03 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m INFO 09-17 17:42:03 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m INFO 09-17 17:42:03 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m INFO 09-17 17:42:03 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m INFO 09-17 17:42:03 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m INFO 09-17 17:42:04 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m INFO 09-17 17:42:04 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m INFO 09-17 17:42:04 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m INFO 09-17 17:42:04 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m INFO 09-17 17:42:04 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m INFO 09-17 17:42:04 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m INFO 09-17 17:42:04 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m INFO 09-17 17:42:04 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:07,  1.44s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.52s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.51s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:05<00:02,  1.48s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.38s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.35s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.41s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m INFO 09-17 17:42:12 [default_loader.py:262] Loading weights took 8.50 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m INFO 09-17 17:42:12 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m INFO 09-17 17:42:13 [default_loader.py:262] Loading weights took 9.03 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m INFO 09-17 17:42:13 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.067563 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m INFO 09-17 17:42:13 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m INFO 09-17 17:42:13 [default_loader.py:262] Loading weights took 9.01 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m INFO 09-17 17:42:13 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m INFO 09-17 17:42:13 [default_loader.py:262] Loading weights took 8.99 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m INFO 09-17 17:42:13 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m INFO 09-17 17:42:14 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.758387 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m INFO 09-17 17:42:14 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.760853 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m INFO 09-17 17:42:14 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.852044 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m INFO 09-17 17:42:30 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m INFO 09-17 17:42:30 [backends.py:541] Dynamo bytecode transform time: 15.72 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m INFO 09-17 17:42:30 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m INFO 09-17 17:42:30 [backends.py:541] Dynamo bytecode transform time: 15.82 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m INFO 09-17 17:42:31 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m INFO 09-17 17:42:31 [backends.py:541] Dynamo bytecode transform time: 15.93 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m INFO 09-17 17:42:31 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m INFO 09-17 17:42:31 [backends.py:541] Dynamo bytecode transform time: 16.45 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m INFO 09-17 17:42:40 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.341 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m INFO 09-17 17:42:41 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.833 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m INFO 09-17 17:42:41 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.504 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m INFO 09-17 17:42:41 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.670 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m INFO 09-17 17:42:47 [monitor.py:34] torch.compile takes 16.45 s in total\n",
      "INFO 09-17 17:42:47 [monitor.py:34] torch.compile takes 15.93 s in total\n",
      "INFO 09-17 17:42:47 [monitor.py:34] torch.compile takes 15.72 s in total\n",
      "INFO 09-17 17:42:47 [monitor.py:34] torch.compile takes 15.82 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m INFO 09-17 17:42:48 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m INFO 09-17 17:42:48 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m INFO 09-17 17:42:48 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m INFO 09-17 17:42:48 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:42:49 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:42:49 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:42:49 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:42:49 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:42:49 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:42:49 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:42:49 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:42:49 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m INFO 09-17 17:42:53 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m INFO 09-17 17:42:53 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m INFO 09-17 17:42:53 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:42:53 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:42:53 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.91 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.1/checkpoint-4500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<43:58,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=493291)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=493292)\u001b[0;0m INFO 09-17 17:42:55 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=493289)\u001b[0;0m INFO 09-17 17:42:55 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:42:55 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=493290)\u001b[0;0m INFO 09-17 17:42:55 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2693.43it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 148.01it/s, est. speed input: 7743.33 toks/s, output: 405.32 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_0_0.5/checkpoint-2500...\n",
      "INFO 09-17 17:43:32 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:43:32 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:43:33 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:43:33 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:43:33 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:43:33 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_32853342'), local_subscribe_addr='ipc:///tmp/a538fa29-0d72-401e-8598-e48ac998ef47', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m INFO 09-17 17:43:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7407cab7'), local_subscribe_addr='ipc:///tmp/f366d91c-feb9-462e-ba9d-783bbed09e4c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m INFO 09-17 17:43:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fd845866'), local_subscribe_addr='ipc:///tmp/37b6c25b-65c0-4c86-9b53-7c0119cebeca', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m INFO 09-17 17:43:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e1fbde39'), local_subscribe_addr='ipc:///tmp/5b0c592b-afa0-4e2c-8c5c-1abd28289ba7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m INFO 09-17 17:43:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fbbf3e67'), local_subscribe_addr='ipc:///tmp/9031b0bf-00f2-4af0-bd6f-3f6018b96219', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m INFO 09-17 17:43:36 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m INFO 09-17 17:43:36 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m INFO 09-17 17:43:36 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m INFO 09-17 17:43:36 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m INFO 09-17 17:43:36 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:43:36 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:43:36 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m INFO 09-17 17:43:36 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m WARNING 09-17 17:43:36 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:43:36 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m WARNING 09-17 17:43:36 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:43:36 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m INFO 09-17 17:43:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_f2e3a506'), local_subscribe_addr='ipc:///tmp/fa44e5b0-bebc-4c2a-801e-5dc24f8c2866', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m INFO 09-17 17:43:36 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m INFO 09-17 17:43:36 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:43:36 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:43:36 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m WARNING 09-17 17:43:36 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m WARNING 09-17 17:43:36 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:43:36 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m WARNING 09-17 17:43:36 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m INFO 09-17 17:43:36 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m INFO 09-17 17:43:36 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m INFO 09-17 17:43:36 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m INFO 09-17 17:43:36 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m INFO 09-17 17:43:37 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m INFO 09-17 17:43:37 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m INFO 09-17 17:43:37 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m INFO 09-17 17:43:37 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m INFO 09-17 17:43:37 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m INFO 09-17 17:43:37 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m INFO 09-17 17:43:37 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m INFO 09-17 17:43:37 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m INFO 09-17 17:43:37 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m INFO 09-17 17:43:37 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m INFO 09-17 17:43:37 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m INFO 09-17 17:43:37 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.61s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.57s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.56s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.54s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.45s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.43s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.48s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m INFO 09-17 17:43:46 [default_loader.py:262] Loading weights took 8.94 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m INFO 09-17 17:43:46 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m INFO 09-17 17:43:47 [default_loader.py:262] Loading weights took 9.18 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m INFO 09-17 17:43:47 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 09-17 17:43:47 [default_loader.py:262] Loading weights took 9.33 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m INFO 09-17 17:43:47 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m INFO 09-17 17:43:47 [default_loader.py:262] Loading weights took 9.14 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m INFO 09-17 17:43:47 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m INFO 09-17 17:43:47 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.522322 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m INFO 09-17 17:43:47 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.982972 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m INFO 09-17 17:43:47 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.927118 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m INFO 09-17 17:43:47 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.832508 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m INFO 09-17 17:44:03 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m INFO 09-17 17:44:03 [backends.py:541] Dynamo bytecode transform time: 15.39 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m INFO 09-17 17:44:04 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m INFO 09-17 17:44:04 [backends.py:541] Dynamo bytecode transform time: 15.73 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m INFO 09-17 17:44:04 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m INFO 09-17 17:44:04 [backends.py:541] Dynamo bytecode transform time: 15.75 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m INFO 09-17 17:44:04 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m INFO 09-17 17:44:04 [backends.py:541] Dynamo bytecode transform time: 16.25 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m INFO 09-17 17:44:13 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.332 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m INFO 09-17 17:44:14 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.448 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m INFO 09-17 17:44:14 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.436 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m INFO 09-17 17:44:15 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.527 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m INFO 09-17 17:44:20 [monitor.py:34] torch.compile takes 15.73 s in total\n",
      "INFO 09-17 17:44:20 [monitor.py:34] torch.compile takes 15.75 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m INFO 09-17 17:44:20 [monitor.py:34] torch.compile takes 16.25 s in total\n",
      "INFO 09-17 17:44:20 [monitor.py:34] torch.compile takes 15.39 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m INFO 09-17 17:44:21 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m INFO 09-17 17:44:21 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m INFO 09-17 17:44:21 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m INFO 09-17 17:44:22 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:44:22 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:44:22 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:44:22 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:44:22 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:44:22 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:44:22 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:44:22 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:44:22 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m INFO 09-17 17:44:26 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m INFO 09-17 17:44:26 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m INFO 09-17 17:44:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:44:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:44:27 [core.py:193] init engine (profile, create kv cache, warmup model) took 39.15 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_0_0.5/checkpoint-2500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<41:16,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=494155)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=494158)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=494156)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=494157)\u001b[0;0m INFO 09-17 17:44:28 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:44:28 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:44:28 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:44:28 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2776.17it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 156.88it/s, est. speed input: 8207.52 toks/s, output: 420.84 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.2/checkpoint-4000...\n",
      "INFO 09-17 17:45:03 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:45:03 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:45:04 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:45:04 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:45:04 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:45:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_83777b94'), local_subscribe_addr='ipc:///tmp/59f89112-67df-47eb-a0d4-2542f3f76dee', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m INFO 09-17 17:45:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_27de3e25'), local_subscribe_addr='ipc:///tmp/84bf6529-bca2-4c6a-9b39-b600e920e0de', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-17 17:45:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bda109e8'), local_subscribe_addr='ipc:///tmp/5fa3a0d2-af94-4090-b82c-57c1b20e5b19', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m INFO 09-17 17:45:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bb82090d'), local_subscribe_addr='ipc:///tmp/40b7e311-8504-415a-9fbb-dd3ddfcf9a71', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m INFO 09-17 17:45:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_15bd66f0'), local_subscribe_addr='ipc:///tmp/6e8260ef-3742-4c3c-911f-51079cb2593a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m INFO 09-17 17:45:07 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:45:07 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:45:07 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:45:07 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m INFO 09-17 17:45:07 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:45:07 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:45:07 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:45:07 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m WARNING 09-17 17:45:08 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:45:08 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m WARNING 09-17 17:45:08 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m WARNING 09-17 17:45:08 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m INFO 09-17 17:45:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_5236becb'), local_subscribe_addr='ipc:///tmp/ccc5faa3-6ffc-47de-a426-c58d8a61b1cc', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m INFO 09-17 17:45:08 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:45:08 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:45:08 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:45:08 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m WARNING 09-17 17:45:08 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m WARNING 09-17 17:45:08 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:45:08 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m WARNING 09-17 17:45:08 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 17:45:08 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m INFO 09-17 17:45:08 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m INFO 09-17 17:45:08 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:45:08 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m INFO 09-17 17:45:08 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m INFO 09-17 17:45:08 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m INFO 09-17 17:45:08 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m INFO 09-17 17:45:08 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m INFO 09-17 17:45:08 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m INFO 09-17 17:45:08 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m INFO 09-17 17:45:08 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m INFO 09-17 17:45:08 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m INFO 09-17 17:45:08 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m INFO 09-17 17:45:09 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m INFO 09-17 17:45:09 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m INFO 09-17 17:45:09 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:07,  1.51s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.67s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:05,  1.67s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.58s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m INFO 09-17 17:45:18 [default_loader.py:262] Loading weights took 9.63 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m INFO 09-17 17:45:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.65s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.63s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m INFO 09-17 17:45:19 [default_loader.py:262] Loading weights took 9.89 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m INFO 09-17 17:45:19 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m INFO 09-17 17:45:19 [default_loader.py:262] Loading weights took 9.83 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m INFO 09-17 17:45:19 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m INFO 09-17 17:45:19 [default_loader.py:262] Loading weights took 9.69 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m INFO 09-17 17:45:19 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m INFO 09-17 17:45:19 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.176417 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m INFO 09-17 17:45:19 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.457764 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m INFO 09-17 17:45:20 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.469224 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m INFO 09-17 17:45:20 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.554258 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m INFO 09-17 17:45:36 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m INFO 09-17 17:45:36 [backends.py:541] Dynamo bytecode transform time: 15.53 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m INFO 09-17 17:45:36 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m INFO 09-17 17:45:36 [backends.py:541] Dynamo bytecode transform time: 15.66 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m INFO 09-17 17:45:36 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m INFO 09-17 17:45:36 [backends.py:541] Dynamo bytecode transform time: 15.72 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m INFO 09-17 17:45:36 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m INFO 09-17 17:45:36 [backends.py:541] Dynamo bytecode transform time: 15.80 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m INFO 09-17 17:45:45 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.324 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m INFO 09-17 17:45:46 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.520 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m INFO 09-17 17:45:46 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.478 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m INFO 09-17 17:45:46 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.505 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m INFO 09-17 17:45:51 [monitor.py:34] torch.compile takes 15.72 s in total\n",
      "INFO 09-17 17:45:51 [monitor.py:34] torch.compile takes 15.66 s in total\n",
      "INFO 09-17 17:45:51 [monitor.py:34] torch.compile takes 15.53 s in total\n",
      "INFO 09-17 17:45:51 [monitor.py:34] torch.compile takes 15.80 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m INFO 09-17 17:45:53 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m INFO 09-17 17:45:53 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m INFO 09-17 17:45:53 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m INFO 09-17 17:45:53 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:45:53 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:45:53 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:45:53 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:45:53 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:45:53 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:45:53 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:45:53 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:45:53 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m INFO 09-17 17:45:58 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m INFO 09-17 17:45:58 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m INFO 09-17 17:45:58 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m INFO 09-17 17:45:58 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:45:58 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.11 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.2/checkpoint-4000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<44:31,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=495002)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=495003)\u001b[0;0m INFO 09-17 17:45:59 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:45:59 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495001)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=495004)\u001b[0;0m INFO 09-17 17:45:59 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:45:59 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2745.09it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 151.49it/s, est. speed input: 7925.16 toks/s, output: 415.66 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank1]:[W917 17:46:34.953607574 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=206, addr=[::ffff:127.0.0.1]:56300, remote=[::ffff:127.0.0.1]:43325): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7225261785e8 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8bfe (0x7224fe97fbfe in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baaf40 (0x7224fe981f40 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab84a (0x7224fe98284a in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7224fe97c2a9 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7224c007b9f9 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdc253 (0x722538df9253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7225396feac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #8: clone + 0x44 (0x72253978fbf4 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[W917 17:46:34.241530995 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 1] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_1_0.999/checkpoint-5...\n",
      "INFO 09-17 17:46:36 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:46:36 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:46:36 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:46:36 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:46:36 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:46:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_fa038b1c'), local_subscribe_addr='ipc:///tmp/d9fab3be-01dc-446a-b184-f96b6c1d142b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m INFO 09-17 17:46:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4467cc63'), local_subscribe_addr='ipc:///tmp/b7cc6b33-0200-478e-aef2-6087af75efaa', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m INFO 09-17 17:46:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_57426b04'), local_subscribe_addr='ipc:///tmp/b77f5860-84b6-48e9-85ec-7762bf58851a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m INFO 09-17 17:46:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b61d63db'), local_subscribe_addr='ipc:///tmp/eb618276-d8d1-4468-9fd3-52f373037e70', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m INFO 09-17 17:46:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9effac73'), local_subscribe_addr='ipc:///tmp/092d6c70-6fff-47df-969e-0ae9318af2b5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m INFO 09-17 17:46:40 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:46:40 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:46:40 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m INFO 09-17 17:46:40 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:46:40 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:46:40 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:46:40 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m INFO 09-17 17:46:40 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m WARNING 09-17 17:46:40 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m WARNING 09-17 17:46:40 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m WARNING 09-17 17:46:40 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:46:40 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m INFO 09-17 17:46:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_2a36e086'), local_subscribe_addr='ipc:///tmp/b6fa2232-82cc-4abd-9935-145bc3fb5ef1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m INFO 09-17 17:46:40 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:46:40 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m INFO 09-17 17:46:40 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m INFO 09-17 17:46:40 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m WARNING 09-17 17:46:40 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:46:40 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m WARNING 09-17 17:46:40 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m INFO 09-17 17:46:40 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:46:40 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m WARNING 09-17 17:46:40 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 17:46:40 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m INFO 09-17 17:46:40 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m INFO 09-17 17:46:41 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m INFO 09-17 17:46:41 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m INFO 09-17 17:46:41 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m INFO 09-17 17:46:41 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m INFO 09-17 17:46:41 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m INFO 09-17 17:46:41 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m INFO 09-17 17:46:41 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m INFO 09-17 17:46:41 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m INFO 09-17 17:46:41 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m INFO 09-17 17:46:41 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m INFO 09-17 17:46:41 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m INFO 09-17 17:46:41 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.70s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:07,  1.82s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.91s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:03,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m INFO 09-17 17:46:49 [default_loader.py:262] Loading weights took 8.34 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m INFO 09-17 17:46:49 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m INFO 09-17 17:46:50 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 8.915821 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.46s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.61s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m INFO 09-17 17:46:51 [default_loader.py:262] Loading weights took 9.70 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m INFO 09-17 17:46:51 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m INFO 09-17 17:46:52 [default_loader.py:262] Loading weights took 9.89 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m INFO 09-17 17:46:52 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m INFO 09-17 17:46:52 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.390765 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m INFO 09-17 17:46:52 [default_loader.py:262] Loading weights took 10.39 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m INFO 09-17 17:46:52 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m INFO 09-17 17:46:52 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.809157 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m INFO 09-17 17:46:53 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.092555 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m INFO 09-17 17:47:09 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m INFO 09-17 17:47:09 [backends.py:541] Dynamo bytecode transform time: 15.58 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m INFO 09-17 17:47:09 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m INFO 09-17 17:47:09 [backends.py:541] Dynamo bytecode transform time: 15.88 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m INFO 09-17 17:47:09 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m INFO 09-17 17:47:09 [backends.py:541] Dynamo bytecode transform time: 16.09 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m INFO 09-17 17:47:09 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m INFO 09-17 17:47:09 [backends.py:541] Dynamo bytecode transform time: 16.11 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m INFO 09-17 17:47:19 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.452 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m INFO 09-17 17:47:19 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.646 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m INFO 09-17 17:47:20 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.672 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m INFO 09-17 17:47:20 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.710 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m INFO 09-17 17:47:24 [monitor.py:34] torch.compile takes 15.58 s in total\n",
      "INFO 09-17 17:47:24 [monitor.py:34] torch.compile takes 15.88 s in total\n",
      "INFO 09-17 17:47:24 [monitor.py:34] torch.compile takes 16.11 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m INFO 09-17 17:47:24 [monitor.py:34] torch.compile takes 16.09 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m INFO 09-17 17:47:26 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m INFO 09-17 17:47:26 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m INFO 09-17 17:47:26 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m INFO 09-17 17:47:26 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:47:26 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:47:26 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:47:26 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:47:26 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:47:26 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:47:26 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:47:26 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:47:26 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m INFO 09-17 17:47:30 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m INFO 09-17 17:47:30 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m INFO 09-17 17:47:30 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m INFO 09-17 17:47:30 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:47:31 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.73 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_1_0.999/checkpoint-5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<47:53,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=495854)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=495851)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=495852)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=495853)\u001b[0;0m INFO 09-17 17:47:32 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:47:32 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:47:32 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:47:32 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2707.45it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 150.44it/s, est. speed input: 7870.61 toks/s, output: 432.54 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank2]:[W917 17:48:07.782374162 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=209, addr=[::ffff:127.0.0.1]:49028, remote=[::ffff:127.0.0.1]:60541): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7225261785e8 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8bfe (0x7224fe97fbfe in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baaf40 (0x7224fe981f40 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab84a (0x7224fe98284a in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7224fe97c2a9 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7224c007b9f9 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdc253 (0x722538df9253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7225396feac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #8: clone + 0x44 (0x72253978fbf4 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[rank2]:[W917 17:48:07.802761408 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 2] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_0_0.999/checkpoint-5...\n",
      "INFO 09-17 17:48:08 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:48:08 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:48:09 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:48:09 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:48:09 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:48:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_674724fc'), local_subscribe_addr='ipc:///tmp/646f62cb-f363-4cf3-929a-5a843f557fbd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m INFO 09-17 17:48:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5d30888a'), local_subscribe_addr='ipc:///tmp/07cb0e30-bf2c-42eb-81ba-d203f200d992', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m INFO 09-17 17:48:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9d82bad0'), local_subscribe_addr='ipc:///tmp/f9a268e0-1bf6-461f-b47e-ac95d3ed8a2e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m INFO 09-17 17:48:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d29b63ed'), local_subscribe_addr='ipc:///tmp/a9fbda03-dfe0-454e-9079-c6e433932db4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m INFO 09-17 17:48:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d0c423b5'), local_subscribe_addr='ipc:///tmp/55c69e79-cb88-4040-8fe2-e23a3595d1cc', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m INFO 09-17 17:48:12 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m INFO 09-17 17:48:12 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:48:12 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:48:12 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m INFO 09-17 17:48:12 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:48:12 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:48:12 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m INFO 09-17 17:48:12 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m WARNING 09-17 17:48:13 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:48:13 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m WARNING 09-17 17:48:13 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:48:13 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m INFO 09-17 17:48:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_6354ee8f'), local_subscribe_addr='ipc:///tmp/79269482-6cba-4861-a00b-ae90503ded8b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m INFO 09-17 17:48:13 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m INFO 09-17 17:48:13 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:48:13 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:48:13 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m WARNING 09-17 17:48:13 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m WARNING 09-17 17:48:13 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m WARNING 09-17 17:48:13 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m INFO 09-17 17:48:13 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m INFO 09-17 17:48:13 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m WARNING 09-17 17:48:13 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 17:48:13 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m INFO 09-17 17:48:13 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m INFO 09-17 17:48:13 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m INFO 09-17 17:48:13 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m INFO 09-17 17:48:14 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m INFO 09-17 17:48:14 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m INFO 09-17 17:48:14 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m INFO 09-17 17:48:14 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m INFO 09-17 17:48:14 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m INFO 09-17 17:48:14 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m INFO 09-17 17:48:14 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m INFO 09-17 17:48:14 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m INFO 09-17 17:48:14 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m INFO 09-17 17:48:14 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.73s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:07,  1.78s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.78s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.73s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.64s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.62s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.67s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m INFO 09-17 17:48:24 [default_loader.py:262] Loading weights took 10.09 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m INFO 09-17 17:48:24 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m INFO 09-17 17:48:24 [default_loader.py:262] Loading weights took 10.11 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m INFO 09-17 17:48:24 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m INFO 09-17 17:48:25 [default_loader.py:262] Loading weights took 10.25 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m INFO 09-17 17:48:25 [default_loader.py:262] Loading weights took 10.35 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m INFO 09-17 17:48:25 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m INFO 09-17 17:48:25 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m INFO 09-17 17:48:25 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.836800 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m INFO 09-17 17:48:25 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.714132 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m INFO 09-17 17:48:25 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.355815 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m INFO 09-17 17:48:25 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.196281 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m INFO 09-17 17:48:41 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m INFO 09-17 17:48:41 [backends.py:541] Dynamo bytecode transform time: 15.35 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m INFO 09-17 17:48:42 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m INFO 09-17 17:48:42 [backends.py:541] Dynamo bytecode transform time: 15.58 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m INFO 09-17 17:48:42 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m INFO 09-17 17:48:42 [backends.py:541] Dynamo bytecode transform time: 15.72 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m INFO 09-17 17:48:42 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m INFO 09-17 17:48:42 [backends.py:541] Dynamo bytecode transform time: 15.82 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m INFO 09-17 17:48:51 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.246 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m INFO 09-17 17:48:51 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.233 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m INFO 09-17 17:48:51 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.355 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m INFO 09-17 17:48:52 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.421 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m INFO 09-17 17:48:57 [monitor.py:34] torch.compile takes 15.58 s in total\n",
      "INFO 09-17 17:48:57 [monitor.py:34] torch.compile takes 15.35 s in total\n",
      "INFO 09-17 17:48:57 [monitor.py:34] torch.compile takes 15.72 s in total\n",
      "INFO 09-17 17:48:57 [monitor.py:34] torch.compile takes 15.82 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m INFO 09-17 17:48:58 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m INFO 09-17 17:48:58 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m INFO 09-17 17:48:58 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m INFO 09-17 17:48:58 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:48:59 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:48:59 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:48:59 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:48:59 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:48:59 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:48:59 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:48:59 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:48:59 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m INFO 09-17 17:49:03 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m INFO 09-17 17:49:03 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m INFO 09-17 17:49:03 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m INFO 09-17 17:49:03 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:49:03 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.36 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_0_0.999/checkpoint-5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<41:48,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=496751)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=496749)\u001b[0;0m INFO 09-17 17:49:04 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:49:04 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=496750)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=496752)\u001b[0;0m INFO 09-17 17:49:04 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:49:04 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2837.23it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 154.86it/s, est. speed input: 8101.47 toks/s, output: 452.25 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.01/checkpoint-4950...\n",
      "INFO 09-17 17:49:40 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:49:40 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:49:41 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:49:41 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:49:41 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:49:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_2cd12fe8'), local_subscribe_addr='ipc:///tmp/6af8efb2-c9bd-45b3-8f3a-13e8a3bdcc00', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m INFO 09-17 17:49:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f245eb2c'), local_subscribe_addr='ipc:///tmp/cfaca901-d842-463a-9c0c-3eb7c37d6889', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m INFO 09-17 17:49:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1995d563'), local_subscribe_addr='ipc:///tmp/0782e74a-285c-4e8c-8f6f-2a6a0d6c048d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m INFO 09-17 17:49:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a728eed9'), local_subscribe_addr='ipc:///tmp/d7b12bff-5bed-4db3-ac50-cbe0a6d56a5d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m INFO 09-17 17:49:44 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a80b5d5b'), local_subscribe_addr='ipc:///tmp/ab94b60b-427d-4009-beb5-f5cb5cffca9d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m INFO 09-17 17:49:44 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:49:44 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:49:44 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:49:44 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m INFO 09-17 17:49:44 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m INFO 09-17 17:49:44 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:49:44 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:49:44 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m WARNING 09-17 17:49:45 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:49:45 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m WARNING 09-17 17:49:45 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:49:45 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m INFO 09-17 17:49:45 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_5e68a50b'), local_subscribe_addr='ipc:///tmp/3af36bb6-9284-425b-839b-557ca2dfbc2f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m INFO 09-17 17:49:45 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m INFO 09-17 17:49:45 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:49:45 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:49:45 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m WARNING 09-17 17:49:45 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m WARNING 09-17 17:49:45 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m WARNING 09-17 17:49:45 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m WARNING 09-17 17:49:45 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 17:49:45 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m INFO 09-17 17:49:45 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:49:45 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:49:45 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m INFO 09-17 17:49:45 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m INFO 09-17 17:49:45 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m INFO 09-17 17:49:45 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m INFO 09-17 17:49:45 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m INFO 09-17 17:49:45 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m INFO 09-17 17:49:45 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m INFO 09-17 17:49:45 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m INFO 09-17 17:49:46 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m INFO 09-17 17:49:46 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m INFO 09-17 17:49:46 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m INFO 09-17 17:49:46 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m INFO 09-17 17:49:46 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:07,  1.59s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.57s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.59s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.56s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.46s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.42s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.48s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m INFO 09-17 17:49:55 [default_loader.py:262] Loading weights took 8.94 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m INFO 09-17 17:49:55 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m INFO 09-17 17:49:55 [default_loader.py:262] Loading weights took 9.21 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m INFO 09-17 17:49:55 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m INFO 09-17 17:49:55 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.504225 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m INFO 09-17 17:49:55 [default_loader.py:262] Loading weights took 9.22 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m INFO 09-17 17:49:55 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m INFO 09-17 17:49:55 [default_loader.py:262] Loading weights took 9.55 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m INFO 09-17 17:49:55 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m INFO 09-17 17:49:56 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.871863 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m INFO 09-17 17:49:56 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.040503 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m INFO 09-17 17:49:56 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.286731 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m INFO 09-17 17:50:12 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m INFO 09-17 17:50:12 [backends.py:541] Dynamo bytecode transform time: 15.46 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m INFO 09-17 17:50:13 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m INFO 09-17 17:50:13 [backends.py:541] Dynamo bytecode transform time: 15.71 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m INFO 09-17 17:50:13 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m INFO 09-17 17:50:13 [backends.py:541] Dynamo bytecode transform time: 15.88 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m INFO 09-17 17:50:13 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m INFO 09-17 17:50:13 [backends.py:541] Dynamo bytecode transform time: 16.13 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m INFO 09-17 17:50:22 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.245 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m INFO 09-17 17:50:22 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.230 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m INFO 09-17 17:50:23 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.457 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m INFO 09-17 17:50:23 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.532 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m INFO 09-17 17:50:28 [monitor.py:34] torch.compile takes 15.71 s in total\n",
      "INFO 09-17 17:50:28 [monitor.py:34] torch.compile takes 16.13 s in total\n",
      "INFO 09-17 17:50:28 [monitor.py:34] torch.compile takes 15.88 s in total\n",
      "INFO 09-17 17:50:28 [monitor.py:34] torch.compile takes 15.46 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m INFO 09-17 17:50:30 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m INFO 09-17 17:50:30 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m INFO 09-17 17:50:30 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m INFO 09-17 17:50:30 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:50:30 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:50:30 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:50:30 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:50:30 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:50:30 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:50:30 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:50:30 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:50:30 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m INFO 09-17 17:50:34 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:50:34 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m INFO 09-17 17:50:34 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m INFO 09-17 17:50:34 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:50:34 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.78 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.01/checkpoint-4950 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<37:44,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=497597)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=497594)\u001b[0;0m INFO 09-17 17:50:37 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:50:37 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=497595)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=497596)\u001b[0;0m INFO 09-17 17:50:37 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:50:37 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 2109.35it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 161.97it/s, est. speed input: 8473.76 toks/s, output: 432.32 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_2_0.5/checkpoint-2500...\n",
      "INFO 09-17 17:51:12 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:51:12 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:51:12 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:51:12 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:51:12 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:51:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_2e2dd137'), local_subscribe_addr='ipc:///tmp/d429c13e-bfac-4658-8fe6-ec6bf002cac9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m INFO 09-17 17:51:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_56f1ec95'), local_subscribe_addr='ipc:///tmp/e9bb86e4-272a-4f7e-b1c8-c0a3672cc9e1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m INFO 09-17 17:51:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a7b9fe04'), local_subscribe_addr='ipc:///tmp/49369051-7bf7-4ef6-a645-497fb964bea3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m INFO 09-17 17:51:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_24c7e560'), local_subscribe_addr='ipc:///tmp/520104c4-62f8-40a0-b680-18b06a7641ca', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m INFO 09-17 17:51:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c149b94c'), local_subscribe_addr='ipc:///tmp/7b045755-e980-429e-90ad-4d6a036f70f9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m INFO 09-17 17:51:15 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:51:15 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:51:15 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:51:15 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m INFO 09-17 17:51:15 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:51:15 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:51:15 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:51:15 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m WARNING 09-17 17:51:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:51:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m WARNING 09-17 17:51:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:51:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m INFO 09-17 17:51:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_c2eb9642'), local_subscribe_addr='ipc:///tmp/75df9447-e7a4-499f-b3e5-09c08ff5147f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m INFO 09-17 17:51:16 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:51:16 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:51:16 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m INFO 09-17 17:51:16 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m WARNING 09-17 17:51:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:51:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m WARNING 09-17 17:51:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m INFO 09-17 17:51:16 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:51:16 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m WARNING 09-17 17:51:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m INFO 09-17 17:51:16 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m INFO 09-17 17:51:16 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m INFO 09-17 17:51:16 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m INFO 09-17 17:51:16 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m INFO 09-17 17:51:16 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m INFO 09-17 17:51:16 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m INFO 09-17 17:51:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m INFO 09-17 17:51:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 09-17 17:51:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m INFO 09-17 17:51:17 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m INFO 09-17 17:51:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m INFO 09-17 17:51:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m INFO 09-17 17:51:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m INFO 09-17 17:51:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:11,  2.36s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:09,  2.34s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:06<00:06,  2.24s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:08<00:04,  2.10s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:10<00:01,  1.91s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.78s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.97s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m INFO 09-17 17:51:29 [default_loader.py:262] Loading weights took 11.86 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m INFO 09-17 17:51:29 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m INFO 09-17 17:51:29 [default_loader.py:262] Loading weights took 11.88 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m INFO 09-17 17:51:29 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m INFO 09-17 17:51:30 [default_loader.py:262] Loading weights took 12.33 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m INFO 09-17 17:51:30 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m INFO 09-17 17:51:30 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.535727 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m INFO 09-17 17:51:30 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.696176 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m INFO 09-17 17:51:30 [default_loader.py:262] Loading weights took 12.85 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m INFO 09-17 17:51:30 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m INFO 09-17 17:51:31 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.313816 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m INFO 09-17 17:51:31 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.589526 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m INFO 09-17 17:51:47 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m INFO 09-17 17:51:47 [backends.py:541] Dynamo bytecode transform time: 15.65 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m INFO 09-17 17:51:47 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m INFO 09-17 17:51:47 [backends.py:541] Dynamo bytecode transform time: 15.70 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m INFO 09-17 17:51:48 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m INFO 09-17 17:51:48 [backends.py:541] Dynamo bytecode transform time: 16.14 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m INFO 09-17 17:51:48 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m INFO 09-17 17:51:48 [backends.py:541] Dynamo bytecode transform time: 15.91 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m INFO 09-17 17:51:57 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.425 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m INFO 09-17 17:51:57 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.359 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m INFO 09-17 17:51:57 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.325 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m INFO 09-17 17:51:57 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.430 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m INFO 09-17 17:52:02 [monitor.py:34] torch.compile takes 15.91 s in total\n",
      "INFO 09-17 17:52:02 [monitor.py:34] torch.compile takes 15.70 s in total\n",
      "INFO 09-17 17:52:02 [monitor.py:34] torch.compile takes 15.65 s in total\n",
      "INFO 09-17 17:52:02 [monitor.py:34] torch.compile takes 16.14 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m INFO 09-17 17:52:04 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m INFO 09-17 17:52:04 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m INFO 09-17 17:52:04 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m INFO 09-17 17:52:04 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:52:05 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:52:05 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:52:05 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:52:05 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:52:05 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:52:05 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:52:05 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:52:05 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m INFO 09-17 17:52:08 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m INFO 09-17 17:52:08 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m INFO 09-17 17:52:08 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m INFO 09-17 17:52:08 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:52:09 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.71 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_2_0.5/checkpoint-2500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<41:52,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=498444)\u001b[0;0m INFO 09-17 17:52:10 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=498441)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=498442)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=498445)\u001b[0;0m INFO 09-17 17:52:10 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:52:10 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:52:10 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2764.17it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:29<00:00, 169.86it/s, est. speed input: 8886.31 toks/s, output: 454.99 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_0_0.6/checkpoint-2000...\n",
      "INFO 09-17 17:52:43 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:52:43 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:52:43 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:52:43 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:52:43 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:52:44 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_9817bb5d'), local_subscribe_addr='ipc:///tmp/dc4dcb0b-9edb-4599-9ea7-e1e5f67bda6d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m INFO 09-17 17:52:45 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d5dd386e'), local_subscribe_addr='ipc:///tmp/6a093bfb-41d8-4193-8fcb-b2684f2f0814', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-17 17:52:45 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_903629c3'), local_subscribe_addr='ipc:///tmp/cd8d0037-d86e-4ce0-be26-ce1bbf76f22c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m INFO 09-17 17:52:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1d353e00'), local_subscribe_addr='ipc:///tmp/bccb2b07-572c-4244-ade6-e20d1c461370', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m INFO 09-17 17:52:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_83c0ee38'), local_subscribe_addr='ipc:///tmp/3bce8414-5dd9-41a4-bd62-311e1946b612', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m INFO 09-17 17:52:47 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m INFO 09-17 17:52:47 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:52:47 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:52:47 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m INFO 09-17 17:52:47 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m INFO 09-17 17:52:47 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:52:47 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:52:47 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m WARNING 09-17 17:52:48 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m WARNING 09-17 17:52:48 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m WARNING 09-17 17:52:48 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:52:48 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m INFO 09-17 17:52:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_be7a947c'), local_subscribe_addr='ipc:///tmp/d91481f1-f764-44d8-849a-fd33968c8493', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m INFO 09-17 17:52:48 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:52:48 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:52:48 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:52:48 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m WARNING 09-17 17:52:48 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m WARNING 09-17 17:52:48 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:52:48 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:52:48 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m INFO 09-17 17:52:48 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m INFO 09-17 17:52:48 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:52:48 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m INFO 09-17 17:52:48 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m INFO 09-17 17:52:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m INFO 09-17 17:52:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m INFO 09-17 17:52:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m INFO 09-17 17:52:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m INFO 09-17 17:52:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m INFO 09-17 17:52:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m INFO 09-17 17:52:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m INFO 09-17 17:52:49 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m INFO 09-17 17:52:49 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m INFO 09-17 17:52:49 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m INFO 09-17 17:52:49 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m INFO 09-17 17:52:49 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:12,  2.52s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:09,  2.48s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:07,  2.46s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:09<00:04,  2.43s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:11<00:02,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m INFO 09-17 17:53:02 [default_loader.py:262] Loading weights took 13.07 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m INFO 09-17 17:53:02 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m INFO 09-17 17:53:02 [default_loader.py:262] Loading weights took 13.59 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m INFO 09-17 17:53:02 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m INFO 09-17 17:53:02 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.645111 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m INFO 09-17 17:53:03 [default_loader.py:262] Loading weights took 13.63 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m INFO 09-17 17:53:03 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:13<00:00,  2.16s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:13<00:00,  2.28s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m INFO 09-17 17:53:03 [default_loader.py:262] Loading weights took 13.77 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m INFO 09-17 17:53:03 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m INFO 09-17 17:53:03 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.254925 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m INFO 09-17 17:53:03 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.437403 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m INFO 09-17 17:53:04 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.495687 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m INFO 09-17 17:53:20 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m INFO 09-17 17:53:20 [backends.py:541] Dynamo bytecode transform time: 15.77 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m INFO 09-17 17:53:20 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m INFO 09-17 17:53:20 [backends.py:541] Dynamo bytecode transform time: 15.96 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m INFO 09-17 17:53:20 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m INFO 09-17 17:53:20 [backends.py:541] Dynamo bytecode transform time: 15.96 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m INFO 09-17 17:53:20 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m INFO 09-17 17:53:20 [backends.py:541] Dynamo bytecode transform time: 16.08 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m INFO 09-17 17:53:30 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.418 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m INFO 09-17 17:53:30 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.306 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m INFO 09-17 17:53:30 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.387 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m INFO 09-17 17:53:30 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.467 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m INFO 09-17 17:53:35 [monitor.py:34] torch.compile takes 15.96 s in total\n",
      "INFO 09-17 17:53:35 [monitor.py:34] torch.compile takes 16.08 s in total\n",
      "INFO 09-17 17:53:35 [monitor.py:34] torch.compile takes 15.77 s in total\n",
      "INFO 09-17 17:53:35 [monitor.py:34] torch.compile takes 15.96 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m INFO 09-17 17:53:37 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m INFO 09-17 17:53:37 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m INFO 09-17 17:53:37 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m INFO 09-17 17:53:37 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:53:37 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:53:37 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:53:37 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:53:37 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:53:37 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:53:37 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:53:37 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:53:37 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m INFO 09-17 17:53:41 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:53:41 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m INFO 09-17 17:53:41 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m INFO 09-17 17:53:41 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:53:41 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.43 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_0_0.6/checkpoint-2000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<44:45,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=499296)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=499294)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=499295)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=499293)\u001b[0;0m INFO 09-17 17:53:43 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:53:43 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:53:43 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:53:43 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2776.26it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 157.96it/s, est. speed input: 8263.82 toks/s, output: 416.61 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_0_0.9/checkpoint-500...\n",
      "INFO 09-17 17:54:17 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:54:17 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:54:18 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:54:18 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:54:18 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:54:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_a4058ef6'), local_subscribe_addr='ipc:///tmp/2dacca10-2b1b-46af-a362-c344b35495cd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m INFO 09-17 17:54:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_13a3c962'), local_subscribe_addr='ipc:///tmp/13ab3737-625d-4d9f-bcdc-6a29ed1ccf74', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m INFO 09-17 17:54:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_76bd1bc9'), local_subscribe_addr='ipc:///tmp/f843ac77-43b9-42e0-9c46-a18903565cab', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m INFO 09-17 17:54:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_079f4fdb'), local_subscribe_addr='ipc:///tmp/2d640ac4-4907-4277-91aa-f920d7329ec1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m INFO 09-17 17:54:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3b451214'), local_subscribe_addr='ipc:///tmp/dc74ccba-4eb2-44eb-ba66-aaf002979061', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m INFO 09-17 17:54:22 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:54:22 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:54:22 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m INFO 09-17 17:54:22 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:54:22 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m INFO 09-17 17:54:22 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:54:22 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m INFO 09-17 17:54:22 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m WARNING 09-17 17:54:22 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:54:22 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m WARNING 09-17 17:54:22 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m WARNING 09-17 17:54:22 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m INFO 09-17 17:54:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_4573a32c'), local_subscribe_addr='ipc:///tmp/6f3dfea6-4adf-494d-ab76-007ea4a07758', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m INFO 09-17 17:54:22 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:54:22 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:54:22 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:54:22 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m WARNING 09-17 17:54:22 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:54:22 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:54:22 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m WARNING 09-17 17:54:22 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m INFO 09-17 17:54:22 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:54:22 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m INFO 09-17 17:54:22 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:54:22 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m INFO 09-17 17:54:23 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m INFO 09-17 17:54:23 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m INFO 09-17 17:54:23 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m INFO 09-17 17:54:23 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m INFO 09-17 17:54:23 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m INFO 09-17 17:54:23 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m INFO 09-17 17:54:23 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m INFO 09-17 17:54:23 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m INFO 09-17 17:54:23 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m INFO 09-17 17:54:23 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m INFO 09-17 17:54:23 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m INFO 09-17 17:54:23 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:11,  2.25s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:08,  2.17s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:06<00:06,  2.09s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:08<00:03,  1.97s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:09<00:01,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m INFO 09-17 17:54:34 [default_loader.py:262] Loading weights took 10.57 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m INFO 09-17 17:54:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.57s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.79s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m INFO 09-17 17:54:34 [default_loader.py:262] Loading weights took 10.80 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m INFO 09-17 17:54:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m INFO 09-17 17:54:34 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.097016 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m INFO 09-17 17:54:35 [default_loader.py:262] Loading weights took 11.26 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m INFO 09-17 17:54:35 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m INFO 09-17 17:54:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.390922 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m INFO 09-17 17:54:35 [default_loader.py:262] Loading weights took 11.64 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m INFO 09-17 17:54:35 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m INFO 09-17 17:54:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.926634 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m INFO 09-17 17:54:36 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.325856 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m INFO 09-17 17:54:52 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m INFO 09-17 17:54:52 [backends.py:541] Dynamo bytecode transform time: 15.39 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m INFO 09-17 17:54:52 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m INFO 09-17 17:54:52 [backends.py:541] Dynamo bytecode transform time: 15.92 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m INFO 09-17 17:54:52 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m INFO 09-17 17:54:52 [backends.py:541] Dynamo bytecode transform time: 16.01 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m INFO 09-17 17:54:52 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m INFO 09-17 17:54:52 [backends.py:541] Dynamo bytecode transform time: 16.01 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m INFO 09-17 17:55:02 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.691 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m INFO 09-17 17:55:02 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.351 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m INFO 09-17 17:55:02 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.595 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m INFO 09-17 17:55:02 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.518 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m INFO 09-17 17:55:07 [monitor.py:34] torch.compile takes 15.92 s in total\n",
      "INFO 09-17 17:55:07 [monitor.py:34] torch.compile takes 15.39 s in total\n",
      "INFO 09-17 17:55:07 [monitor.py:34] torch.compile takes 16.01 s in total\n",
      "INFO 09-17 17:55:07 [monitor.py:34] torch.compile takes 16.01 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m INFO 09-17 17:55:09 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m INFO 09-17 17:55:09 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m INFO 09-17 17:55:09 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m INFO 09-17 17:55:09 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:55:09 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:55:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:55:09 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:55:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:55:09 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:55:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:55:09 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:55:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m INFO 09-17 17:55:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m INFO 09-17 17:55:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m INFO 09-17 17:55:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m INFO 09-17 17:55:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:55:13 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.74 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_0_0.9/checkpoint-500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<38:45,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=500154)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=500151)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=500152)\u001b[0;0m INFO 09-17 17:55:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:55:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:55:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=500153)\u001b[0;0m INFO 09-17 17:55:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2911.60it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:37<00:00, 132.70it/s, est. speed input: 6942.16 toks/s, output: 375.99 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_1_0.001/checkpoint-4995...\n",
      "INFO 09-17 17:55:56 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:55:56 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:55:57 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:55:57 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:55:57 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:55:57 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_104b9a66'), local_subscribe_addr='ipc:///tmp/70c82378-527e-4a33-b585-87e9ae8f2073', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m INFO 09-17 17:55:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0effdf6d'), local_subscribe_addr='ipc:///tmp/1dfe9384-9aeb-49b3-b56f-77fec0c1c57d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m INFO 09-17 17:55:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1f693656'), local_subscribe_addr='ipc:///tmp/132fbfb5-cf4c-4b79-8d0a-46919fc345ac', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m INFO 09-17 17:56:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e24594a8'), local_subscribe_addr='ipc:///tmp/3f8ab1e4-e7e3-432b-9e5a-bea67982930c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m INFO 09-17 17:56:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3a2bd851'), local_subscribe_addr='ipc:///tmp/4320b250-42f7-413e-9f26-8e55bd2522e7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m INFO 09-17 17:56:00 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m INFO 09-17 17:56:00 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m INFO 09-17 17:56:00 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:56:00 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:56:00 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m INFO 09-17 17:56:00 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:56:00 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:56:00 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m WARNING 09-17 17:56:01 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m WARNING 09-17 17:56:01 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m WARNING 09-17 17:56:01 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m WARNING 09-17 17:56:01 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m INFO 09-17 17:56:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_3ba41367'), local_subscribe_addr='ipc:///tmp/ce87390d-b7b2-4fd0-b5f6-bcf90838a142', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m INFO 09-17 17:56:01 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:56:01 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m INFO 09-17 17:56:01 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m INFO 09-17 17:56:01 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m WARNING 09-17 17:56:01 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m WARNING 09-17 17:56:01 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:56:01 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m INFO 09-17 17:56:01 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m INFO 09-17 17:56:01 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m INFO 09-17 17:56:01 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "WARNING 09-17 17:56:01 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m INFO 09-17 17:56:01 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m INFO 09-17 17:56:01 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m INFO 09-17 17:56:01 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m INFO 09-17 17:56:01 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m INFO 09-17 17:56:01 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m INFO 09-17 17:56:01 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m INFO 09-17 17:56:01 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m INFO 09-17 17:56:01 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m INFO 09-17 17:56:01 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m INFO 09-17 17:56:02 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m INFO 09-17 17:56:02 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m INFO 09-17 17:56:02 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m INFO 09-17 17:56:02 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:09,  1.86s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:08,  2.06s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:06<00:06,  2.05s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:03,  1.93s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:09<00:01,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m INFO 09-17 17:56:12 [default_loader.py:262] Loading weights took 9.82 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m INFO 09-17 17:56:12 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m INFO 09-17 17:56:12 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.441934 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.67s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.79s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m INFO 09-17 17:56:13 [default_loader.py:262] Loading weights took 10.82 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m INFO 09-17 17:56:13 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m INFO 09-17 17:56:13 [default_loader.py:262] Loading weights took 10.78 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m INFO 09-17 17:56:13 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m INFO 09-17 17:56:13 [default_loader.py:262] Loading weights took 11.43 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m INFO 09-17 17:56:13 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m INFO 09-17 17:56:14 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.725710 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m INFO 09-17 17:56:14 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.639080 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m INFO 09-17 17:56:14 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.120942 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m INFO 09-17 17:56:30 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m INFO 09-17 17:56:30 [backends.py:541] Dynamo bytecode transform time: 15.65 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m INFO 09-17 17:56:30 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m INFO 09-17 17:56:30 [backends.py:541] Dynamo bytecode transform time: 15.74 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m INFO 09-17 17:56:31 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m INFO 09-17 17:56:31 [backends.py:541] Dynamo bytecode transform time: 15.85 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m INFO 09-17 17:56:31 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m INFO 09-17 17:56:31 [backends.py:541] Dynamo bytecode transform time: 15.85 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m INFO 09-17 17:56:40 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.659 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m INFO 09-17 17:56:41 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.828 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m INFO 09-17 17:56:41 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.693 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m INFO 09-17 17:56:41 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.732 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m INFO 09-17 17:56:46 [monitor.py:34] torch.compile takes 15.74 s in total\n",
      "INFO 09-17 17:56:46 [monitor.py:34] torch.compile takes 15.85 s in total\n",
      "INFO 09-17 17:56:46 [monitor.py:34] torch.compile takes 15.65 s in total\n",
      "INFO 09-17 17:56:46 [monitor.py:34] torch.compile takes 15.85 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m INFO 09-17 17:56:48 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m INFO 09-17 17:56:48 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m INFO 09-17 17:56:48 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m INFO 09-17 17:56:48 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:56:48 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:56:48 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:56:48 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:56:48 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:56:48 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:56:48 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:56:48 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:56:48 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m INFO 09-17 17:56:52 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m INFO 09-17 17:56:52 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m INFO 09-17 17:56:52 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m INFO 09-17 17:56:52 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:56:52 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.24 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_1_0.001/checkpoint-4995 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<41:42,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=501020)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=501021)\u001b[0;0m INFO 09-17 17:56:54 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501023)\u001b[0;0m INFO 09-17 17:56:54 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:56:54 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501022)\u001b[0;0m INFO 09-17 17:56:54 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2559.38it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 166.30it/s, est. speed input: 8699.88 toks/s, output: 436.90 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_1_0.8/checkpoint-1000...\n",
      "INFO 09-17 17:57:27 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:57:27 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:57:28 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:57:28 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:57:28 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:57:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_c9a5c90c'), local_subscribe_addr='ipc:///tmp/ff927007-bfa5-4cc9-8356-baf1475543d3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m INFO 09-17 17:57:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b6ebb121'), local_subscribe_addr='ipc:///tmp/c173e5e5-f14d-47bf-89ff-d1de7337f7e9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m INFO 09-17 17:57:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_90ed3f84'), local_subscribe_addr='ipc:///tmp/1f75c477-34aa-4626-be8a-b4c716d14730', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m INFO 09-17 17:57:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e7938948'), local_subscribe_addr='ipc:///tmp/f289742c-efde-4de6-9571-2dc966c08fcd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m INFO 09-17 17:57:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b5227520'), local_subscribe_addr='ipc:///tmp/7d44f671-b289-4bcb-a270-35f219e7345a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m INFO 09-17 17:57:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m INFO 09-17 17:57:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:57:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:57:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m INFO 09-17 17:57:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m INFO 09-17 17:57:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:57:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:57:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m WARNING 09-17 17:57:32 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m WARNING 09-17 17:57:32 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m WARNING 09-17 17:57:32 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:57:32 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m INFO 09-17 17:57:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_27357d6a'), local_subscribe_addr='ipc:///tmp/d297ea4a-1f73-4e7b-85cf-9d8a6cba3ad5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m INFO 09-17 17:57:32 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:57:32 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 17:57:32 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:57:32 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m WARNING 09-17 17:57:32 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:57:32 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m WARNING 09-17 17:57:32 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m WARNING 09-17 17:57:32 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m INFO 09-17 17:57:32 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m INFO 09-17 17:57:32 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:57:32 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:57:32 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m INFO 09-17 17:57:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m INFO 09-17 17:57:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m INFO 09-17 17:57:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m INFO 09-17 17:57:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m INFO 09-17 17:57:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m INFO 09-17 17:57:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m INFO 09-17 17:57:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m INFO 09-17 17:57:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m INFO 09-17 17:57:33 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m INFO 09-17 17:57:33 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m INFO 09-17 17:57:33 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m INFO 09-17 17:57:33 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:09,  1.82s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:07,  1.78s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.78s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:03,  1.74s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m INFO 09-17 17:57:42 [default_loader.py:262] Loading weights took 9.09 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m INFO 09-17 17:57:42 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m INFO 09-17 17:57:43 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.637410 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m INFO 09-17 17:57:43 [default_loader.py:262] Loading weights took 9.91 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m INFO 09-17 17:57:43 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.52s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.63s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m INFO 09-17 17:57:43 [default_loader.py:262] Loading weights took 9.91 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m INFO 09-17 17:57:43 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m INFO 09-17 17:57:43 [default_loader.py:262] Loading weights took 9.82 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m INFO 09-17 17:57:43 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m INFO 09-17 17:57:44 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.498812 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m INFO 09-17 17:57:44 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.715538 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m INFO 09-17 17:57:44 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.625240 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m INFO 09-17 17:58:01 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m INFO 09-17 17:58:01 [backends.py:541] Dynamo bytecode transform time: 15.81 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m INFO 09-17 17:58:01 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m INFO 09-17 17:58:01 [backends.py:541] Dynamo bytecode transform time: 15.94 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m INFO 09-17 17:58:01 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m INFO 09-17 17:58:01 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "INFO 09-17 17:58:01 [backends.py:541] Dynamo bytecode transform time: 16.02 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m INFO 09-17 17:58:01 [backends.py:541] Dynamo bytecode transform time: 16.06 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m INFO 09-17 17:58:10 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.419 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m INFO 09-17 17:58:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.385 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m INFO 09-17 17:58:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.626 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m INFO 09-17 17:58:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.408 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m INFO 09-17 17:58:16 [monitor.py:34] torch.compile takes 16.02 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m INFO 09-17 17:58:16 [monitor.py:34] torch.compile takes 15.81 s in total\n",
      "INFO 09-17 17:58:16 [monitor.py:34] torch.compile takes 16.06 s in total\n",
      "INFO 09-17 17:58:16 [monitor.py:34] torch.compile takes 15.94 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m INFO 09-17 17:58:18 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m INFO 09-17 17:58:18 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m INFO 09-17 17:58:18 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m INFO 09-17 17:58:18 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:58:19 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:58:19 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:58:19 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:58:19 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:58:19 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:58:19 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:58:19 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:58:19 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m INFO 09-17 17:58:23 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m INFO 09-17 17:58:23 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m INFO 09-17 17:58:23 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m INFO 09-17 17:58:23 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:58:23 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.81 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_1_0.8/checkpoint-1000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<40:18,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=501875)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=501876)\u001b[0;0m INFO 09-17 17:58:25 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:58:25 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=501877)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=501878)\u001b[0;0m INFO 09-17 17:58:25 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:58:25 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 1896.22it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:29<00:00, 166.76it/s, est. speed input: 8724.27 toks/s, output: 442.96 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_2_0.9/checkpoint-500...\n",
      "INFO 09-17 17:58:59 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 17:58:59 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 17:59:00 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 17:59:00 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 17:59:00 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:59:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_3e58a710'), local_subscribe_addr='ipc:///tmp/e07be615-5af2-47d0-b746-eb158f87caea', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m INFO 09-17 17:59:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_830e7799'), local_subscribe_addr='ipc:///tmp/17a47c9f-d709-4c27-a1b9-602596ce5f58', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m INFO 09-17 17:59:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f5e28917'), local_subscribe_addr='ipc:///tmp/31d5a00a-32f8-47a3-b111-38075612921c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m INFO 09-17 17:59:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_27b6f480'), local_subscribe_addr='ipc:///tmp/3f107ebd-178c-4f5c-8905-8eae9acbd9c8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m INFO 09-17 17:59:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e4ddae6f'), local_subscribe_addr='ipc:///tmp/7c1d827d-432b-4058-b78a-dfe6608045ff', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m INFO 09-17 17:59:03 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m INFO 09-17 17:59:03 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m INFO 09-17 17:59:03 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:59:03 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m INFO 09-17 17:59:03 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m INFO 09-17 17:59:03 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m INFO 09-17 17:59:03 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 17:59:03 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m WARNING 09-17 17:59:04 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m WARNING 09-17 17:59:04 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m WARNING 09-17 17:59:04 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 17:59:04 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m INFO 09-17 17:59:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_65b83769'), local_subscribe_addr='ipc:///tmp/b6973fd8-58b2-4979-b364-8dbe9275e01f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m INFO 09-17 17:59:04 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 17:59:04 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 17:59:04 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 17:59:04 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m WARNING 09-17 17:59:04 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:59:04 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:59:04 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 17:59:04 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m INFO 09-17 17:59:04 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:59:04 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 17:59:04 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m INFO 09-17 17:59:04 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m INFO 09-17 17:59:04 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m INFO 09-17 17:59:04 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m INFO 09-17 17:59:04 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m INFO 09-17 17:59:04 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m INFO 09-17 17:59:04 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m INFO 09-17 17:59:04 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m INFO 09-17 17:59:04 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m INFO 09-17 17:59:04 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m INFO 09-17 17:59:04 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m INFO 09-17 17:59:04 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m INFO 09-17 17:59:04 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m INFO 09-17 17:59:05 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:07,  1.54s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.69s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.65s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.64s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m INFO 09-17 17:59:14 [default_loader.py:262] Loading weights took 9.24 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m INFO 09-17 17:59:14 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m INFO 09-17 17:59:14 [default_loader.py:262] Loading weights took 9.48 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m INFO 09-17 17:59:14 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.52s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.58s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m INFO 09-17 17:59:14 [default_loader.py:262] Loading weights took 9.51 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m INFO 09-17 17:59:14 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m INFO 09-17 17:59:14 [default_loader.py:262] Loading weights took 9.44 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m INFO 09-17 17:59:14 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m INFO 09-17 17:59:15 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.814485 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m INFO 09-17 17:59:15 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.257519 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m INFO 09-17 17:59:15 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.347495 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m INFO 09-17 17:59:15 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.181344 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m INFO 09-17 17:59:32 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m INFO 09-17 17:59:32 [backends.py:541] Dynamo bytecode transform time: 15.82 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m INFO 09-17 17:59:32 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m INFO 09-17 17:59:32 [backends.py:541] Dynamo bytecode transform time: 16.07 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m INFO 09-17 17:59:32 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m INFO 09-17 17:59:32 [backends.py:541] Dynamo bytecode transform time: 16.18 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m INFO 09-17 17:59:32 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m INFO 09-17 17:59:32 [backends.py:541] Dynamo bytecode transform time: 16.08 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m INFO 09-17 17:59:42 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.552 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m INFO 09-17 17:59:42 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.499 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m INFO 09-17 17:59:42 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.519 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m INFO 09-17 17:59:42 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.592 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m INFO 09-17 17:59:47 [monitor.py:34] torch.compile takes 16.07 s in total\n",
      "INFO 09-17 17:59:47 [monitor.py:34] torch.compile takes 16.18 s in total\n",
      "INFO 09-17 17:59:47 [monitor.py:34] torch.compile takes 15.82 s in total\n",
      "INFO 09-17 17:59:47 [monitor.py:34] torch.compile takes 16.08 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m INFO 09-17 17:59:49 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m INFO 09-17 17:59:49 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m INFO 09-17 17:59:49 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m INFO 09-17 17:59:49 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 17:59:50 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 17:59:50 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 17:59:50 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:59:50 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:59:50 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:59:50 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 17:59:50 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 17:59:50 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m INFO 09-17 17:59:54 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m INFO 09-17 17:59:54 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:59:54 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m INFO 09-17 17:59:54 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 17:59:54 [core.py:193] init engine (profile, create kv cache, warmup model) took 39.00 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_2_0.9/checkpoint-500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<41:14,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=502733)\u001b[0;0m INFO 09-17 17:59:56 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=502732)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=502730)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=502729)\u001b[0;0m INFO 09-17 17:59:56 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:59:56 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 17:59:56 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2707.86it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 163.06it/s, est. speed input: 8530.40 toks/s, output: 439.64 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_2_0.8/checkpoint-1000...\n",
      "INFO 09-17 18:00:30 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:00:30 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:00:30 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:00:30 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:00:30 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:00:30 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_9968772c'), local_subscribe_addr='ipc:///tmp/0d760fde-429a-4343-92c7-8699f54c87a0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m INFO 09-17 18:00:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3ff07bbc'), local_subscribe_addr='ipc:///tmp/d3c24809-878f-4af2-9fbe-6865daafcc94', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m INFO 09-17 18:00:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_055d2753'), local_subscribe_addr='ipc:///tmp/e6107891-ef19-4b4a-af39-2d387263d577', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m INFO 09-17 18:00:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_01e8f875'), local_subscribe_addr='ipc:///tmp/7185f170-9dfe-4793-98d6-43288543763a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m INFO 09-17 18:00:33 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2a53d7a7'), local_subscribe_addr='ipc:///tmp/48ce36a1-4700-4793-9734-a3321e11a8b6', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m INFO 09-17 18:00:34 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:00:34 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:00:34 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:00:34 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m INFO 09-17 18:00:34 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:00:34 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:00:34 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:00:34 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m WARNING 09-17 18:00:34 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:00:34 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m WARNING 09-17 18:00:34 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:00:34 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m INFO 09-17 18:00:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_5023baf1'), local_subscribe_addr='ipc:///tmp/05ccf5e1-18c9-4895-aa5a-e47dd9dba392', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m INFO 09-17 18:00:34 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:00:34 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 18:00:34 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:00:34 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m WARNING 09-17 18:00:34 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m WARNING 09-17 18:00:34 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m WARNING 09-17 18:00:34 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m INFO 09-17 18:00:34 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m WARNING 09-17 18:00:34 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m INFO 09-17 18:00:34 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m INFO 09-17 18:00:34 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:00:34 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m INFO 09-17 18:00:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m INFO 09-17 18:00:35 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m INFO 09-17 18:00:35 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m INFO 09-17 18:00:35 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m INFO 09-17 18:00:35 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m INFO 09-17 18:00:35 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m INFO 09-17 18:00:35 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m INFO 09-17 18:00:35 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m INFO 09-17 18:00:35 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m INFO 09-17 18:00:35 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m INFO 09-17 18:00:35 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m INFO 09-17 18:00:35 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:13,  2.63s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:05<00:11,  2.82s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:08<00:07,  2.66s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:10<00:04,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m INFO 09-17 18:00:47 [default_loader.py:262] Loading weights took 11.70 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m INFO 09-17 18:00:47 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:11<00:02,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m INFO 09-17 18:00:47 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.271506 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m INFO 09-17 18:00:48 [default_loader.py:262] Loading weights took 12.47 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m INFO 09-17 18:00:48 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:12<00:00,  1.83s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:12<00:00,  2.16s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m INFO 09-17 18:00:48 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.075762 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m INFO 09-17 18:00:48 [default_loader.py:262] Loading weights took 13.03 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m INFO 09-17 18:00:48 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m INFO 09-17 18:00:48 [default_loader.py:262] Loading weights took 13.20 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m INFO 09-17 18:00:48 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m INFO 09-17 18:00:49 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.824832 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m INFO 09-17 18:00:49 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.837101 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m INFO 09-17 18:01:06 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m INFO 09-17 18:01:06 [backends.py:541] Dynamo bytecode transform time: 15.81 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m INFO 09-17 18:01:06 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m INFO 09-17 18:01:06 [backends.py:541] Dynamo bytecode transform time: 15.86 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m INFO 09-17 18:01:06 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m INFO 09-17 18:01:06 [backends.py:541] Dynamo bytecode transform time: 16.08 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m INFO 09-17 18:01:06 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m INFO 09-17 18:01:06 [backends.py:541] Dynamo bytecode transform time: 16.33 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m INFO 09-17 18:01:16 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.586 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m INFO 09-17 18:01:16 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.599 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m INFO 09-17 18:01:16 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.410 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m INFO 09-17 18:01:16 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.568 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m INFO 09-17 18:01:22 [monitor.py:34] torch.compile takes 15.81 s in total\n",
      "INFO 09-17 18:01:22 [monitor.py:34] torch.compile takes 15.86 s in total\n",
      "INFO 09-17 18:01:22 [monitor.py:34] torch.compile takes 16.33 s in total\n",
      "INFO 09-17 18:01:22 [monitor.py:34] torch.compile takes 16.08 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m INFO 09-17 18:01:23 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m INFO 09-17 18:01:23 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m INFO 09-17 18:01:23 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m INFO 09-17 18:01:23 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:01:23 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:01:23 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:01:24 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:01:24 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:01:24 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:01:24 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:01:24 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:01:24 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m INFO 09-17 18:01:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m INFO 09-17 18:01:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:01:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m INFO 09-17 18:01:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:01:28 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.28 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_2_0.8/checkpoint-1000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<39:35,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=503584)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=503583)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=503586)\u001b[0;0m INFO 09-17 18:01:29 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:01:29 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:01:29 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=503585)\u001b[0;0m INFO 09-17 18:01:29 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2839.28it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 156.91it/s, est. speed input: 8208.82 toks/s, output: 418.89 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank3]:[W917 18:02:02.332549888 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=212, addr=[::ffff:127.0.0.1]:54214, remote=[::ffff:127.0.0.1]:52531): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7225261785e8 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8bfe (0x7224fe97fbfe in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baaf40 (0x7224fe981f40 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab84a (0x7224fe98284a in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7224fe97c2a9 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7224c007b9f9 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdc253 (0x722538df9253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7225396feac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #8: clone + 0x44 (0x72253978fbf4 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[W917 18:02:02.340726441 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 3] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.5/checkpoint-2500...\n",
      "INFO 09-17 18:02:04 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:02:04 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:02:04 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:02:04 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:02:04 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:02:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_50f1e6fc'), local_subscribe_addr='ipc:///tmp/8097b63c-d246-4abf-b088-7d169b04a20b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m INFO 09-17 18:02:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2008da28'), local_subscribe_addr='ipc:///tmp/66f06b9e-6942-448f-84a1-1d6eb52d703b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m INFO 09-17 18:02:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8c932e57'), local_subscribe_addr='ipc:///tmp/fb68e09e-d563-4192-92d7-95d895fc9a17', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m INFO 09-17 18:02:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fc8bde24'), local_subscribe_addr='ipc:///tmp/7e0407eb-491b-44ab-aaee-a15636440164', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m INFO 09-17 18:02:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_00dfc309'), local_subscribe_addr='ipc:///tmp/41eea54b-a0e4-4dd8-a5ad-2fae3a7841cf', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m INFO 09-17 18:02:08 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m INFO 09-17 18:02:08 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:02:08 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:02:08 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m INFO 09-17 18:02:08 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m INFO 09-17 18:02:08 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:02:08 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:02:08 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m WARNING 09-17 18:02:08 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:02:08 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m WARNING 09-17 18:02:08 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m WARNING 09-17 18:02:08 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m INFO 09-17 18:02:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_561176b4'), local_subscribe_addr='ipc:///tmp/e085d6e3-e20f-4bf7-a3a2-abea4be45d39', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m INFO 09-17 18:02:08 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:02:08 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 18:02:08 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m INFO 09-17 18:02:08 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m WARNING 09-17 18:02:08 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m WARNING 09-17 18:02:08 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:02:08 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m INFO 09-17 18:02:08 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m WARNING 09-17 18:02:08 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m INFO 09-17 18:02:08 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m INFO 09-17 18:02:08 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m INFO 09-17 18:02:08 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m INFO 09-17 18:02:08 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m INFO 09-17 18:02:08 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m INFO 09-17 18:02:08 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m INFO 09-17 18:02:09 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m INFO 09-17 18:02:09 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m INFO 09-17 18:02:09 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m INFO 09-17 18:02:09 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m INFO 09-17 18:02:09 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m INFO 09-17 18:02:09 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m INFO 09-17 18:02:09 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m INFO 09-17 18:02:09 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m INFO 09-17 18:02:09 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:11,  2.29s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:08,  2.16s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:06<00:06,  2.03s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:03,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m INFO 09-17 18:02:18 [default_loader.py:262] Loading weights took 8.82 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m INFO 09-17 18:02:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m INFO 09-17 18:02:18 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.367002 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.42s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.67s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m INFO 09-17 18:02:19 [default_loader.py:262] Loading weights took 10.06 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m INFO 09-17 18:02:19 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m INFO 09-17 18:02:20 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.756796 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m INFO 09-17 18:02:20 [default_loader.py:262] Loading weights took 10.68 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m INFO 09-17 18:02:20 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m INFO 09-17 18:02:20 [default_loader.py:262] Loading weights took 10.97 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m INFO 09-17 18:02:20 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m INFO 09-17 18:02:21 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.494551 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m INFO 09-17 18:02:21 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.612949 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m INFO 09-17 18:02:37 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m INFO 09-17 18:02:37 [backends.py:541] Dynamo bytecode transform time: 15.74 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m INFO 09-17 18:02:37 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m INFO 09-17 18:02:37 [backends.py:541] Dynamo bytecode transform time: 15.94 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m INFO 09-17 18:02:38 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m INFO 09-17 18:02:38 [backends.py:541] Dynamo bytecode transform time: 16.15 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m INFO 09-17 18:02:38 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m INFO 09-17 18:02:38 [backends.py:541] Dynamo bytecode transform time: 16.32 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m INFO 09-17 18:02:47 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.452 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m INFO 09-17 18:02:47 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.300 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m INFO 09-17 18:02:48 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.371 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m INFO 09-17 18:02:48 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.569 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m INFO 09-17 18:02:53 [monitor.py:34] torch.compile takes 16.15 s in total\n",
      "INFO 09-17 18:02:53 [monitor.py:34] torch.compile takes 15.94 s in total\n",
      "INFO 09-17 18:02:53 [monitor.py:34] torch.compile takes 16.32 s in total\n",
      "INFO 09-17 18:02:53 [monitor.py:34] torch.compile takes 15.74 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m INFO 09-17 18:02:54 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m INFO 09-17 18:02:54 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m INFO 09-17 18:02:54 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m INFO 09-17 18:02:55 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:02:55 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:02:55 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:02:55 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:02:55 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:02:55 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:02:55 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:02:55 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:02:55 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m INFO 09-17 18:02:59 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m INFO 09-17 18:02:59 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m INFO 09-17 18:02:59 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m INFO 09-17 18:02:59 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:02:59 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.02 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.5/checkpoint-2500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<48:34,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=504439)\u001b[0;0m INFO 09-17 18:03:01 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=504440)\u001b[0;0m INFO 09-17 18:03:01 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=504437)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=504438)\u001b[0;0m INFO 09-17 18:03:01 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:03:01 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2595.40it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 153.53it/s, est. speed input: 8031.91 toks/s, output: 418.53 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.6/checkpoint-2000...\n",
      "INFO 09-17 18:03:36 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:03:36 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:03:37 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:03:37 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:03:37 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:03:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_d56593df'), local_subscribe_addr='ipc:///tmp/a402d499-235c-4a5b-81e6-d2c9281d0825', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m INFO 09-17 18:03:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9ee39d61'), local_subscribe_addr='ipc:///tmp/2fe5a162-8666-4168-b049-cec42fb52760', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m INFO 09-17 18:03:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d5e1ee85'), local_subscribe_addr='ipc:///tmp/77c3c4ff-5c05-432d-848f-e1d3e5364644', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m INFO 09-17 18:03:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f58568b2'), local_subscribe_addr='ipc:///tmp/ed1c8e4f-d0f8-4915-80c2-a99769f09cf9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m INFO 09-17 18:03:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d416601e'), local_subscribe_addr='ipc:///tmp/21c11437-90e2-4e13-a78c-4f3b3f937fba', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m INFO 09-17 18:03:40 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:03:40 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m INFO 09-17 18:03:40 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m INFO 09-17 18:03:40 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m INFO 09-17 18:03:40 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:03:40 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m INFO 09-17 18:03:40 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:03:40 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m WARNING 09-17 18:03:41 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:03:41 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m WARNING 09-17 18:03:41 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m WARNING 09-17 18:03:41 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m INFO 09-17 18:03:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_ff66d4f0'), local_subscribe_addr='ipc:///tmp/755cd112-ade7-44ca-b19b-506b6eb7979c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m INFO 09-17 18:03:41 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:03:41 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:03:41 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m WARNING 09-17 18:03:41 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:03:41 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m INFO 09-17 18:03:41 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "WARNING 09-17 18:03:41 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m INFO 09-17 18:03:41 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m INFO 09-17 18:03:41 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m INFO 09-17 18:03:41 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m WARNING 09-17 18:03:41 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m INFO 09-17 18:03:41 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m INFO 09-17 18:03:41 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m INFO 09-17 18:03:41 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m INFO 09-17 18:03:41 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m INFO 09-17 18:03:41 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m INFO 09-17 18:03:41 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m INFO 09-17 18:03:41 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m INFO 09-17 18:03:41 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m INFO 09-17 18:03:41 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m INFO 09-17 18:03:42 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m INFO 09-17 18:03:42 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m INFO 09-17 18:03:42 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m INFO 09-17 18:03:42 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:07,  1.55s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.61s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.62s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.61s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.55s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.55s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.57s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m INFO 09-17 18:03:51 [default_loader.py:262] Loading weights took 9.49 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m INFO 09-17 18:03:51 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m INFO 09-17 18:03:51 [default_loader.py:262] Loading weights took 9.42 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m INFO 09-17 18:03:51 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m INFO 09-17 18:03:51 [default_loader.py:262] Loading weights took 9.31 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m INFO 09-17 18:03:52 [default_loader.py:262] Loading weights took 9.47 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m INFO 09-17 18:03:52 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m INFO 09-17 18:03:52 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m INFO 09-17 18:03:52 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.150619 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m INFO 09-17 18:03:52 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.131550 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m INFO 09-17 18:03:52 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.342968 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m INFO 09-17 18:03:52 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.347481 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m INFO 09-17 18:04:09 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m INFO 09-17 18:04:09 [backends.py:541] Dynamo bytecode transform time: 15.72 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m INFO 09-17 18:04:09 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m INFO 09-17 18:04:09 [backends.py:541] Dynamo bytecode transform time: 15.96 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m INFO 09-17 18:04:09 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m INFO 09-17 18:04:09 [backends.py:541] Dynamo bytecode transform time: 16.02 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m INFO 09-17 18:04:09 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m INFO 09-17 18:04:09 [backends.py:541] Dynamo bytecode transform time: 16.09 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m INFO 09-17 18:04:18 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.271 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m INFO 09-17 18:04:19 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.321 s\n",
      "INFO 09-17 18:04:19 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.316 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m INFO 09-17 18:04:19 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.462 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m INFO 09-17 18:04:24 [monitor.py:34] torch.compile takes 16.09 s in total\n",
      "INFO 09-17 18:04:24 [monitor.py:34] torch.compile takes 15.72 s in total\n",
      "INFO 09-17 18:04:24 [monitor.py:34] torch.compile takes 16.02 s in total\n",
      "INFO 09-17 18:04:24 [monitor.py:34] torch.compile takes 15.96 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m INFO 09-17 18:04:26 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m INFO 09-17 18:04:26 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m INFO 09-17 18:04:26 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m INFO 09-17 18:04:26 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:04:26 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:04:26 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:04:26 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:04:26 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:04:26 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:04:26 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:04:26 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:04:26 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m INFO 09-17 18:04:30 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m INFO 09-17 18:04:30 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m INFO 09-17 18:04:30 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m INFO 09-17 18:04:30 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:04:30 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.95 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.6/checkpoint-2000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<40:04,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=505358)\u001b[0;0m INFO 09-17 18:04:32 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=505357)\u001b[0;0m INFO 09-17 18:04:32 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=505360)\u001b[0;0m INFO 09-17 18:04:32 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=505359)\u001b[0;0m INFO 09-17 18:04:32 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2716.51it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 162.73it/s, est. speed input: 8513.33 toks/s, output: 428.09 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.01/checkpoint-4950...\n",
      "INFO 09-17 18:05:07 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:05:07 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:05:07 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:05:07 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:05:07 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:05:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_45c18239'), local_subscribe_addr='ipc:///tmp/668d016d-54a0-4c8b-8629-cb18c3b5b81a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m INFO 09-17 18:05:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e119a717'), local_subscribe_addr='ipc:///tmp/f0ff2c11-64c0-42cc-ba83-1062a9ecbbda', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m INFO 09-17 18:05:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a0adc49c'), local_subscribe_addr='ipc:///tmp/51e23496-cbc1-43da-964e-1719e3553441', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m INFO 09-17 18:05:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f8ce492e'), local_subscribe_addr='ipc:///tmp/11a8404b-31fa-4bdf-8bd3-b7c1162dc3dd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m INFO 09-17 18:05:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0ee1ba6a'), local_subscribe_addr='ipc:///tmp/848270ec-859c-434d-8b95-d325231b65dc', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m INFO 09-17 18:05:10 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:05:10 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:05:10 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m INFO 09-17 18:05:10 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:05:10 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:05:10 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m INFO 09-17 18:05:10 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m INFO 09-17 18:05:10 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m WARNING 09-17 18:05:11 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:05:11 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m WARNING 09-17 18:05:11 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m WARNING 09-17 18:05:11 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m INFO 09-17 18:05:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_3a5cbe26'), local_subscribe_addr='ipc:///tmp/2afc1ec4-be88-430a-b314-48e45e28269a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m INFO 09-17 18:05:11 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:05:11 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:05:11 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:05:11 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m WARNING 09-17 18:05:11 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:05:11 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m WARNING 09-17 18:05:11 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:05:11 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m INFO 09-17 18:05:11 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m INFO 09-17 18:05:11 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m INFO 09-17 18:05:11 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:05:11 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m INFO 09-17 18:05:11 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m INFO 09-17 18:05:11 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m INFO 09-17 18:05:11 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m INFO 09-17 18:05:11 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m INFO 09-17 18:05:11 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m INFO 09-17 18:05:11 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m INFO 09-17 18:05:12 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m INFO 09-17 18:05:12 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m INFO 09-17 18:05:12 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m INFO 09-17 18:05:12 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m INFO 09-17 18:05:12 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m INFO 09-17 18:05:12 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:10,  2.03s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:08,  2.13s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:06<00:06,  2.12s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:03,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m INFO 09-17 18:05:21 [default_loader.py:262] Loading weights took 9.12 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m INFO 09-17 18:05:21 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:09<00:01,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m INFO 09-17 18:05:22 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.782375 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.53s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.74s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m INFO 09-17 18:05:23 [default_loader.py:262] Loading weights took 10.51 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m INFO 09-17 18:05:23 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m INFO 09-17 18:05:23 [default_loader.py:262] Loading weights took 10.94 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m INFO 09-17 18:05:23 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m INFO 09-17 18:05:23 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.362985 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m INFO 09-17 18:05:23 [default_loader.py:262] Loading weights took 11.39 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m INFO 09-17 18:05:23 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m INFO 09-17 18:05:24 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.807105 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m INFO 09-17 18:05:24 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.031472 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m INFO 09-17 18:05:41 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m INFO 09-17 18:05:41 [backends.py:541] Dynamo bytecode transform time: 15.80 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m INFO 09-17 18:05:41 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m INFO 09-17 18:05:41 [backends.py:541] Dynamo bytecode transform time: 15.88 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m INFO 09-17 18:05:41 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m INFO 09-17 18:05:41 [backends.py:541] Dynamo bytecode transform time: 15.92 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m INFO 09-17 18:05:41 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m INFO 09-17 18:05:41 [backends.py:541] Dynamo bytecode transform time: 15.87 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m INFO 09-17 18:05:50 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.435 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m INFO 09-17 18:05:51 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.460 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m INFO 09-17 18:05:51 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.465 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m INFO 09-17 18:05:51 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.558 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m INFO 09-17 18:05:56 [monitor.py:34] torch.compile takes 15.88 s in total\n",
      "INFO 09-17 18:05:56 [monitor.py:34] torch.compile takes 15.92 s in total\n",
      "INFO 09-17 18:05:56 [monitor.py:34] torch.compile takes 15.87 s in total\n",
      "INFO 09-17 18:05:56 [monitor.py:34] torch.compile takes 15.80 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m INFO 09-17 18:05:57 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m INFO 09-17 18:05:57 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m INFO 09-17 18:05:57 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m INFO 09-17 18:05:57 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "INFO 09-17 18:05:58 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:05:58 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:05:58 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:05:58 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:05:58 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:05:58 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:05:58 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:05:58 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m INFO 09-17 18:06:02 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m INFO 09-17 18:06:02 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m INFO 09-17 18:06:02 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m INFO 09-17 18:06:02 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:06:02 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.82 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.01/checkpoint-4950 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<46:39,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=506202)\u001b[0;0m INFO 09-17 18:06:04 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=506206)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=506204)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=506203)\u001b[0;0m INFO 09-17 18:06:04 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:06:04 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:06:04 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2712.47it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 155.66it/s, est. speed input: 8143.66 toks/s, output: 419.37 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_1_0.6/checkpoint-2000...\n",
      "INFO 09-17 18:06:39 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:06:39 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:06:40 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:06:40 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:06:40 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:06:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_4fd1e7c9'), local_subscribe_addr='ipc:///tmp/0260fb1e-ee60-4b24-a3b4-506e3d909bd3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m INFO 09-17 18:06:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1a4ce3d9'), local_subscribe_addr='ipc:///tmp/e1f5b69c-b3fa-474b-81c1-077c08535119', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m INFO 09-17 18:06:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5f9f5808'), local_subscribe_addr='ipc:///tmp/86ea1db4-d2c1-4c5e-99b8-2463c21ce147', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m INFO 09-17 18:06:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c8d1f08e'), local_subscribe_addr='ipc:///tmp/fdd20f5e-ccd9-4826-85c1-89b0e02cc7c7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m INFO 09-17 18:06:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5836bf41'), local_subscribe_addr='ipc:///tmp/19881eaf-546b-4fa7-84e1-442b55f6d128', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m INFO 09-17 18:06:43 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m INFO 09-17 18:06:43 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:06:43 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:06:43 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:06:43 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m INFO 09-17 18:06:43 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:06:43 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:06:43 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m WARNING 09-17 18:06:43 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:06:43 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m WARNING 09-17 18:06:43 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m WARNING 09-17 18:06:43 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m INFO 09-17 18:06:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_f9b699e5'), local_subscribe_addr='ipc:///tmp/53b93f8e-97d3-4b34-a317-9c95724caadc', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m INFO 09-17 18:06:43 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 18:06:43 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:06:43 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:06:43 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m WARNING 09-17 18:06:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m WARNING 09-17 18:06:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m WARNING 09-17 18:06:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:06:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m INFO 09-17 18:06:43 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m INFO 09-17 18:06:43 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:06:43 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m INFO 09-17 18:06:43 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m INFO 09-17 18:06:44 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m INFO 09-17 18:06:44 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m INFO 09-17 18:06:44 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m INFO 09-17 18:06:44 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m INFO 09-17 18:06:44 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m INFO 09-17 18:06:44 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m INFO 09-17 18:06:44 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m INFO 09-17 18:06:44 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m INFO 09-17 18:06:44 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m INFO 09-17 18:06:44 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m INFO 09-17 18:06:44 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m INFO 09-17 18:06:44 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.62s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.75s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.81s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:03,  1.77s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.64s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.62s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.67s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m INFO 09-17 18:06:55 [default_loader.py:262] Loading weights took 10.09 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m INFO 09-17 18:06:55 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m INFO 09-17 18:06:55 [default_loader.py:262] Loading weights took 10.23 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m INFO 09-17 18:06:55 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m INFO 09-17 18:06:55 [default_loader.py:262] Loading weights took 10.05 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m INFO 09-17 18:06:55 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m INFO 09-17 18:06:55 [default_loader.py:262] Loading weights took 10.48 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m INFO 09-17 18:06:55 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m INFO 09-17 18:06:55 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.794867 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m INFO 09-17 18:06:55 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.821314 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m INFO 09-17 18:06:56 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.916678 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m INFO 09-17 18:06:56 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.075483 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m INFO 09-17 18:07:12 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m INFO 09-17 18:07:12 [backends.py:541] Dynamo bytecode transform time: 15.69 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m INFO 09-17 18:07:12 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m INFO 09-17 18:07:12 [backends.py:541] Dynamo bytecode transform time: 15.66 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m INFO 09-17 18:07:12 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m INFO 09-17 18:07:12 [backends.py:541] Dynamo bytecode transform time: 15.83 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m INFO 09-17 18:07:12 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m INFO 09-17 18:07:12 [backends.py:541] Dynamo bytecode transform time: 15.90 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m INFO 09-17 18:07:22 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.489 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m INFO 09-17 18:07:22 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.477 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m INFO 09-17 18:07:22 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.509 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m INFO 09-17 18:07:22 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.680 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m INFO 09-17 18:07:28 [monitor.py:34] torch.compile takes 15.90 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m INFO 09-17 18:07:28 [monitor.py:34] torch.compile takes 15.69 s in total\n",
      "INFO 09-17 18:07:28 [monitor.py:34] torch.compile takes 15.66 s in total\n",
      "INFO 09-17 18:07:28 [monitor.py:34] torch.compile takes 15.83 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m INFO 09-17 18:07:29 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m INFO 09-17 18:07:29 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m INFO 09-17 18:07:29 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m INFO 09-17 18:07:29 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "INFO 09-17 18:07:30 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:07:30 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:07:30 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:07:30 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:07:30 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:07:30 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:07:30 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:07:30 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m INFO 09-17 18:07:34 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m INFO 09-17 18:07:34 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m INFO 09-17 18:07:34 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m INFO 09-17 18:07:34 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:07:34 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.76 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_1_0.6/checkpoint-2000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<46:02,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=507055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=507056)\u001b[0;0m INFO 09-17 18:07:36 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:07:36 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507053)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=507054)\u001b[0;0m INFO 09-17 18:07:36 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:07:36 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 2382.38it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 159.51it/s, est. speed input: 8344.81 toks/s, output: 432.41 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank3]:[W917 18:08:10.511075135 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=211, addr=[::ffff:127.0.0.1]:38092, remote=[::ffff:127.0.0.1]:41979): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7225261785e8 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8bfe (0x7224fe97fbfe in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baaf40 (0x7224fe981f40 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab84a (0x7224fe98284a in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7224fe97c2a9 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7224c007b9f9 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdc253 (0x722538df9253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7225396feac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #8: clone + 0x44 (0x72253978fbf4 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[W917 18:08:10.576131115 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 3] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_1_0.4/checkpoint-3000...\n",
      "INFO 09-17 18:08:11 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:08:11 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:08:12 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:08:12 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:08:12 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:08:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_ada3a4c0'), local_subscribe_addr='ipc:///tmp/8b8e8b2b-1d57-4e51-bb41-7be94e3b6c08', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m INFO 09-17 18:08:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c1ec78aa'), local_subscribe_addr='ipc:///tmp/e3736d53-a489-4dd6-9520-4e99ecf1b127', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m INFO 09-17 18:08:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f1e715e8'), local_subscribe_addr='ipc:///tmp/bbda530b-d65c-4a9a-b0c7-da8668d5052a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m INFO 09-17 18:08:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fc4c0b4e'), local_subscribe_addr='ipc:///tmp/90251c75-1b84-4c21-88bb-f4d60fbfeb08', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m INFO 09-17 18:08:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9bba828c'), local_subscribe_addr='ipc:///tmp/90757376-4221-4e47-9dd3-73a12f8547ea', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m INFO 09-17 18:08:15 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:08:15 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:08:15 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:08:15 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m INFO 09-17 18:08:15 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:08:15 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m INFO 09-17 18:08:15 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:08:15 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m WARNING 09-17 18:08:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:08:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m WARNING 09-17 18:08:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:08:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m INFO 09-17 18:08:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_98ef5504'), local_subscribe_addr='ipc:///tmp/0788315b-e5cc-46b7-9f3b-a1df3a602e93', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m INFO 09-17 18:08:16 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 18:08:16 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:08:16 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:08:16 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m WARNING 09-17 18:08:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:08:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m WARNING 09-17 18:08:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m INFO 09-17 18:08:16 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "WARNING 09-17 18:08:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 18:08:16 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m INFO 09-17 18:08:16 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m INFO 09-17 18:08:16 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m INFO 09-17 18:08:16 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m INFO 09-17 18:08:16 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m INFO 09-17 18:08:16 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m INFO 09-17 18:08:16 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m INFO 09-17 18:08:16 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m INFO 09-17 18:08:16 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m INFO 09-17 18:08:16 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m INFO 09-17 18:08:16 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m INFO 09-17 18:08:16 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m INFO 09-17 18:08:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m INFO 09-17 18:08:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m INFO 09-17 18:08:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.63s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.61s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.54s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:02,  1.49s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.36s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.32s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.41s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m INFO 09-17 18:08:25 [default_loader.py:262] Loading weights took 8.53 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m INFO 09-17 18:08:25 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m INFO 09-17 18:08:26 [default_loader.py:262] Loading weights took 9.10 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m INFO 09-17 18:08:26 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m INFO 09-17 18:08:26 [default_loader.py:262] Loading weights took 9.02 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m INFO 09-17 18:08:26 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m INFO 09-17 18:08:26 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.073042 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m INFO 09-17 18:08:26 [default_loader.py:262] Loading weights took 9.21 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m INFO 09-17 18:08:26 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m INFO 09-17 18:08:27 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.715670 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m INFO 09-17 18:08:27 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.708864 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m INFO 09-17 18:08:27 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.131155 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m INFO 09-17 18:08:44 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m INFO 09-17 18:08:44 [backends.py:541] Dynamo bytecode transform time: 16.20 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m INFO 09-17 18:08:44 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m INFO 09-17 18:08:44 [backends.py:541] Dynamo bytecode transform time: 16.24 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m INFO 09-17 18:08:44 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m INFO 09-17 18:08:44 [backends.py:541] Dynamo bytecode transform time: 16.31 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m INFO 09-17 18:08:44 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m INFO 09-17 18:08:44 [backends.py:541] Dynamo bytecode transform time: 16.39 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m INFO 09-17 18:08:54 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.546 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m INFO 09-17 18:08:54 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.598 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m INFO 09-17 18:08:54 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.539 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m INFO 09-17 18:08:54 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.618 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m INFO 09-17 18:08:59 [monitor.py:34] torch.compile takes 16.24 s in total\n",
      "INFO 09-17 18:08:59 [monitor.py:34] torch.compile takes 16.39 s in total\n",
      "INFO 09-17 18:08:59 [monitor.py:34] torch.compile takes 16.31 s in total\n",
      "INFO 09-17 18:08:59 [monitor.py:34] torch.compile takes 16.20 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m INFO 09-17 18:09:01 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m INFO 09-17 18:09:01 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m INFO 09-17 18:09:01 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m INFO 09-17 18:09:01 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:09:01 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:09:01 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:09:01 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:09:01 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:09:01 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:09:01 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:09:01 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:09:01 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m INFO 09-17 18:09:05 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m INFO 09-17 18:09:05 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m INFO 09-17 18:09:05 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m INFO 09-17 18:09:05 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:09:05 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.51 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_1_0.4/checkpoint-3000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<47:19,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=507911)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=507908)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=507910)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=507909)\u001b[0;0m INFO 09-17 18:09:07 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:09:07 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:09:07 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:09:07 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2759.05it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 156.57it/s, est. speed input: 8191.23 toks/s, output: 413.83 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.9/checkpoint-500...\n",
      "INFO 09-17 18:09:42 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:09:42 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:09:43 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:09:43 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:09:43 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:09:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_51945d26'), local_subscribe_addr='ipc:///tmp/a6700d44-6896-4fb1-a384-d9335469afaa', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m INFO 09-17 18:09:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8c2a3cae'), local_subscribe_addr='ipc:///tmp/71d4cfc0-0c74-4bcc-8f78-9c99efc8c291', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m INFO 09-17 18:09:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_adc268aa'), local_subscribe_addr='ipc:///tmp/f4aee084-b690-4913-bc22-cb2b1a213c88', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m INFO 09-17 18:09:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_aa27da78'), local_subscribe_addr='ipc:///tmp/809ced21-65db-451f-bca7-91486c6b7e06', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m INFO 09-17 18:09:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5c150cb1'), local_subscribe_addr='ipc:///tmp/0bfcb82d-f4da-4e3f-a1ad-278bd02f39f5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m INFO 09-17 18:09:47 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:09:47 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:09:47 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:09:47 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:09:47 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m INFO 09-17 18:09:47 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:09:47 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:09:47 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m WARNING 09-17 18:09:47 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:09:47 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m WARNING 09-17 18:09:47 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m WARNING 09-17 18:09:47 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m INFO 09-17 18:09:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_405adb48'), local_subscribe_addr='ipc:///tmp/3f2263d4-84a5-47d6-b924-35c2bdca48af', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m INFO 09-17 18:09:47 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:09:47 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m INFO 09-17 18:09:47 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:09:47 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m WARNING 09-17 18:09:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:09:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m INFO 09-17 18:09:47 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "WARNING 09-17 18:09:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 18:09:47 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m WARNING 09-17 18:09:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m INFO 09-17 18:09:47 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m INFO 09-17 18:09:47 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m INFO 09-17 18:09:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m INFO 09-17 18:09:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m INFO 09-17 18:09:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m INFO 09-17 18:09:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m INFO 09-17 18:09:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m INFO 09-17 18:09:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m INFO 09-17 18:09:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m INFO 09-17 18:09:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m INFO 09-17 18:09:48 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m INFO 09-17 18:09:48 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 09-17 18:09:48 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m INFO 09-17 18:09:48 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.67s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.66s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.66s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.63s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.55s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.54s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.58s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m INFO 09-17 18:09:58 [default_loader.py:262] Loading weights took 9.54 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m INFO 09-17 18:09:58 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m INFO 09-17 18:09:58 [default_loader.py:262] Loading weights took 9.65 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m INFO 09-17 18:09:58 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m INFO 09-17 18:09:58 [default_loader.py:262] Loading weights took 9.73 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m INFO 09-17 18:09:58 [default_loader.py:262] Loading weights took 10.07 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m INFO 09-17 18:09:58 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m INFO 09-17 18:09:58 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m INFO 09-17 18:09:59 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.237671 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m INFO 09-17 18:09:59 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.470189 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m INFO 09-17 18:09:59 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.704600 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m INFO 09-17 18:09:59 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.581189 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m INFO 09-17 18:10:15 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m INFO 09-17 18:10:15 [backends.py:541] Dynamo bytecode transform time: 15.39 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m INFO 09-17 18:10:15 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m INFO 09-17 18:10:15 [backends.py:541] Dynamo bytecode transform time: 15.79 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m INFO 09-17 18:10:16 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m INFO 09-17 18:10:16 [backends.py:541] Dynamo bytecode transform time: 15.87 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m INFO 09-17 18:10:16 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m INFO 09-17 18:10:16 [backends.py:541] Dynamo bytecode transform time: 15.93 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m INFO 09-17 18:10:25 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.703 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m INFO 09-17 18:10:26 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.696 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m INFO 09-17 18:10:26 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.650 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m INFO 09-17 18:10:26 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.005 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m INFO 09-17 18:10:31 [monitor.py:34] torch.compile takes 15.79 s in total\n",
      "INFO 09-17 18:10:31 [monitor.py:34] torch.compile takes 15.87 s in total\n",
      "INFO 09-17 18:10:31 [monitor.py:34] torch.compile takes 15.93 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m INFO 09-17 18:10:31 [monitor.py:34] torch.compile takes 15.39 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m INFO 09-17 18:10:33 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m INFO 09-17 18:10:33 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m INFO 09-17 18:10:33 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m INFO 09-17 18:10:33 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:10:34 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:10:34 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:10:34 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:10:34 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:10:34 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:10:34 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:10:34 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:10:34 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m INFO 09-17 18:10:38 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:10:38 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m INFO 09-17 18:10:38 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m INFO 09-17 18:10:38 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:10:38 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.94 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.9/checkpoint-500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<1:03:10,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=508754)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=508757)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=508756)\u001b[0;0m INFO 09-17 18:10:40 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:10:40 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:10:40 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=508755)\u001b[0;0m INFO 09-17 18:10:40 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 2454.25it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 161.07it/s, est. speed input: 8426.19 toks/s, output: 452.73 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.9/checkpoint-500...\n",
      "INFO 09-17 18:11:14 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:11:14 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:11:15 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:11:15 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:11:15 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:11:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_38c9491b'), local_subscribe_addr='ipc:///tmp/5ad1e25c-a8e6-4ce2-873e-2240251f3260', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m INFO 09-17 18:11:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_aef3dba2'), local_subscribe_addr='ipc:///tmp/f9965148-f3d4-4117-820f-e5c06690d330', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m INFO 09-17 18:11:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fc75ecff'), local_subscribe_addr='ipc:///tmp/ad0f99c7-b507-4703-9c07-81826f578b52', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m INFO 09-17 18:11:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cb25fddb'), local_subscribe_addr='ipc:///tmp/5b74e74b-6c6b-4838-bc6c-fcc079f3de0d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m INFO 09-17 18:11:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ebe2b4ed'), local_subscribe_addr='ipc:///tmp/8ad99c65-a737-4157-89bb-3ed2b60f0095', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m INFO 09-17 18:11:19 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:11:19 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m INFO 09-17 18:11:19 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:11:19 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m INFO 09-17 18:11:19 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m INFO 09-17 18:11:19 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:11:19 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m INFO 09-17 18:11:19 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m WARNING 09-17 18:11:19 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:11:19 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m WARNING 09-17 18:11:19 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m WARNING 09-17 18:11:19 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m INFO 09-17 18:11:19 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_ea845c3f'), local_subscribe_addr='ipc:///tmp/c5dadc54-4a85-472d-9565-ea0676d3d723', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m INFO 09-17 18:11:19 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m INFO 09-17 18:11:19 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:11:19 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:11:19 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m WARNING 09-17 18:11:19 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m WARNING 09-17 18:11:19 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:11:19 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m WARNING 09-17 18:11:20 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 18:11:20 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m INFO 09-17 18:11:20 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:11:20 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m INFO 09-17 18:11:20 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m INFO 09-17 18:11:20 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m INFO 09-17 18:11:20 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m INFO 09-17 18:11:20 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m INFO 09-17 18:11:20 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m INFO 09-17 18:11:20 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m INFO 09-17 18:11:20 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m INFO 09-17 18:11:20 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m INFO 09-17 18:11:20 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m INFO 09-17 18:11:20 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m INFO 09-17 18:11:20 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m INFO 09-17 18:11:20 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m INFO 09-17 18:11:21 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.69s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.64s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.61s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.56s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.48s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.43s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.50s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m INFO 09-17 18:11:29 [default_loader.py:262] Loading weights took 9.08 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m INFO 09-17 18:11:29 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m INFO 09-17 18:11:30 [default_loader.py:262] Loading weights took 9.35 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m INFO 09-17 18:11:30 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m INFO 09-17 18:11:30 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.583084 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m INFO 09-17 18:11:30 [default_loader.py:262] Loading weights took 9.50 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m INFO 09-17 18:11:30 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m INFO 09-17 18:11:30 [default_loader.py:262] Loading weights took 9.27 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m INFO 09-17 18:11:30 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m INFO 09-17 18:11:30 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.949258 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m INFO 09-17 18:11:31 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.244228 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m INFO 09-17 18:11:31 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.262768 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m INFO 09-17 18:11:47 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m INFO 09-17 18:11:47 [backends.py:541] Dynamo bytecode transform time: 15.71 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m INFO 09-17 18:11:47 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m INFO 09-17 18:11:47 [backends.py:541] Dynamo bytecode transform time: 15.90 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m INFO 09-17 18:11:48 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m INFO 09-17 18:11:48 [backends.py:541] Dynamo bytecode transform time: 16.44 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m INFO 09-17 18:11:48 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m INFO 09-17 18:11:48 [backends.py:541] Dynamo bytecode transform time: 16.44 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m INFO 09-17 18:11:57 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.332 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m INFO 09-17 18:11:57 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.586 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m INFO 09-17 18:11:58 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.541 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m INFO 09-17 18:11:58 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.609 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m INFO 09-17 18:12:03 [monitor.py:34] torch.compile takes 16.44 s in total\n",
      "INFO 09-17 18:12:03 [monitor.py:34] torch.compile takes 16.44 s in total\n",
      "INFO 09-17 18:12:03 [monitor.py:34] torch.compile takes 15.71 s in total\n",
      "INFO 09-17 18:12:03 [monitor.py:34] torch.compile takes 15.90 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m INFO 09-17 18:12:05 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m INFO 09-17 18:12:05 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m INFO 09-17 18:12:05 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m INFO 09-17 18:12:05 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:12:05 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:12:05 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:12:05 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:12:05 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:12:05 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:12:05 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:12:05 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:12:05 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m INFO 09-17 18:12:09 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:12:09 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m INFO 09-17 18:12:09 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m INFO 09-17 18:12:09 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:12:10 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.80 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.9/checkpoint-500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<44:15,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=509621)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=509623)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=509622)\u001b[0;0m INFO 09-17 18:12:12 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:12:12 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=509624)\u001b[0;0m INFO 09-17 18:12:12 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:12:12 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 2001.95it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 161.83it/s, est. speed input: 8466.45 toks/s, output: 431.55 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.1/checkpoint-4500...\n",
      "INFO 09-17 18:12:47 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:12:47 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:12:47 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:12:47 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:12:47 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:12:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_f382f73c'), local_subscribe_addr='ipc:///tmp/b3abab6f-1a71-45bc-990e-366fd70ffd4d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m INFO 09-17 18:12:49 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5bf920cd'), local_subscribe_addr='ipc:///tmp/cbd4960c-9191-4230-9470-01cbe32ac20c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m INFO 09-17 18:12:49 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8bf8cd3c'), local_subscribe_addr='ipc:///tmp/a950f4c6-b549-490d-accf-fee8685e74c7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m INFO 09-17 18:12:49 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bdbeb0ab'), local_subscribe_addr='ipc:///tmp/7cbf75ba-0759-4354-a7a2-a75bd289d1f1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m INFO 09-17 18:12:49 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_675ed03b'), local_subscribe_addr='ipc:///tmp/73e9c92f-0281-446b-b6f6-bd52b29c453f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m INFO 09-17 18:12:51 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m INFO 09-17 18:12:51 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:12:51 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m INFO 09-17 18:12:51 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m INFO 09-17 18:12:51 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:12:51 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m INFO 09-17 18:12:51 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m INFO 09-17 18:12:51 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m WARNING 09-17 18:12:51 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:12:51 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m WARNING 09-17 18:12:51 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m WARNING 09-17 18:12:51 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m INFO 09-17 18:12:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_7a3ca255'), local_subscribe_addr='ipc:///tmp/dc450a53-193d-451b-a42b-44801b92335c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m INFO 09-17 18:12:51 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:12:51 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:12:51 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:12:51 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m WARNING 09-17 18:12:51 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:12:51 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:12:51 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m WARNING 09-17 18:12:51 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m INFO 09-17 18:12:51 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:12:51 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m INFO 09-17 18:12:51 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m INFO 09-17 18:12:51 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m INFO 09-17 18:12:51 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m INFO 09-17 18:12:51 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m INFO 09-17 18:12:51 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m INFO 09-17 18:12:51 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m INFO 09-17 18:12:51 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m INFO 09-17 18:12:52 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m INFO 09-17 18:12:52 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m INFO 09-17 18:12:52 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m INFO 09-17 18:12:52 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m INFO 09-17 18:12:52 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m INFO 09-17 18:12:52 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 09-17 18:12:52 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.63s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.69s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.71s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.71s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m INFO 09-17 18:13:02 [default_loader.py:262] Loading weights took 9.96 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m INFO 09-17 18:13:02 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.64s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.66s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m INFO 09-17 18:13:02 [default_loader.py:262] Loading weights took 10.09 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m INFO 09-17 18:13:02 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m INFO 09-17 18:13:02 [default_loader.py:262] Loading weights took 10.04 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m INFO 09-17 18:13:02 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m INFO 09-17 18:13:02 [default_loader.py:262] Loading weights took 9.97 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m INFO 09-17 18:13:02 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m INFO 09-17 18:13:03 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.544333 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m INFO 09-17 18:13:03 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.843266 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m INFO 09-17 18:13:03 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.894468 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m INFO 09-17 18:13:03 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.841055 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m INFO 09-17 18:13:19 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m INFO 09-17 18:13:19 [backends.py:541] Dynamo bytecode transform time: 15.65 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m INFO 09-17 18:13:19 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m INFO 09-17 18:13:19 [backends.py:541] Dynamo bytecode transform time: 15.74 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m INFO 09-17 18:13:19 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m INFO 09-17 18:13:19 [backends.py:541] Dynamo bytecode transform time: 15.78 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m INFO 09-17 18:13:20 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m INFO 09-17 18:13:20 [backends.py:541] Dynamo bytecode transform time: 16.11 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m INFO 09-17 18:13:29 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.534 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m INFO 09-17 18:13:29 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.412 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m INFO 09-17 18:13:29 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.583 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m INFO 09-17 18:13:30 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.327 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m INFO 09-17 18:13:35 [monitor.py:34] torch.compile takes 15.74 s in total\n",
      "INFO 09-17 18:13:35 [monitor.py:34] torch.compile takes 16.11 s in total\n",
      "INFO 09-17 18:13:35 [monitor.py:34] torch.compile takes 15.65 s in total\n",
      "INFO 09-17 18:13:35 [monitor.py:34] torch.compile takes 15.78 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m INFO 09-17 18:13:36 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m INFO 09-17 18:13:36 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m INFO 09-17 18:13:36 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m INFO 09-17 18:13:36 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:13:37 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:13:37 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:13:37 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:13:37 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:13:37 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:13:37 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:13:37 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:13:37 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m INFO 09-17 18:13:41 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m INFO 09-17 18:13:41 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m INFO 09-17 18:13:41 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m INFO 09-17 18:13:41 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:13:41 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.35 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.1/checkpoint-4500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<44:04,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=510585)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=510586)\u001b[0;0m INFO 09-17 18:13:43 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:13:43 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=510588)\u001b[0;0m INFO 09-17 18:13:43 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=510587)\u001b[0;0m INFO 09-17 18:13:43 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2811.80it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 159.99it/s, est. speed input: 8370.03 toks/s, output: 427.82 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_1_0.5/checkpoint-2500...\n",
      "INFO 09-17 18:14:17 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:14:17 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:14:18 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:14:18 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:14:18 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:14:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_4e9093b3'), local_subscribe_addr='ipc:///tmp/3247ed1b-76a9-41d4-9078-8199d4692be8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m INFO 09-17 18:14:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e1199e2d'), local_subscribe_addr='ipc:///tmp/3f967d21-6b93-4644-a6ad-0412aaa0d174', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m INFO 09-17 18:14:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_378da733'), local_subscribe_addr='ipc:///tmp/57a8a7ce-bde2-436a-8324-a9a2319408bf', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m INFO 09-17 18:14:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2b9b17aa'), local_subscribe_addr='ipc:///tmp/04c13655-263b-4505-b2a9-775818539896', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m INFO 09-17 18:14:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_43f04a27'), local_subscribe_addr='ipc:///tmp/e6d0476f-0daa-43b1-b4c1-41a35e0f4619', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m INFO 09-17 18:14:21 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m INFO 09-17 18:14:21 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m INFO 09-17 18:14:21 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m INFO 09-17 18:14:21 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:14:21 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m INFO 09-17 18:14:21 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:14:21 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m INFO 09-17 18:14:21 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m WARNING 09-17 18:14:22 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:14:22 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m WARNING 09-17 18:14:22 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:14:22 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m INFO 09-17 18:14:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_015c9554'), local_subscribe_addr='ipc:///tmp/8cb12c7b-928c-48f5-b74c-46709a045d6a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m INFO 09-17 18:14:22 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 18:14:22 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:14:22 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:14:22 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m WARNING 09-17 18:14:22 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:14:22 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m WARNING 09-17 18:14:22 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m INFO 09-17 18:14:22 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m WARNING 09-17 18:14:22 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m INFO 09-17 18:14:22 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:14:22 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:14:22 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m INFO 09-17 18:14:22 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m INFO 09-17 18:14:22 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m INFO 09-17 18:14:22 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m INFO 09-17 18:14:22 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m INFO 09-17 18:14:22 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m INFO 09-17 18:14:22 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m INFO 09-17 18:14:22 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m INFO 09-17 18:14:22 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m INFO 09-17 18:14:22 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m INFO 09-17 18:14:22 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m INFO 09-17 18:14:23 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m INFO 09-17 18:14:23 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:07,  1.60s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:07,  1.78s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.82s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.70s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m INFO 09-17 18:14:32 [default_loader.py:262] Loading weights took 9.38 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m INFO 09-17 18:14:32 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.51s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.60s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m INFO 09-17 18:14:32 [default_loader.py:262] Loading weights took 9.67 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m INFO 09-17 18:14:32 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m INFO 09-17 18:14:32 [default_loader.py:262] Loading weights took 9.56 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m INFO 09-17 18:14:32 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m INFO 09-17 18:14:33 [default_loader.py:262] Loading weights took 9.77 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m INFO 09-17 18:14:33 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m INFO 09-17 18:14:33 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.959966 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m INFO 09-17 18:14:33 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.277033 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m INFO 09-17 18:14:33 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.372367 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m INFO 09-17 18:14:33 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.470350 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m INFO 09-17 18:14:50 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m INFO 09-17 18:14:50 [backends.py:541] Dynamo bytecode transform time: 15.80 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m INFO 09-17 18:14:50 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m INFO 09-17 18:14:50 [backends.py:541] Dynamo bytecode transform time: 15.87 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m INFO 09-17 18:14:50 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m INFO 09-17 18:14:50 [backends.py:541] Dynamo bytecode transform time: 15.93 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m INFO 09-17 18:14:50 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m INFO 09-17 18:14:50 [backends.py:541] Dynamo bytecode transform time: 16.16 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m INFO 09-17 18:15:00 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.730 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m INFO 09-17 18:15:00 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.677 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m INFO 09-17 18:15:00 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.755 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m INFO 09-17 18:15:01 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.941 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m INFO 09-17 18:15:06 [monitor.py:34] torch.compile takes 15.93 s in total\n",
      "INFO 09-17 18:15:06 [monitor.py:34] torch.compile takes 15.87 s in total\n",
      "INFO 09-17 18:15:06 [monitor.py:34] torch.compile takes 15.80 s in total\n",
      "INFO 09-17 18:15:06 [monitor.py:34] torch.compile takes 16.16 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m INFO 09-17 18:15:08 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m INFO 09-17 18:15:08 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m INFO 09-17 18:15:08 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m INFO 09-17 18:15:08 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "INFO 09-17 18:15:08 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:15:08 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:15:08 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:15:08 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:15:08 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:15:08 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:15:08 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:15:08 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m INFO 09-17 18:15:12 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m INFO 09-17 18:15:12 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m INFO 09-17 18:15:12 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m INFO 09-17 18:15:12 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:15:12 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.89 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_1_0.5/checkpoint-2500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<42:52,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=511431)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=511432)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=511430)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=511429)\u001b[0;0m INFO 09-17 18:15:14 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:15:14 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:15:14 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:15:14 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2863.87it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 154.21it/s, est. speed input: 8067.50 toks/s, output: 413.96 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_2_0.6/checkpoint-2000...\n",
      "INFO 09-17 18:15:50 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:15:50 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:15:50 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:15:50 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:15:50 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:15:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_202a5d2e'), local_subscribe_addr='ipc:///tmp/7cd914ee-01d5-41da-809f-0693faafe507', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m INFO 09-17 18:15:53 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_40251c62'), local_subscribe_addr='ipc:///tmp/d6d8fc27-aab9-4a90-a55a-161d258ca063', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m INFO 09-17 18:15:53 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ca001e38'), local_subscribe_addr='ipc:///tmp/a0dba739-735e-4d98-92f2-374b66ccf88c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m INFO 09-17 18:15:53 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_74dddf13'), local_subscribe_addr='ipc:///tmp/59e79ce6-ff1b-419f-ba52-0431a9bf7e17', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m INFO 09-17 18:15:53 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cd1b62a1'), local_subscribe_addr='ipc:///tmp/e4fc1a28-8cb3-4a0d-bbee-80f56447ffa6', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m INFO 09-17 18:15:54 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:15:54 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:15:54 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m INFO 09-17 18:15:54 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:15:54 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:15:54 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:15:54 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m INFO 09-17 18:15:54 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m WARNING 09-17 18:15:55 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:15:55 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m WARNING 09-17 18:15:55 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:15:55 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m INFO 09-17 18:15:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_d43a8019'), local_subscribe_addr='ipc:///tmp/bb835874-a19b-41ef-92bf-2afc3e8a9ef7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m INFO 09-17 18:15:55 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m INFO 09-17 18:15:55 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:15:55 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m INFO 09-17 18:15:55 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m WARNING 09-17 18:15:55 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:15:55 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:15:55 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m WARNING 09-17 18:15:55 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m INFO 09-17 18:15:55 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:15:55 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m INFO 09-17 18:15:55 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m INFO 09-17 18:15:55 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m INFO 09-17 18:15:55 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m INFO 09-17 18:15:55 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 09-17 18:15:55 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m INFO 09-17 18:15:55 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m INFO 09-17 18:15:55 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m INFO 09-17 18:15:55 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m INFO 09-17 18:15:55 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m INFO 09-17 18:15:55 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m INFO 09-17 18:15:56 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m INFO 09-17 18:15:56 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m INFO 09-17 18:15:56 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m INFO 09-17 18:15:56 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.67s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.73s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.74s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.73s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m INFO 09-17 18:16:05 [default_loader.py:262] Loading weights took 9.05 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m INFO 09-17 18:16:05 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m INFO 09-17 18:16:06 [default_loader.py:262] Loading weights took 9.74 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m INFO 09-17 18:16:06 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.672700 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m INFO 09-17 18:16:06 [default_loader.py:262] Loading weights took 9.63 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m INFO 09-17 18:16:06 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m INFO 09-17 18:16:06 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.54s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.63s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m INFO 09-17 18:16:06 [default_loader.py:262] Loading weights took 9.81 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m INFO 09-17 18:16:06 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m INFO 09-17 18:16:06 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.450010 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m INFO 09-17 18:16:07 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.474783 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m INFO 09-17 18:16:07 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.708781 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m INFO 09-17 18:16:23 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m INFO 09-17 18:16:23 [backends.py:541] Dynamo bytecode transform time: 15.72 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m INFO 09-17 18:16:23 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m INFO 09-17 18:16:23 [backends.py:541] Dynamo bytecode transform time: 15.79 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m INFO 09-17 18:16:23 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m INFO 09-17 18:16:23 [backends.py:541] Dynamo bytecode transform time: 15.82 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m INFO 09-17 18:16:24 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m INFO 09-17 18:16:24 [backends.py:541] Dynamo bytecode transform time: 16.63 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m INFO 09-17 18:16:33 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.305 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m INFO 09-17 18:16:33 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.461 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m INFO 09-17 18:16:33 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.614 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m INFO 09-17 18:16:34 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.731 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m INFO 09-17 18:16:39 [monitor.py:34] torch.compile takes 15.79 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m INFO 09-17 18:16:39 [monitor.py:34] torch.compile takes 16.63 s in total\n",
      "INFO 09-17 18:16:39 [monitor.py:34] torch.compile takes 15.82 s in total\n",
      "INFO 09-17 18:16:39 [monitor.py:34] torch.compile takes 15.72 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m INFO 09-17 18:16:41 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m INFO 09-17 18:16:41 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m INFO 09-17 18:16:41 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m INFO 09-17 18:16:41 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "INFO 09-17 18:16:41 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:16:41 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:16:41 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:16:41 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:16:41 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:16:41 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:16:41 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:16:41 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m INFO 09-17 18:16:45 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m INFO 09-17 18:16:45 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m INFO 09-17 18:16:45 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m INFO 09-17 18:16:45 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:16:45 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.61 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_2_0.6/checkpoint-2000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<42:23,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=512281)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=512278)\u001b[0;0m INFO 09-17 18:16:47 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:16:47 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=512280)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=512279)\u001b[0;0m INFO 09-17 18:16:47 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:16:47 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2559.74it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 164.57it/s, est. speed input: 8609.71 toks/s, output: 446.69 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.6/checkpoint-2000...\n",
      "INFO 09-17 18:17:21 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:17:21 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:17:21 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:17:21 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:17:21 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:17:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_e27f871a'), local_subscribe_addr='ipc:///tmp/6928b610-9dac-4259-abc5-183b548f18fd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m INFO 09-17 18:17:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_692c2732'), local_subscribe_addr='ipc:///tmp/0b3012a1-6ce3-411f-9cf5-61d20b8fbdd2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m INFO 09-17 18:17:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_54d61704'), local_subscribe_addr='ipc:///tmp/a6261ce4-7d26-41dc-9f71-138075528c05', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m INFO 09-17 18:17:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0fdad1d1'), local_subscribe_addr='ipc:///tmp/edef510a-b368-490f-9cb9-1b6622c6f8c2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m INFO 09-17 18:17:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e55847df'), local_subscribe_addr='ipc:///tmp/8cc73ff3-04f6-407a-844b-a27dc27cf123', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m INFO 09-17 18:17:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:17:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:17:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m INFO 09-17 18:17:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m INFO 09-17 18:17:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:17:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:17:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m INFO 09-17 18:17:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m WARNING 09-17 18:17:25 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m WARNING 09-17 18:17:25 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m WARNING 09-17 18:17:25 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:17:25 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m INFO 09-17 18:17:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_9711385d'), local_subscribe_addr='ipc:///tmp/d5bd0753-1298-4576-8fdf-53a2d638c918', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m INFO 09-17 18:17:25 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 18:17:25 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:17:25 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:17:25 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m WARNING 09-17 18:17:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m WARNING 09-17 18:17:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:17:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:17:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m INFO 09-17 18:17:25 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m INFO 09-17 18:17:25 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m INFO 09-17 18:17:25 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:17:25 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m INFO 09-17 18:17:26 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m INFO 09-17 18:17:26 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m INFO 09-17 18:17:26 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m INFO 09-17 18:17:26 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m INFO 09-17 18:17:26 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m INFO 09-17 18:17:26 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m INFO 09-17 18:17:26 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m INFO 09-17 18:17:26 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m INFO 09-17 18:17:26 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m INFO 09-17 18:17:26 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m INFO 09-17 18:17:26 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m INFO 09-17 18:17:26 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:07,  1.60s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.58s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.56s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.50s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.39s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.33s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.42s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m INFO 09-17 18:17:35 [default_loader.py:262] Loading weights took 8.60 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m INFO 09-17 18:17:35 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m INFO 09-17 18:17:36 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.201235 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m INFO 09-17 18:17:36 [default_loader.py:262] Loading weights took 9.23 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m INFO 09-17 18:17:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m INFO 09-17 18:17:36 [default_loader.py:262] Loading weights took 9.36 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m INFO 09-17 18:17:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m INFO 09-17 18:17:36 [default_loader.py:262] Loading weights took 9.22 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m INFO 09-17 18:17:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m INFO 09-17 18:17:36 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.016469 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m INFO 09-17 18:17:37 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.093170 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m INFO 09-17 18:17:37 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.160444 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m INFO 09-17 18:17:53 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m INFO 09-17 18:17:53 [backends.py:541] Dynamo bytecode transform time: 15.93 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m INFO 09-17 18:17:53 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m INFO 09-17 18:17:53 [backends.py:541] Dynamo bytecode transform time: 16.12 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m INFO 09-17 18:17:53 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m INFO 09-17 18:17:53 [backends.py:541] Dynamo bytecode transform time: 16.09 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m INFO 09-17 18:17:54 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m INFO 09-17 18:17:54 [backends.py:541] Dynamo bytecode transform time: 16.48 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m INFO 09-17 18:18:03 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.364 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m INFO 09-17 18:18:03 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.508 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m INFO 09-17 18:18:03 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.510 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m INFO 09-17 18:18:04 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.504 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m INFO 09-17 18:18:09 [monitor.py:34] torch.compile takes 16.48 s in total\n",
      "INFO 09-17 18:18:09 [monitor.py:34] torch.compile takes 15.93 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m INFO 09-17 18:18:09 [monitor.py:34] torch.compile takes 16.09 s in total\n",
      "INFO 09-17 18:18:09 [monitor.py:34] torch.compile takes 16.12 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m INFO 09-17 18:18:10 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m INFO 09-17 18:18:10 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "INFO 09-17 18:18:10 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m INFO 09-17 18:18:10 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:18:11 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:18:11 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:18:11 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:18:11 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:18:11 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:18:11 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:18:11 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:18:11 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m INFO 09-17 18:18:15 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m INFO 09-17 18:18:15 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m INFO 09-17 18:18:15 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m INFO 09-17 18:18:15 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:18:16 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.80 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.6/checkpoint-2000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<40:21,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=513134)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=513137)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=513136)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=513135)\u001b[0;0m INFO 09-17 18:18:17 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:18:17 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:18:17 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:18:17 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2677.22it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 161.87it/s, est. speed input: 8468.40 toks/s, output: 446.94 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.5/checkpoint-2500...\n",
      "INFO 09-17 18:18:51 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:18:51 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:18:52 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:18:52 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:18:52 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:18:52 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_c1e21b21'), local_subscribe_addr='ipc:///tmp/965fec4d-67c4-4483-a8f6-64d867b08d37', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m INFO 09-17 18:18:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fd038b63'), local_subscribe_addr='ipc:///tmp/c813db23-e188-47ab-a81a-29546c206f00', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-17 18:18:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2ab5cfde'), local_subscribe_addr='ipc:///tmp/1345aa78-23be-4153-b802-7f495ae4e1f1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m INFO 09-17 18:18:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_64d2ce9d'), local_subscribe_addr='ipc:///tmp/b2b35065-a57c-4541-8d07-e4837b8e67cc', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m INFO 09-17 18:18:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_95e93c5c'), local_subscribe_addr='ipc:///tmp/3b7cfa2c-5d8c-4062-be83-9661eee11f4a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m INFO 09-17 18:18:55 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m INFO 09-17 18:18:55 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m INFO 09-17 18:18:55 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m INFO 09-17 18:18:55 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:18:55 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m INFO 09-17 18:18:55 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m INFO 09-17 18:18:55 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:18:55 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m WARNING 09-17 18:18:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:18:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m WARNING 09-17 18:18:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m WARNING 09-17 18:18:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m INFO 09-17 18:18:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_66be8633'), local_subscribe_addr='ipc:///tmp/8e816886-b887-46ff-8fac-b735cb0bac7b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m INFO 09-17 18:18:56 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:18:56 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:18:56 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:18:56 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m WARNING 09-17 18:18:56 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m WARNING 09-17 18:18:56 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:18:56 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m WARNING 09-17 18:18:56 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 18:18:56 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m INFO 09-17 18:18:56 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m INFO 09-17 18:18:56 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m INFO 09-17 18:18:56 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m INFO 09-17 18:18:56 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m INFO 09-17 18:18:56 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m INFO 09-17 18:18:56 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m INFO 09-17 18:18:56 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m INFO 09-17 18:18:56 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 09-17 18:18:56 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m INFO 09-17 18:18:57 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m INFO 09-17 18:18:57 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m INFO 09-17 18:18:57 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m INFO 09-17 18:18:57 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m INFO 09-17 18:18:57 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m INFO 09-17 18:18:57 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.62s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.74s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.61s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.53s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m INFO 09-17 18:19:05 [default_loader.py:262] Loading weights took 8.64 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m INFO 09-17 18:19:05 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.36s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.46s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m INFO 09-17 18:19:06 [default_loader.py:262] Loading weights took 8.83 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m INFO 09-17 18:19:06 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m INFO 09-17 18:19:06 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.168119 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m INFO 09-17 18:19:06 [default_loader.py:262] Loading weights took 8.85 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m INFO 09-17 18:19:06 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m INFO 09-17 18:19:06 [default_loader.py:262] Loading weights took 9.15 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m INFO 09-17 18:19:06 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m INFO 09-17 18:19:07 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.477679 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m INFO 09-17 18:19:07 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.661737 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m INFO 09-17 18:19:07 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.967944 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m INFO 09-17 18:19:23 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m INFO 09-17 18:19:23 [backends.py:541] Dynamo bytecode transform time: 15.72 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m INFO 09-17 18:19:24 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m INFO 09-17 18:19:24 [backends.py:541] Dynamo bytecode transform time: 15.83 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m INFO 09-17 18:19:24 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m INFO 09-17 18:19:24 [backends.py:541] Dynamo bytecode transform time: 15.93 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m INFO 09-17 18:19:24 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m INFO 09-17 18:19:24 [backends.py:541] Dynamo bytecode transform time: 15.94 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m INFO 09-17 18:19:33 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.424 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m INFO 09-17 18:19:33 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.360 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m INFO 09-17 18:19:33 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.485 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m INFO 09-17 18:19:33 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.375 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m INFO 09-17 18:19:39 [monitor.py:34] torch.compile takes 15.83 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m INFO 09-17 18:19:39 [monitor.py:34] torch.compile takes 15.93 s in total\n",
      "INFO 09-17 18:19:39 [monitor.py:34] torch.compile takes 15.94 s in total\n",
      "INFO 09-17 18:19:39 [monitor.py:34] torch.compile takes 15.72 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m INFO 09-17 18:19:40 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m INFO 09-17 18:19:40 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m INFO 09-17 18:19:40 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m INFO 09-17 18:19:40 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:19:41 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:19:41 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:19:41 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:19:41 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:19:41 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:19:41 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:19:41 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:19:41 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m INFO 09-17 18:19:45 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m INFO 09-17 18:19:45 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m INFO 09-17 18:19:45 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m INFO 09-17 18:19:45 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:19:45 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.60 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.5/checkpoint-2500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<39:20,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=513993)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=513994)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=513991)\u001b[0;0m INFO 09-17 18:19:47 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:19:47 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=513992)\u001b[0;0m INFO 09-17 18:19:47 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:19:47 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 1950.90it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 154.58it/s, est. speed input: 8087.00 toks/s, output: 421.86 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_2_0.2/checkpoint-4000...\n",
      "INFO 09-17 18:20:23 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:20:23 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:20:24 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:20:24 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:20:24 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:20:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_7bc3c883'), local_subscribe_addr='ipc:///tmp/d40c8442-2aa5-4cfb-bf2e-3ec239634cc0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m INFO 09-17 18:20:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_14aa3e2b'), local_subscribe_addr='ipc:///tmp/fe1280e8-7d69-4e04-a11b-ec94755b3a56', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-17 18:20:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fedcc66c'), local_subscribe_addr='ipc:///tmp/d333da9c-c52a-4362-a329-93b6309fdf07', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m INFO 09-17 18:20:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e531fe1c'), local_subscribe_addr='ipc:///tmp/e6bc4cc3-df46-4966-8c2d-ca6ca4c4abb5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m INFO 09-17 18:20:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_56d7fdc9'), local_subscribe_addr='ipc:///tmp/79cb086a-2a29-4fb5-985a-85ad4fce1a91', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m INFO 09-17 18:20:27 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:20:27 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:20:27 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:20:27 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m INFO 09-17 18:20:27 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:20:27 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:20:27 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:20:27 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m WARNING 09-17 18:20:27 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:20:27 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m WARNING 09-17 18:20:27 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:20:27 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m INFO 09-17 18:20:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_6a2a59cc'), local_subscribe_addr='ipc:///tmp/1e8bedd7-b005-405f-8c1c-e6f4ee76e829', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m INFO 09-17 18:20:27 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:20:27 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 18:20:27 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:20:27 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m WARNING 09-17 18:20:27 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m WARNING 09-17 18:20:27 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:20:27 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:20:27 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m INFO 09-17 18:20:27 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m INFO 09-17 18:20:27 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m INFO 09-17 18:20:27 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:20:27 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m INFO 09-17 18:20:27 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m INFO 09-17 18:20:27 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m INFO 09-17 18:20:28 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m INFO 09-17 18:20:28 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m INFO 09-17 18:20:28 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m INFO 09-17 18:20:28 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m INFO 09-17 18:20:28 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m INFO 09-17 18:20:28 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m INFO 09-17 18:20:28 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m INFO 09-17 18:20:28 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m INFO 09-17 18:20:28 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m INFO 09-17 18:20:28 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:11,  2.24s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:08,  2.18s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:06<00:06,  2.06s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:03,  1.89s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:09<00:01,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m INFO 09-17 18:20:38 [default_loader.py:262] Loading weights took 9.73 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m INFO 09-17 18:20:38 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m INFO 09-17 18:20:38 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.307386 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.51s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.73s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m INFO 09-17 18:20:39 [default_loader.py:262] Loading weights took 10.45 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m INFO 09-17 18:20:39 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m INFO 09-17 18:20:39 [default_loader.py:262] Loading weights took 10.79 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m INFO 09-17 18:20:39 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m INFO 09-17 18:20:39 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.290319 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m INFO 09-17 18:20:39 [default_loader.py:262] Loading weights took 11.29 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m INFO 09-17 18:20:39 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m INFO 09-17 18:20:40 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.325820 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m INFO 09-17 18:20:40 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.882507 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m INFO 09-17 18:20:57 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m INFO 09-17 18:20:57 [backends.py:541] Dynamo bytecode transform time: 15.84 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m INFO 09-17 18:20:57 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m INFO 09-17 18:20:57 [backends.py:541] Dynamo bytecode transform time: 15.85 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m INFO 09-17 18:20:57 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m INFO 09-17 18:20:57 [backends.py:541] Dynamo bytecode transform time: 15.98 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m INFO 09-17 18:20:57 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m INFO 09-17 18:20:57 [backends.py:541] Dynamo bytecode transform time: 16.02 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m INFO 09-17 18:21:06 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.354 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m INFO 09-17 18:21:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.336 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m INFO 09-17 18:21:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.520 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m INFO 09-17 18:21:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.317 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m INFO 09-17 18:21:12 [monitor.py:34] torch.compile takes 15.85 s in total\n",
      "INFO 09-17 18:21:12 [monitor.py:34] torch.compile takes 16.02 s in total\n",
      "INFO 09-17 18:21:12 [monitor.py:34] torch.compile takes 15.84 s in total\n",
      "INFO 09-17 18:21:12 [monitor.py:34] torch.compile takes 15.98 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m INFO 09-17 18:21:13 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m INFO 09-17 18:21:13 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m INFO 09-17 18:21:13 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m INFO 09-17 18:21:14 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:21:14 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:21:14 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:21:14 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:21:14 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:21:14 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:21:14 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:21:14 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:21:14 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m INFO 09-17 18:21:18 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m INFO 09-17 18:21:18 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m INFO 09-17 18:21:18 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m INFO 09-17 18:21:18 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:21:18 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.76 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_2_0.2/checkpoint-4000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<45:48,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=515040)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=515039)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=515037)\u001b[0;0m INFO 09-17 18:21:19 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=515038)\u001b[0;0m INFO 09-17 18:21:19 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:21:19 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:21:19 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2526.42it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 156.53it/s, est. speed input: 8189.08 toks/s, output: 437.89 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_1_0.2/checkpoint-4000...\n",
      "INFO 09-17 18:21:55 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:21:55 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:21:56 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:21:56 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:21:56 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:21:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_77054612'), local_subscribe_addr='ipc:///tmp/b56b264b-b725-435a-b16a-c3e4796e54a9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m INFO 09-17 18:21:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_087341a2'), local_subscribe_addr='ipc:///tmp/20b4a3d6-ff32-48cb-bf8b-8ae7273d7797', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m INFO 09-17 18:21:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_eec88e29'), local_subscribe_addr='ipc:///tmp/35c84a1e-ad07-4737-825f-c82c02c2f26c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m INFO 09-17 18:21:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_524b1e0c'), local_subscribe_addr='ipc:///tmp/0ad4a27b-9ce4-4865-b581-4cec5e555a45', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m INFO 09-17 18:21:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cc0765fb'), local_subscribe_addr='ipc:///tmp/086d94f1-6789-46fc-807e-515f697cbe3a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m INFO 09-17 18:21:59 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m INFO 09-17 18:21:59 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:21:59 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m INFO 09-17 18:21:59 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m INFO 09-17 18:21:59 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m INFO 09-17 18:21:59 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:21:59 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:21:59 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m WARNING 09-17 18:21:59 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:21:59 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m WARNING 09-17 18:21:59 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m WARNING 09-17 18:21:59 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m INFO 09-17 18:21:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_8b516539'), local_subscribe_addr='ipc:///tmp/5611f08a-2ed6-4c3a-91b7-404afa3fccb9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m INFO 09-17 18:21:59 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:21:59 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 18:21:59 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m INFO 09-17 18:21:59 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m WARNING 09-17 18:21:59 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:21:59 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m WARNING 09-17 18:21:59 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m INFO 09-17 18:21:59 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:21:59 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m WARNING 09-17 18:21:59 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m INFO 09-17 18:21:59 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m INFO 09-17 18:21:59 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m INFO 09-17 18:22:00 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m INFO 09-17 18:22:00 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m INFO 09-17 18:22:00 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m INFO 09-17 18:22:00 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m INFO 09-17 18:22:00 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m INFO 09-17 18:22:00 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m INFO 09-17 18:22:00 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m INFO 09-17 18:22:00 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m INFO 09-17 18:22:00 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m INFO 09-17 18:22:00 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m INFO 09-17 18:22:00 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m INFO 09-17 18:22:00 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:10,  2.06s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:08,  2.14s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:06<00:06,  2.15s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:08<00:04,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m INFO 09-17 18:22:09 [default_loader.py:262] Loading weights took 8.70 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m INFO 09-17 18:22:09 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m INFO 09-17 18:22:10 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.252524 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:09<00:01,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m INFO 09-17 18:22:11 [default_loader.py:262] Loading weights took 10.57 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m INFO 09-17 18:22:11 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m INFO 09-17 18:22:12 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.233025 seconds\n",
      "INFO 09-17 18:22:12 [default_loader.py:262] Loading weights took 11.08 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.72s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.87s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m INFO 09-17 18:22:12 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m INFO 09-17 18:22:12 [default_loader.py:262] Loading weights took 11.29 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m INFO 09-17 18:22:12 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m INFO 09-17 18:22:13 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.891723 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m INFO 09-17 18:22:13 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.961674 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m INFO 09-17 18:22:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m INFO 09-17 18:22:29 [backends.py:541] Dynamo bytecode transform time: 15.60 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m INFO 09-17 18:22:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m INFO 09-17 18:22:29 [backends.py:541] Dynamo bytecode transform time: 16.07 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m INFO 09-17 18:22:30 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m INFO 09-17 18:22:30 [backends.py:541] Dynamo bytecode transform time: 16.22 s\n",
      "INFO 09-17 18:22:30 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m INFO 09-17 18:22:30 [backends.py:541] Dynamo bytecode transform time: 16.24 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m INFO 09-17 18:22:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.405 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m INFO 09-17 18:22:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.366 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m INFO 09-17 18:22:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.379 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m INFO 09-17 18:22:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.312 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m INFO 09-17 18:22:45 [monitor.py:34] torch.compile takes 16.22 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m INFO 09-17 18:22:45 [monitor.py:34] torch.compile takes 16.07 s in total\n",
      "INFO 09-17 18:22:45 [monitor.py:34] torch.compile takes 15.60 s in total\n",
      "INFO 09-17 18:22:45 [monitor.py:34] torch.compile takes 16.24 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m INFO 09-17 18:22:46 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m INFO 09-17 18:22:46 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m INFO 09-17 18:22:46 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m INFO 09-17 18:22:46 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:22:47 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:22:47 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:22:47 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:22:47 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:22:47 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:22:47 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:22:47 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:22:47 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m INFO 09-17 18:22:51 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m INFO 09-17 18:22:51 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m INFO 09-17 18:22:51 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m INFO 09-17 18:22:51 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:22:51 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.14 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_1_0.2/checkpoint-4000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<46:10,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=516089)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=516090)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=516088)\u001b[0;0m INFO 09-17 18:22:52 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:22:52 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516087)\u001b[0;0m INFO 09-17 18:22:52 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:22:52 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2734.51it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 155.28it/s, est. speed input: 8123.85 toks/s, output: 417.26 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_1_0.01/checkpoint-4950...\n",
      "INFO 09-17 18:23:28 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:23:28 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:23:29 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:23:29 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:23:29 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:23:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_6bdbbd54'), local_subscribe_addr='ipc:///tmp/3ade1cfe-2728-4b66-b6b8-0bb99ba56a96', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m INFO 09-17 18:23:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_117b628c'), local_subscribe_addr='ipc:///tmp/dbdcbfa4-b331-445d-940b-f73f8a35215b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m INFO 09-17 18:23:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9cfccdfd'), local_subscribe_addr='ipc:///tmp/8f39001e-a4c8-45f2-b3bd-9c57d29815ff', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m INFO 09-17 18:23:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e6325078'), local_subscribe_addr='ipc:///tmp/15ecf91a-a03c-4ba4-9655-48890db87941', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m INFO 09-17 18:23:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4fdfc469'), local_subscribe_addr='ipc:///tmp/1d4387b5-d93d-4805-a447-cba70e08c50f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m INFO 09-17 18:23:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m INFO 09-17 18:23:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:23:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:23:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:23:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m INFO 09-17 18:23:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m INFO 09-17 18:23:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:23:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m WARNING 09-17 18:23:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:23:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m WARNING 09-17 18:23:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m WARNING 09-17 18:23:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m INFO 09-17 18:23:33 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_279b1640'), local_subscribe_addr='ipc:///tmp/699d4550-9f88-4f03-9dd7-54b0ee913651', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m INFO 09-17 18:23:33 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:23:33 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 18:23:33 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:23:33 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m WARNING 09-17 18:23:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:23:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:23:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:23:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m INFO 09-17 18:23:33 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m INFO 09-17 18:23:33 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:23:33 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m INFO 09-17 18:23:33 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m INFO 09-17 18:23:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m INFO 09-17 18:23:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m INFO 09-17 18:23:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m INFO 09-17 18:23:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 09-17 18:23:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m INFO 09-17 18:23:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m INFO 09-17 18:23:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m INFO 09-17 18:23:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m INFO 09-17 18:23:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m INFO 09-17 18:23:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m INFO 09-17 18:23:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m INFO 09-17 18:23:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:09,  1.80s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.73s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.68s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.63s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m INFO 09-17 18:23:43 [default_loader.py:262] Loading weights took 9.55 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m INFO 09-17 18:23:44 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.53s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.59s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m INFO 09-17 18:23:44 [default_loader.py:262] Loading weights took 9.62 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m INFO 09-17 18:23:44 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m INFO 09-17 18:23:44 [default_loader.py:262] Loading weights took 9.64 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m INFO 09-17 18:23:44 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m INFO 09-17 18:23:44 [default_loader.py:262] Loading weights took 9.85 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m INFO 09-17 18:23:44 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m INFO 09-17 18:23:44 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.196420 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m INFO 09-17 18:23:45 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.407219 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m INFO 09-17 18:23:45 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.413345 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m INFO 09-17 18:23:45 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.492755 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m INFO 09-17 18:24:01 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m INFO 09-17 18:24:01 [backends.py:541] Dynamo bytecode transform time: 15.44 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m INFO 09-17 18:24:01 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m INFO 09-17 18:24:01 [backends.py:541] Dynamo bytecode transform time: 15.80 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m INFO 09-17 18:24:01 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m INFO 09-17 18:24:01 [backends.py:541] Dynamo bytecode transform time: 15.87 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m INFO 09-17 18:24:01 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m INFO 09-17 18:24:01 [backends.py:541] Dynamo bytecode transform time: 16.03 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m INFO 09-17 18:24:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.571 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m INFO 09-17 18:24:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.329 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m INFO 09-17 18:24:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.495 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m INFO 09-17 18:24:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.732 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m INFO 09-17 18:24:17 [monitor.py:34] torch.compile takes 16.03 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m INFO 09-17 18:24:17 [monitor.py:34] torch.compile takes 15.80 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m INFO 09-17 18:24:17 [monitor.py:34] torch.compile takes 15.44 s in total\n",
      "INFO 09-17 18:24:17 [monitor.py:34] torch.compile takes 15.87 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m INFO 09-17 18:24:18 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m INFO 09-17 18:24:18 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m INFO 09-17 18:24:18 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m INFO 09-17 18:24:19 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:24:19 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:24:19 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:24:19 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:24:19 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:24:19 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:24:19 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:24:19 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:24:19 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m INFO 09-17 18:24:23 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m INFO 09-17 18:24:23 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m INFO 09-17 18:24:23 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:24:23 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:24:23 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.35 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_1_0.01/checkpoint-4950 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<43:36,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=516968)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=516970)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=516967)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=516969)\u001b[0;0m INFO 09-17 18:24:25 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:24:25 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:24:25 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:24:25 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2675.00it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 161.31it/s, est. speed input: 8439.25 toks/s, output: 423.84 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_1_0.9/checkpoint-500...\n",
      "INFO 09-17 18:24:59 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:24:59 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:25:00 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:25:00 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:25:00 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:25:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_8747c52e'), local_subscribe_addr='ipc:///tmp/48e9f4e6-6d98-4b2d-8a54-30856bbe6ac2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m INFO 09-17 18:25:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f9304ee6'), local_subscribe_addr='ipc:///tmp/42ac51d4-248c-4e83-a4dc-fd8b91d261ac', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-17 18:25:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d40b8187'), local_subscribe_addr='ipc:///tmp/0287a828-2b9e-4b5e-85cf-1ce9a4362e61', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m INFO 09-17 18:25:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3b289eda'), local_subscribe_addr='ipc:///tmp/13d2c5fb-cac8-419b-ac75-8e4da00f1cc7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m INFO 09-17 18:25:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e098fde9'), local_subscribe_addr='ipc:///tmp/1e9bac7a-55be-4905-b55e-7765f431d308', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m INFO 09-17 18:25:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:25:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:25:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:25:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m INFO 09-17 18:25:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:25:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:25:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:25:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m WARNING 09-17 18:25:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m WARNING 09-17 18:25:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m WARNING 09-17 18:25:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:25:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m INFO 09-17 18:25:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_0b683385'), local_subscribe_addr='ipc:///tmp/155e3245-8319-4e17-b0ca-d4d3756b50ae', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m INFO 09-17 18:25:05 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:25:05 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 18:25:05 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:25:05 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m WARNING 09-17 18:25:05 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m WARNING 09-17 18:25:05 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:25:05 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:25:05 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m INFO 09-17 18:25:05 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:25:05 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:25:05 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m INFO 09-17 18:25:05 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m INFO 09-17 18:25:05 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m INFO 09-17 18:25:05 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m INFO 09-17 18:25:06 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m INFO 09-17 18:25:06 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m INFO 09-17 18:25:06 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m INFO 09-17 18:25:06 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m INFO 09-17 18:25:06 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m INFO 09-17 18:25:06 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m INFO 09-17 18:25:06 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m INFO 09-17 18:25:06 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m INFO 09-17 18:25:06 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m INFO 09-17 18:25:06 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.62s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.68s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.68s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.66s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.55s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.63s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.63s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m INFO 09-17 18:25:16 [default_loader.py:262] Loading weights took 9.85 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m INFO 09-17 18:25:16 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m INFO 09-17 18:25:16 [default_loader.py:262] Loading weights took 10.03 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m INFO 09-17 18:25:16 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m INFO 09-17 18:25:16 [default_loader.py:262] Loading weights took 10.00 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m INFO 09-17 18:25:16 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m INFO 09-17 18:25:16 [default_loader.py:262] Loading weights took 10.47 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m INFO 09-17 18:25:16 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m INFO 09-17 18:25:17 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.552382 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m INFO 09-17 18:25:17 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.707136 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m INFO 09-17 18:25:17 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.086962 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m INFO 09-17 18:25:18 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.779372 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m INFO 09-17 18:25:34 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m INFO 09-17 18:25:34 [backends.py:541] Dynamo bytecode transform time: 15.47 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m INFO 09-17 18:25:34 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m INFO 09-17 18:25:34 [backends.py:541] Dynamo bytecode transform time: 15.83 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m INFO 09-17 18:25:34 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m INFO 09-17 18:25:34 [backends.py:541] Dynamo bytecode transform time: 15.95 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m INFO 09-17 18:25:35 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m INFO 09-17 18:25:35 [backends.py:541] Dynamo bytecode transform time: 16.14 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m INFO 09-17 18:25:43 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.268 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m INFO 09-17 18:25:44 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.480 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m INFO 09-17 18:25:44 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.529 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m INFO 09-17 18:25:45 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.552 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m INFO 09-17 18:25:50 [monitor.py:34] torch.compile takes 15.83 s in total\n",
      "INFO 09-17 18:25:50 [monitor.py:34] torch.compile takes 15.47 s in total\n",
      "INFO 09-17 18:25:50 [monitor.py:34] torch.compile takes 15.95 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m INFO 09-17 18:25:50 [monitor.py:34] torch.compile takes 16.14 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m INFO 09-17 18:25:51 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m INFO 09-17 18:25:51 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m INFO 09-17 18:25:51 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m INFO 09-17 18:25:52 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:25:53 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:25:53 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:25:53 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:25:53 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:25:53 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:25:53 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:25:53 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:25:53 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m INFO 09-17 18:25:57 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m INFO 09-17 18:25:57 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m INFO 09-17 18:25:57 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m INFO 09-17 18:25:57 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:25:57 [core.py:193] init engine (profile, create kv cache, warmup model) took 39.19 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_1_0.9/checkpoint-500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<42:36,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=517826)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=517829)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=517828)\u001b[0;0m INFO 09-17 18:25:58 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=517827)\u001b[0;0m INFO 09-17 18:25:58 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:25:58 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:25:58 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2792.91it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 159.57it/s, est. speed input: 8348.03 toks/s, output: 430.59 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_2_0.01/checkpoint-4950...\n",
      "INFO 09-17 18:26:34 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:26:34 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:26:34 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:26:34 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:26:34 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:26:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_e376d522'), local_subscribe_addr='ipc:///tmp/28be1615-813a-437d-a392-bc47941215d8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m INFO 09-17 18:26:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4ddfcdd7'), local_subscribe_addr='ipc:///tmp/d11837c9-a7d4-4d8c-9480-a1959859356e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m INFO 09-17 18:26:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c7fed2a5'), local_subscribe_addr='ipc:///tmp/d5533c93-f346-4676-84c0-b852c44679b4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m INFO 09-17 18:26:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a9c35616'), local_subscribe_addr='ipc:///tmp/6aecca05-31db-45be-b33a-3c84a7e9b56d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m INFO 09-17 18:26:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_86644642'), local_subscribe_addr='ipc:///tmp/e2ff8187-39da-4143-8e86-0006090a0520', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m INFO 09-17 18:26:38 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:26:38 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:26:38 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:26:38 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m INFO 09-17 18:26:38 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:26:38 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:26:38 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:26:38 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m WARNING 09-17 18:26:39 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:26:39 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m WARNING 09-17 18:26:39 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m WARNING 09-17 18:26:39 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m INFO 09-17 18:26:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_680c9c8a'), local_subscribe_addr='ipc:///tmp/1f679995-4931-493d-bee1-898ce490d3f3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m INFO 09-17 18:26:39 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:26:39 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:26:39 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:26:39 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m WARNING 09-17 18:26:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:26:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m WARNING 09-17 18:26:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:26:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m INFO 09-17 18:26:39 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m INFO 09-17 18:26:39 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:26:39 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m INFO 09-17 18:26:39 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m INFO 09-17 18:26:39 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m INFO 09-17 18:26:39 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m INFO 09-17 18:26:39 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m INFO 09-17 18:26:39 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m INFO 09-17 18:26:39 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 09-17 18:26:39 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m INFO 09-17 18:26:39 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m INFO 09-17 18:26:39 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m INFO 09-17 18:26:40 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m INFO 09-17 18:26:40 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m INFO 09-17 18:26:40 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m INFO 09-17 18:26:40 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:13,  2.72s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:05<00:11,  2.80s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:08<00:08,  2.96s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:11<00:05,  2.94s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:14<00:02,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m INFO 09-17 18:26:56 [default_loader.py:262] Loading weights took 16.74 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m INFO 09-17 18:26:56 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m INFO 09-17 18:26:57 [default_loader.py:262] Loading weights took 16.71 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m INFO 09-17 18:26:57 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:16<00:00,  2.68s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:16<00:00,  2.77s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m INFO 09-17 18:26:57 [default_loader.py:262] Loading weights took 16.72 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m INFO 09-17 18:26:57 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m INFO 09-17 18:26:57 [default_loader.py:262] Loading weights took 16.66 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m INFO 09-17 18:26:57 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m INFO 09-17 18:26:57 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 17.402448 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m INFO 09-17 18:26:57 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 17.411794 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m INFO 09-17 18:26:58 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 17.504321 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m INFO 09-17 18:26:58 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 17.672125 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m INFO 09-17 18:27:14 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m INFO 09-17 18:27:14 [backends.py:541] Dynamo bytecode transform time: 15.70 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m INFO 09-17 18:27:14 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m INFO 09-17 18:27:14 [backends.py:541] Dynamo bytecode transform time: 15.87 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m INFO 09-17 18:27:14 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m INFO 09-17 18:27:14 [backends.py:541] Dynamo bytecode transform time: 15.93 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m INFO 09-17 18:27:14 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m INFO 09-17 18:27:14 [backends.py:541] Dynamo bytecode transform time: 15.99 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m INFO 09-17 18:27:24 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.427 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m INFO 09-17 18:27:24 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.456 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m INFO 09-17 18:27:24 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.402 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m INFO 09-17 18:27:24 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.558 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m INFO 09-17 18:27:29 [monitor.py:34] torch.compile takes 15.87 s in total\n",
      "INFO 09-17 18:27:29 [monitor.py:34] torch.compile takes 15.93 s in total\n",
      "INFO 09-17 18:27:29 [monitor.py:34] torch.compile takes 15.70 s in total\n",
      "INFO 09-17 18:27:29 [monitor.py:34] torch.compile takes 15.99 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m INFO 09-17 18:27:31 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m INFO 09-17 18:27:31 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m INFO 09-17 18:27:31 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m INFO 09-17 18:27:31 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:27:31 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:27:31 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:27:31 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:27:31 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:27:31 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:27:31 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:27:31 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:27:31 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m INFO 09-17 18:27:35 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m INFO 09-17 18:27:35 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:27:35 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m INFO 09-17 18:27:35 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:27:36 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.94 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_2_0.01/checkpoint-4950 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<48:57,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=518690)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=518693)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=518691)\u001b[0;0m INFO 09-17 18:27:37 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:27:37 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=518692)\u001b[0;0m INFO 09-17 18:27:37 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:27:37 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 2458.09it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:29<00:00, 167.18it/s, est. speed input: 8746.28 toks/s, output: 439.47 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.001/checkpoint-4995...\n",
      "INFO 09-17 18:28:10 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:28:10 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:28:11 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:28:11 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:28:11 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:28:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_5dbb017d'), local_subscribe_addr='ipc:///tmp/71a80152-bcbd-483f-9050-d278a0fd0cef', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m INFO 09-17 18:28:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_028b6b3e'), local_subscribe_addr='ipc:///tmp/5c6f4f5d-f5ad-40fb-8f7f-aab088d824f0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m INFO 09-17 18:28:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7f3e4012'), local_subscribe_addr='ipc:///tmp/e7383fcc-503b-4896-8638-98bdcc8104d6', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m INFO 09-17 18:28:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_28aaad82'), local_subscribe_addr='ipc:///tmp/cbe74f9a-058a-463d-92f9-ccc917c1befa', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m INFO 09-17 18:28:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ada5997d'), local_subscribe_addr='ipc:///tmp/235184cc-9f2f-409f-a987-b9ccaa100e3f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m INFO 09-17 18:28:14 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:28:14 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m INFO 09-17 18:28:14 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m INFO 09-17 18:28:14 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m INFO 09-17 18:28:14 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:28:14 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m INFO 09-17 18:28:14 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:28:14 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m WARNING 09-17 18:28:15 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:28:15 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m WARNING 09-17 18:28:15 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:28:15 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m INFO 09-17 18:28:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_ddd93379'), local_subscribe_addr='ipc:///tmp/c2018c02-d934-4150-85b3-f05d1362c706', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m INFO 09-17 18:28:15 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:28:15 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:28:15 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:28:15 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m WARNING 09-17 18:28:15 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m WARNING 09-17 18:28:15 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:28:15 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m WARNING 09-17 18:28:15 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 18:28:15 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m INFO 09-17 18:28:15 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:28:15 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m INFO 09-17 18:28:15 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m INFO 09-17 18:28:15 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m INFO 09-17 18:28:15 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m INFO 09-17 18:28:15 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m INFO 09-17 18:28:15 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m INFO 09-17 18:28:15 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m INFO 09-17 18:28:16 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m INFO 09-17 18:28:16 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m INFO 09-17 18:28:16 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m INFO 09-17 18:28:16 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m INFO 09-17 18:28:16 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m INFO 09-17 18:28:16 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m INFO 09-17 18:28:16 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:07,  1.51s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:02<00:05,  1.48s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.51s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:05<00:02,  1.44s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:06<00:01,  1.33s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.29s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.36s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m INFO 09-17 18:28:24 [default_loader.py:262] Loading weights took 8.24 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m INFO 09-17 18:28:24 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m INFO 09-17 18:28:25 [default_loader.py:262] Loading weights took 8.61 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m INFO 09-17 18:28:25 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m INFO 09-17 18:28:25 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 8.806749 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m INFO 09-17 18:28:25 [default_loader.py:262] Loading weights took 8.84 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m INFO 09-17 18:28:25 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m INFO 09-17 18:28:25 [default_loader.py:262] Loading weights took 8.87 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m INFO 09-17 18:28:25 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m INFO 09-17 18:28:25 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.288303 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m INFO 09-17 18:28:26 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.464221 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m INFO 09-17 18:28:26 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.654166 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m INFO 09-17 18:28:42 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m INFO 09-17 18:28:42 [backends.py:541] Dynamo bytecode transform time: 15.67 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m INFO 09-17 18:28:42 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m INFO 09-17 18:28:42 [backends.py:541] Dynamo bytecode transform time: 15.94 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m INFO 09-17 18:28:42 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m INFO 09-17 18:28:42 [backends.py:541] Dynamo bytecode transform time: 16.03 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m INFO 09-17 18:28:43 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m INFO 09-17 18:28:43 [backends.py:541] Dynamo bytecode transform time: 16.15 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m INFO 09-17 18:28:52 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.457 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m INFO 09-17 18:28:52 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.418 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m INFO 09-17 18:28:53 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.543 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m INFO 09-17 18:28:53 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.795 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m INFO 09-17 18:28:58 [monitor.py:34] torch.compile takes 16.03 s in total\n",
      "INFO 09-17 18:28:58 [monitor.py:34] torch.compile takes 15.94 s in total\n",
      "INFO 09-17 18:28:58 [monitor.py:34] torch.compile takes 15.67 s in total\n",
      "INFO 09-17 18:28:58 [monitor.py:34] torch.compile takes 16.15 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m INFO 09-17 18:29:00 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m INFO 09-17 18:29:00 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m INFO 09-17 18:29:00 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m INFO 09-17 18:29:00 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:29:00 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:29:00 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:29:00 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:29:00 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:29:00 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:29:00 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:29:00 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:29:00 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m INFO 09-17 18:29:04 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m INFO 09-17 18:29:04 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m INFO 09-17 18:29:04 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m INFO 09-17 18:29:04 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:29:04 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.70 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/bottom_indices_misaligned_0.001/checkpoint-4995 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<45:38,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=519568)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=519566)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=519567)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=519569)\u001b[0;0m INFO 09-17 18:29:06 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:29:06 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:29:06 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:29:06 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2693.63it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:35<00:00, 139.08it/s, est. speed input: 7276.16 toks/s, output: 386.99 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W917 18:29:44.519763552 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=205, addr=[::ffff:127.0.0.1]:46394, remote=[::ffff:127.0.0.1]:56533): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7225261785e8 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8bfe (0x7224fe97fbfe in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baaf40 (0x7224fe981f40 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab84a (0x7224fe98284a in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7224fe97c2a9 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7224c007b9f9 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdc253 (0x722538df9253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7225396feac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #8: clone + 0x44 (0x72253978fbf4 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[W917 18:29:44.539643641 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 1] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_0_0.1/checkpoint-4500...\n",
      "INFO 09-17 18:29:45 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:29:45 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:29:46 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:29:46 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:29:46 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:29:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_e96f1395'), local_subscribe_addr='ipc:///tmp/b842c508-58b7-4cdb-b4a4-db6cb29a6fa0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m INFO 09-17 18:29:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7157f778'), local_subscribe_addr='ipc:///tmp/d957ad5d-f88d-43ef-aff8-69f541ff28dd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m INFO 09-17 18:29:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_06d7de3c'), local_subscribe_addr='ipc:///tmp/c2e1e6b7-252b-4bc9-bf43-aa2ceecfe488', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m INFO 09-17 18:29:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7f9c1dec'), local_subscribe_addr='ipc:///tmp/76257584-34b9-43a6-8f62-49cd0d87bb69', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m INFO 09-17 18:29:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1f348d45'), local_subscribe_addr='ipc:///tmp/dc3fe5dd-f7d7-4468-8aa5-9b7758f1134b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m INFO 09-17 18:29:49 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:29:49 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:29:49 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:29:49 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m INFO 09-17 18:29:49 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:29:49 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:29:49 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:29:49 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m WARNING 09-17 18:29:49 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:29:49 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m WARNING 09-17 18:29:49 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:29:49 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m INFO 09-17 18:29:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_08c7c5bf'), local_subscribe_addr='ipc:///tmp/907422a9-b94d-40fc-8b6d-3669b72ff839', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m INFO 09-17 18:29:50 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:29:50 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:29:50 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:29:50 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m WARNING 09-17 18:29:50 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:29:50 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m WARNING 09-17 18:29:50 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m INFO 09-17 18:29:50 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:29:50 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m INFO 09-17 18:29:50 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m WARNING 09-17 18:29:50 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m INFO 09-17 18:29:50 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m INFO 09-17 18:29:50 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m INFO 09-17 18:29:50 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m INFO 09-17 18:29:50 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m INFO 09-17 18:29:50 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m INFO 09-17 18:29:50 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m INFO 09-17 18:29:50 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m INFO 09-17 18:29:50 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m INFO 09-17 18:29:50 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m INFO 09-17 18:29:50 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m INFO 09-17 18:29:50 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m INFO 09-17 18:29:50 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m INFO 09-17 18:29:51 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.64s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.66s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.87s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:04,  2.04s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:09<00:02,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m INFO 09-17 18:30:01 [default_loader.py:262] Loading weights took 10.06 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m INFO 09-17 18:30:01 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m INFO 09-17 18:30:02 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.833061 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m INFO 09-17 18:30:02 [default_loader.py:262] Loading weights took 11.09 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m INFO 09-17 18:30:02 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m INFO 09-17 18:30:02 [default_loader.py:262] Loading weights took 11.21 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m INFO 09-17 18:30:02 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.92s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.91s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m INFO 09-17 18:30:02 [default_loader.py:262] Loading weights took 11.50 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m INFO 09-17 18:30:02 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.720172 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m INFO 09-17 18:30:02 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m INFO 09-17 18:30:03 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.129512 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m INFO 09-17 18:30:03 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.160580 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m INFO 09-17 18:30:19 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m INFO 09-17 18:30:19 [backends.py:541] Dynamo bytecode transform time: 15.66 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m INFO 09-17 18:30:20 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m INFO 09-17 18:30:20 [backends.py:541] Dynamo bytecode transform time: 16.10 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m INFO 09-17 18:30:20 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m INFO 09-17 18:30:20 [backends.py:541] Dynamo bytecode transform time: 16.18 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m INFO 09-17 18:30:20 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m INFO 09-17 18:30:20 [backends.py:541] Dynamo bytecode transform time: 16.23 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m INFO 09-17 18:30:29 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.552 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m INFO 09-17 18:30:30 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.286 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m INFO 09-17 18:30:30 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.623 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m INFO 09-17 18:30:30 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.589 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m INFO 09-17 18:30:35 [monitor.py:34] torch.compile takes 15.66 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m INFO 09-17 18:30:35 [monitor.py:34] torch.compile takes 16.23 s in total\n",
      "INFO 09-17 18:30:35 [monitor.py:34] torch.compile takes 16.10 s in total\n",
      "INFO 09-17 18:30:35 [monitor.py:34] torch.compile takes 16.18 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m INFO 09-17 18:30:36 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m INFO 09-17 18:30:37 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m INFO 09-17 18:30:37 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m INFO 09-17 18:30:37 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:30:37 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:30:37 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:30:37 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:30:37 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:30:37 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:30:37 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:30:37 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:30:37 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m INFO 09-17 18:30:41 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:30:41 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m INFO 09-17 18:30:41 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m INFO 09-17 18:30:41 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:30:41 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.21 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_0_0.1/checkpoint-4500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<48:23,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=520427)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=520428)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=520430)\u001b[0;0m INFO 09-17 18:30:43 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:30:43 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:30:43 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=520429)\u001b[0;0m INFO 09-17 18:30:43 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2668.13it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 153.40it/s, est. speed input: 8025.55 toks/s, output: 416.81 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_0_0.8/checkpoint-1000...\n",
      "INFO 09-17 18:31:18 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:31:18 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:31:19 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:31:19 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:31:19 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:31:19 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_24bcaee4'), local_subscribe_addr='ipc:///tmp/82776b28-81ea-4562-86a8-8d1d3a5e412d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m INFO 09-17 18:31:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3107fe22'), local_subscribe_addr='ipc:///tmp/00382a4f-e4c6-4383-ada1-55af7b44fa7f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m INFO 09-17 18:31:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_deb07ef9'), local_subscribe_addr='ipc:///tmp/7369dfc3-7b45-4d84-bc39-da267bcbfad9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m INFO 09-17 18:31:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_dd2b2f2c'), local_subscribe_addr='ipc:///tmp/ca66a202-c5d5-4e5e-b5e6-919a7c989837', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m INFO 09-17 18:31:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d737c84d'), local_subscribe_addr='ipc:///tmp/44b816d8-11ae-4ef0-9f74-e1e147cd28bb', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m INFO 09-17 18:31:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:31:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m INFO 09-17 18:31:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m INFO 09-17 18:31:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:31:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m INFO 09-17 18:31:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m INFO 09-17 18:31:23 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m INFO 09-17 18:31:23 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m WARNING 09-17 18:31:23 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:31:23 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m WARNING 09-17 18:31:23 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:31:23 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m INFO 09-17 18:31:23 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_88220f10'), local_subscribe_addr='ipc:///tmp/0294c3d0-d551-48c8-8828-38279c9b24cf', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m INFO 09-17 18:31:23 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:31:23 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:31:23 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 18:31:23 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m WARNING 09-17 18:31:23 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m WARNING 09-17 18:31:23 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m WARNING 09-17 18:31:23 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 18:31:23 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m WARNING 09-17 18:31:23 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m INFO 09-17 18:31:23 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:31:23 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m INFO 09-17 18:31:23 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m INFO 09-17 18:31:24 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m INFO 09-17 18:31:24 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m INFO 09-17 18:31:24 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m INFO 09-17 18:31:24 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m INFO 09-17 18:31:24 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m INFO 09-17 18:31:24 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m INFO 09-17 18:31:24 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m INFO 09-17 18:31:24 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m INFO 09-17 18:31:24 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m INFO 09-17 18:31:24 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m INFO 09-17 18:31:24 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m INFO 09-17 18:31:24 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:11,  2.32s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:09,  2.29s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:07,  2.35s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:09<00:04,  2.36s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:11<00:02,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m INFO 09-17 18:31:37 [default_loader.py:262] Loading weights took 12.43 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m INFO 09-17 18:31:37 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m INFO 09-17 18:31:37 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.005922 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 18:31:37 [default_loader.py:262] Loading weights took 13.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:12<00:00,  2.03s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:12<00:00,  2.16s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m INFO 09-17 18:31:37 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m INFO 09-17 18:31:37 [default_loader.py:262] Loading weights took 13.04 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m INFO 09-17 18:31:37 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m INFO 09-17 18:31:38 [default_loader.py:262] Loading weights took 13.45 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m INFO 09-17 18:31:38 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m INFO 09-17 18:31:38 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.855690 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m INFO 09-17 18:31:38 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.915097 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m INFO 09-17 18:31:39 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.190481 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m INFO 09-17 18:31:55 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m INFO 09-17 18:31:55 [backends.py:541] Dynamo bytecode transform time: 15.88 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m INFO 09-17 18:31:55 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m INFO 09-17 18:31:55 [backends.py:541] Dynamo bytecode transform time: 16.18 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m INFO 09-17 18:31:55 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m INFO 09-17 18:31:55 [backends.py:541] Dynamo bytecode transform time: 16.17 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m INFO 09-17 18:31:56 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m INFO 09-17 18:31:56 [backends.py:541] Dynamo bytecode transform time: 16.21 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m INFO 09-17 18:32:05 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.465 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m INFO 09-17 18:32:05 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.516 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m INFO 09-17 18:32:05 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.590 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m INFO 09-17 18:32:06 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.807 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m INFO 09-17 18:32:11 [monitor.py:34] torch.compile takes 16.17 s in total\n",
      "INFO 09-17 18:32:11 [monitor.py:34] torch.compile takes 15.88 s in total\n",
      "INFO 09-17 18:32:11 [monitor.py:34] torch.compile takes 16.18 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m INFO 09-17 18:32:11 [monitor.py:34] torch.compile takes 16.21 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m INFO 09-17 18:32:13 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m INFO 09-17 18:32:13 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m INFO 09-17 18:32:13 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m INFO 09-17 18:32:13 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:32:13 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:32:13 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:32:13 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:32:13 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:32:13 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:32:13 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:32:13 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:32:13 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m INFO 09-17 18:32:17 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 18:32:17 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m INFO 09-17 18:32:17 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m INFO 09-17 18:32:17 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:32:17 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.63 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_0_0.8/checkpoint-1000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<45:36,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=521290)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=521291)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=521287)\u001b[0;0m INFO 09-17 18:32:19 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:32:19 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=521289)\u001b[0;0m INFO 09-17 18:32:19 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:32:19 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2697.24it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 157.68it/s, est. speed input: 8249.44 toks/s, output: 425.70 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_2_0.99/checkpoint-50...\n",
      "INFO 09-17 18:32:54 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:32:54 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:32:55 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:32:55 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:32:55 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:32:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_73c5f0f2'), local_subscribe_addr='ipc:///tmp/2e9ee7d0-1e21-41bd-a4fe-c766fb446f80', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m INFO 09-17 18:32:57 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f7022ba2'), local_subscribe_addr='ipc:///tmp/ba878e68-3cbd-4113-82bb-e42b170ddb0c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m INFO 09-17 18:32:57 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_29427674'), local_subscribe_addr='ipc:///tmp/c2d537e1-e52b-45e0-a978-b4c429d0ca22', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m INFO 09-17 18:32:57 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a48e4b3e'), local_subscribe_addr='ipc:///tmp/44c99002-f8a4-4c3b-b11f-f603f2f5ad4a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m INFO 09-17 18:32:57 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e693dd16'), local_subscribe_addr='ipc:///tmp/e39f08b2-064f-4eda-bce9-31647df9bfd9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m INFO 09-17 18:32:58 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:32:58 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m INFO 09-17 18:32:58 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:32:58 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m INFO 09-17 18:32:58 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:32:58 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m INFO 09-17 18:32:58 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:32:58 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m WARNING 09-17 18:32:58 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m WARNING 09-17 18:32:58 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m WARNING 09-17 18:32:58 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m WARNING 09-17 18:32:58 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m INFO 09-17 18:32:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_7a7691a6'), local_subscribe_addr='ipc:///tmp/d7d4f449-6a75-4c99-9dae-b78ab9702c60', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m INFO 09-17 18:32:58 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m INFO 09-17 18:32:58 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:32:58 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 18:32:58 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m WARNING 09-17 18:32:58 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m WARNING 09-17 18:32:58 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:32:58 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m INFO 09-17 18:32:58 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "WARNING 09-17 18:32:58 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m INFO 09-17 18:32:58 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:32:58 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m INFO 09-17 18:32:58 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m INFO 09-17 18:32:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m INFO 09-17 18:32:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m INFO 09-17 18:32:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m INFO 09-17 18:32:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m INFO 09-17 18:32:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m INFO 09-17 18:32:59 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m INFO 09-17 18:32:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m INFO 09-17 18:32:59 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m INFO 09-17 18:32:59 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m INFO 09-17 18:32:59 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m INFO 09-17 18:32:59 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m INFO 09-17 18:32:59 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:10,  2.20s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:08,  2.16s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.91s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:03,  1.59s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.38s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.26s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.51s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m INFO 09-17 18:33:08 [default_loader.py:262] Loading weights took 9.15 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m INFO 09-17 18:33:08 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m INFO 09-17 18:33:09 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.696805 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m INFO 09-17 18:33:10 [default_loader.py:262] Loading weights took 11.00 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m INFO 09-17 18:33:10 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m INFO 09-17 18:33:11 [default_loader.py:262] Loading weights took 11.56 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m INFO 09-17 18:33:11 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m INFO 09-17 18:33:11 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.588028 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m INFO 09-17 18:33:11 [default_loader.py:262] Loading weights took 11.70 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m INFO 09-17 18:33:11 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m INFO 09-17 18:33:12 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.342560 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m INFO 09-17 18:33:12 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.423082 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m INFO 09-17 18:33:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m INFO 09-17 18:33:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "INFO 09-17 18:33:29 [backends.py:541] Dynamo bytecode transform time: 16.00 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m INFO 09-17 18:33:29 [backends.py:541] Dynamo bytecode transform time: 16.03 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m INFO 09-17 18:33:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m INFO 09-17 18:33:29 [backends.py:541] Dynamo bytecode transform time: 16.13 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m INFO 09-17 18:33:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m INFO 09-17 18:33:29 [backends.py:541] Dynamo bytecode transform time: 15.93 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m INFO 09-17 18:33:38 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.383 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m INFO 09-17 18:33:38 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.364 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m INFO 09-17 18:33:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.500 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m INFO 09-17 18:33:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.705 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m INFO 09-17 18:33:44 [monitor.py:34] torch.compile takes 16.03 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m INFO 09-17 18:33:44 [monitor.py:34] torch.compile takes 16.00 s in total\n",
      "INFO 09-17 18:33:44 [monitor.py:34] torch.compile takes 16.13 s in total\n",
      "INFO 09-17 18:33:44 [monitor.py:34] torch.compile takes 15.93 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m INFO 09-17 18:33:46 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m INFO 09-17 18:33:46 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m INFO 09-17 18:33:46 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m INFO 09-17 18:33:46 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:33:46 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:33:46 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:33:46 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:33:46 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:33:46 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:33:46 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:33:46 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:33:46 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m INFO 09-17 18:33:50 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:33:50 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m INFO 09-17 18:33:50 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m INFO 09-17 18:33:50 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:33:50 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.32 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_2_0.99/checkpoint-50 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<46:57,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=522161)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=522159)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=522160)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=522162)\u001b[0;0m INFO 09-17 18:33:53 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:33:53 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:33:53 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:33:53 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 1918.78it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:34<00:00, 143.16it/s, est. speed input: 7489.33 toks/s, output: 403.37 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_2_0.1/checkpoint-4500...\n",
      "INFO 09-17 18:34:31 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:34:31 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:34:32 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:34:32 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:34:32 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:34:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_fa8c9405'), local_subscribe_addr='ipc:///tmp/183914ec-8310-4a0c-a9b8-8b20b1431ad4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m INFO 09-17 18:34:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6bc3f3f1'), local_subscribe_addr='ipc:///tmp/e6ecebfa-7de9-42bf-b9ef-bdd01d222e4f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m INFO 09-17 18:34:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_42492c72'), local_subscribe_addr='ipc:///tmp/cdf40927-8254-4d00-baad-22644ab903d9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m INFO 09-17 18:34:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b92c3a0d'), local_subscribe_addr='ipc:///tmp/2c633531-2add-4cb7-8778-991a96885c70', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m INFO 09-17 18:34:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_306c4560'), local_subscribe_addr='ipc:///tmp/42a9de5f-adc8-46ba-8a52-01b85a875b9e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m INFO 09-17 18:34:35 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:34:35 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:34:35 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:34:35 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m INFO 09-17 18:34:35 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:34:35 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:34:35 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:34:35 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m WARNING 09-17 18:34:35 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:34:35 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m WARNING 09-17 18:34:35 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m WARNING 09-17 18:34:35 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m INFO 09-17 18:34:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_9b4ca1fe'), local_subscribe_addr='ipc:///tmp/cb693977-ad38-4ae9-815c-dbbaa6efd0d5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m INFO 09-17 18:34:36 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:34:36 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:34:36 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 18:34:36 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m WARNING 09-17 18:34:36 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:34:36 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:34:36 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:34:36 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m INFO 09-17 18:34:36 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:34:36 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:34:36 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m INFO 09-17 18:34:36 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m INFO 09-17 18:34:36 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m INFO 09-17 18:34:36 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m INFO 09-17 18:34:36 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m INFO 09-17 18:34:36 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m INFO 09-17 18:34:36 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m INFO 09-17 18:34:36 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m INFO 09-17 18:34:36 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m INFO 09-17 18:34:36 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m INFO 09-17 18:34:36 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 09-17 18:34:36 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m INFO 09-17 18:34:36 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m INFO 09-17 18:34:37 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.62s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.67s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.68s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.67s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.60s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.63s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.64s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m INFO 09-17 18:34:46 [default_loader.py:262] Loading weights took 9.90 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m INFO 09-17 18:34:46 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m INFO 09-17 18:34:46 [default_loader.py:262] Loading weights took 10.15 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m INFO 09-17 18:34:47 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m INFO 09-17 18:34:47 [default_loader.py:262] Loading weights took 9.94 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m INFO 09-17 18:34:47 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m INFO 09-17 18:34:47 [default_loader.py:262] Loading weights took 9.86 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m INFO 09-17 18:34:47 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m INFO 09-17 18:34:47 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.526359 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m INFO 09-17 18:34:47 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.719856 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m INFO 09-17 18:34:47 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.606574 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m INFO 09-17 18:34:47 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.490711 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m INFO 09-17 18:35:04 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m INFO 09-17 18:35:04 [backends.py:541] Dynamo bytecode transform time: 15.52 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m INFO 09-17 18:35:04 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m INFO 09-17 18:35:04 [backends.py:541] Dynamo bytecode transform time: 15.71 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m INFO 09-17 18:35:04 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m INFO 09-17 18:35:04 [backends.py:541] Dynamo bytecode transform time: 16.19 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m INFO 09-17 18:35:04 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m INFO 09-17 18:35:04 [backends.py:541] Dynamo bytecode transform time: 16.30 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m INFO 09-17 18:35:13 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.422 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m INFO 09-17 18:35:14 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.435 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m INFO 09-17 18:35:14 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.474 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m INFO 09-17 18:35:15 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.610 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m INFO 09-17 18:35:20 [monitor.py:34] torch.compile takes 16.19 s in total\n",
      "INFO 09-17 18:35:20 [monitor.py:34] torch.compile takes 15.71 s in total\n",
      "INFO 09-17 18:35:20 [monitor.py:34] torch.compile takes 15.52 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m INFO 09-17 18:35:20 [monitor.py:34] torch.compile takes 16.30 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m INFO 09-17 18:35:21 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m INFO 09-17 18:35:21 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m INFO 09-17 18:35:21 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m INFO 09-17 18:35:22 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:35:22 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:35:22 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:35:22 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:35:22 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:35:22 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:35:22 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:35:22 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:35:22 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m INFO 09-17 18:35:26 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:35:26 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m INFO 09-17 18:35:26 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m INFO 09-17 18:35:26 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:35:26 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.99 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_2_0.1/checkpoint-4500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<41:22,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=523027)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=523026)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=523025)\u001b[0;0m INFO 09-17 18:35:28 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:35:28 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523028)\u001b[0;0m INFO 09-17 18:35:28 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:35:28 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2810.05it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 159.71it/s, est. speed input: 8355.34 toks/s, output: 439.69 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_2_0.4/checkpoint-3000...\n",
      "INFO 09-17 18:36:02 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:36:02 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:36:03 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:36:03 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:36:03 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:36:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_f75e6665'), local_subscribe_addr='ipc:///tmp/097d4832-1fb4-4469-96d0-c53a727aa2e3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m INFO 09-17 18:36:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e266bc62'), local_subscribe_addr='ipc:///tmp/6b51cf5c-a6c2-45f9-a743-fbb64ccea4e9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m INFO 09-17 18:36:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a26f153a'), local_subscribe_addr='ipc:///tmp/f911ed3b-a128-4dbf-a573-037914397579', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m INFO 09-17 18:36:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_502a1464'), local_subscribe_addr='ipc:///tmp/aee531cc-d27a-4d7b-999d-89a997a549ec', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m INFO 09-17 18:36:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_641770ea'), local_subscribe_addr='ipc:///tmp/50867364-5762-44f6-a729-7bfd064928bd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m INFO 09-17 18:36:06 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:36:06 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m INFO 09-17 18:36:06 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m INFO 09-17 18:36:06 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m INFO 09-17 18:36:06 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:36:06 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m INFO 09-17 18:36:06 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m INFO 09-17 18:36:06 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m WARNING 09-17 18:36:07 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:36:07 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m WARNING 09-17 18:36:07 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m WARNING 09-17 18:36:07 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m INFO 09-17 18:36:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_0fa53718'), local_subscribe_addr='ipc:///tmp/bd20d6bf-3519-4423-9716-eca691da7667', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m INFO 09-17 18:36:07 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m INFO 09-17 18:36:07 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:36:07 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:36:07 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m WARNING 09-17 18:36:07 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m WARNING 09-17 18:36:07 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m WARNING 09-17 18:36:07 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m WARNING 09-17 18:36:07 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 18:36:07 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m INFO 09-17 18:36:07 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m INFO 09-17 18:36:07 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m INFO 09-17 18:36:07 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m INFO 09-17 18:36:07 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m INFO 09-17 18:36:07 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m INFO 09-17 18:36:07 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m INFO 09-17 18:36:07 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m INFO 09-17 18:36:07 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 09-17 18:36:07 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m INFO 09-17 18:36:08 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m INFO 09-17 18:36:08 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m INFO 09-17 18:36:08 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m INFO 09-17 18:36:08 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m INFO 09-17 18:36:08 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m INFO 09-17 18:36:08 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.70s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.67s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.68s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.68s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m INFO 09-17 18:36:18 [default_loader.py:262] Loading weights took 9.81 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m INFO 09-17 18:36:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.61s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.64s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m INFO 09-17 18:36:18 [default_loader.py:262] Loading weights took 9.77 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m INFO 09-17 18:36:18 [default_loader.py:262] Loading weights took 9.89 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m INFO 09-17 18:36:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m INFO 09-17 18:36:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m INFO 09-17 18:36:18 [default_loader.py:262] Loading weights took 9.78 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m INFO 09-17 18:36:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m INFO 09-17 18:36:18 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.385162 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m INFO 09-17 18:36:18 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.611503 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m INFO 09-17 18:36:19 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.473254 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m INFO 09-17 18:36:19 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.588877 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m INFO 09-17 18:36:35 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m INFO 09-17 18:36:35 [backends.py:541] Dynamo bytecode transform time: 15.76 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m INFO 09-17 18:36:35 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m INFO 09-17 18:36:35 [backends.py:541] Dynamo bytecode transform time: 15.72 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m INFO 09-17 18:36:35 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m INFO 09-17 18:36:35 [backends.py:541] Dynamo bytecode transform time: 15.81 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m INFO 09-17 18:36:35 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m INFO 09-17 18:36:35 [backends.py:541] Dynamo bytecode transform time: 15.85 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m INFO 09-17 18:36:45 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.440 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m INFO 09-17 18:36:45 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.403 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m INFO 09-17 18:36:45 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.445 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m INFO 09-17 18:36:45 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.527 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m INFO 09-17 18:36:50 [monitor.py:34] torch.compile takes 15.81 s in total\n",
      "INFO 09-17 18:36:50 [monitor.py:34] torch.compile takes 15.85 s in total\n",
      "INFO 09-17 18:36:50 [monitor.py:34] torch.compile takes 15.76 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m INFO 09-17 18:36:50 [monitor.py:34] torch.compile takes 15.72 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m INFO 09-17 18:36:52 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m INFO 09-17 18:36:52 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m INFO 09-17 18:36:52 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m INFO 09-17 18:36:52 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:36:52 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:36:52 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:36:52 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:36:52 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:36:52 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:36:52 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:36:52 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:36:52 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m INFO 09-17 18:36:56 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:36:56 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m INFO 09-17 18:36:56 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m INFO 09-17 18:36:56 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:36:56 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.68 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_2_0.4/checkpoint-3000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<48:13,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=523877)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=523878)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=523880)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=523879)\u001b[0;0m INFO 09-17 18:36:58 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:36:58 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:36:58 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:36:58 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2674.38it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 161.14it/s, est. speed input: 8429.97 toks/s, output: 432.11 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.4/checkpoint-3000...\n",
      "INFO 09-17 18:37:32 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:37:32 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:37:33 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:37:33 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:37:33 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:37:33 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_e78f597f'), local_subscribe_addr='ipc:///tmp/2c195bf3-ff2c-4b76-a225-4323103d78dc', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m INFO 09-17 18:37:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_16b90aad'), local_subscribe_addr='ipc:///tmp/0b9d8d84-b7cc-49ea-acd5-dd7aedc082ee', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m INFO 09-17 18:37:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b83fae49'), local_subscribe_addr='ipc:///tmp/6ed43113-7910-4118-b673-49f5eae7abd8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m INFO 09-17 18:37:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b27fa3d7'), local_subscribe_addr='ipc:///tmp/79821bbf-2380-4c9a-8a1d-dac2d9484dad', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m INFO 09-17 18:37:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ea515e93'), local_subscribe_addr='ipc:///tmp/5122f7ed-faad-4d71-bec0-5c6456a2227b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m INFO 09-17 18:37:36 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:37:36 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:37:36 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m INFO 09-17 18:37:36 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:37:36 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:37:36 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m INFO 09-17 18:37:36 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m INFO 09-17 18:37:36 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m WARNING 09-17 18:37:37 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:37:37 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m WARNING 09-17 18:37:37 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:37:37 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m INFO 09-17 18:37:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_2e7f756a'), local_subscribe_addr='ipc:///tmp/406cda01-ca62-4e3a-a821-1c62c220f631', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m INFO 09-17 18:37:37 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:37:37 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:37:37 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 18:37:37 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m WARNING 09-17 18:37:37 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m WARNING 09-17 18:37:37 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:37:37 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:37:37 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m INFO 09-17 18:37:37 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:37:37 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m INFO 09-17 18:37:37 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m INFO 09-17 18:37:37 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m INFO 09-17 18:37:37 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m INFO 09-17 18:37:37 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m INFO 09-17 18:37:37 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m INFO 09-17 18:37:37 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m INFO 09-17 18:37:37 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 09-17 18:37:37 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m INFO 09-17 18:37:37 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m INFO 09-17 18:37:37 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m INFO 09-17 18:37:37 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m INFO 09-17 18:37:37 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m INFO 09-17 18:37:37 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m INFO 09-17 18:37:38 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:06,  1.26s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:02<00:06,  1.50s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.54s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.55s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m INFO 09-17 18:37:47 [default_loader.py:262] Loading weights took 8.98 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m INFO 09-17 18:37:47 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.46s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.48s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m INFO 09-17 18:37:47 [default_loader.py:262] Loading weights took 8.93 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m INFO 09-17 18:37:47 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m INFO 09-17 18:37:47 [default_loader.py:262] Loading weights took 8.93 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m INFO 09-17 18:37:47 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m INFO 09-17 18:37:47 [default_loader.py:262] Loading weights took 9.04 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m INFO 09-17 18:37:47 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m INFO 09-17 18:37:47 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.683976 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m INFO 09-17 18:37:47 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.578426 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m INFO 09-17 18:37:48 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.809412 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m INFO 09-17 18:37:48 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.928866 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m INFO 09-17 18:38:04 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m INFO 09-17 18:38:04 [backends.py:541] Dynamo bytecode transform time: 15.50 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m INFO 09-17 18:38:04 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m INFO 09-17 18:38:04 [backends.py:541] Dynamo bytecode transform time: 15.75 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m INFO 09-17 18:38:04 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m INFO 09-17 18:38:04 [backends.py:541] Dynamo bytecode transform time: 15.89 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m INFO 09-17 18:38:05 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m INFO 09-17 18:38:05 [backends.py:541] Dynamo bytecode transform time: 15.99 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m INFO 09-17 18:38:14 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.381 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m INFO 09-17 18:38:14 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.468 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m INFO 09-17 18:38:15 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.701 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m INFO 09-17 18:38:15 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.009 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m INFO 09-17 18:38:20 [monitor.py:34] torch.compile takes 15.89 s in total\n",
      "INFO 09-17 18:38:20 [monitor.py:34] torch.compile takes 15.75 s in total\n",
      "INFO 09-17 18:38:20 [monitor.py:34] torch.compile takes 15.50 s in total\n",
      "INFO 09-17 18:38:20 [monitor.py:34] torch.compile takes 15.99 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m INFO 09-17 18:38:22 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m INFO 09-17 18:38:22 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m INFO 09-17 18:38:22 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m INFO 09-17 18:38:22 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:38:23 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:38:23 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:38:23 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:38:23 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:38:23 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:38:23 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:38:23 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:38:23 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m INFO 09-17 18:38:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m INFO 09-17 18:38:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m INFO 09-17 18:38:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m INFO 09-17 18:38:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:38:27 [core.py:193] init engine (profile, create kv cache, warmup model) took 39.21 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.4/checkpoint-3000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<43:59,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=524721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=524720)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=524722)\u001b[0;0m INFO 09-17 18:38:29 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:38:29 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:38:29 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=524723)\u001b[0;0m INFO 09-17 18:38:29 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2847.30it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 161.66it/s, est. speed input: 8457.13 toks/s, output: 428.49 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank2]:[W917 18:39:01.076075260 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=209, addr=[::ffff:127.0.0.1]:46706, remote=[::ffff:127.0.0.1]:52373): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7225261785e8 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8bfe (0x7224fe97fbfe in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baaf40 (0x7224fe981f40 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab84a (0x7224fe98284a in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7224fe97c2a9 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7224c007b9f9 in /mnt/ssd-1/soar-data_attribution/mike/owl_numbers_try_2/subliminal-learning/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdc253 (0x722538df9253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7225396feac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #8: clone + 0x44 (0x72253978fbf4 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[W917 18:39:01.102781962 ProcessGroupNCCL.cpp:1662] [PG ID 0 PG GUID 0 Rank 2] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.999/checkpoint-5...\n",
      "INFO 09-17 18:39:03 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:39:03 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:39:03 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:39:03 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:39:03 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:39:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_773b8dba'), local_subscribe_addr='ipc:///tmp/58bf98c1-644e-4c7a-b9b1-29718bbce16d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m INFO 09-17 18:39:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9b051d41'), local_subscribe_addr='ipc:///tmp/9f18181c-0ab3-486f-aad6-69c2d1ee8a0a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m INFO 09-17 18:39:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6add82fc'), local_subscribe_addr='ipc:///tmp/0c10bab7-47e8-4841-9e5d-096d648cc867', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m INFO 09-17 18:39:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_edf357a3'), local_subscribe_addr='ipc:///tmp/461c72ba-caa3-4c05-9421-e1eb0ce17088', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m INFO 09-17 18:39:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4483a24b'), local_subscribe_addr='ipc:///tmp/96a4e629-7a5f-40e8-86c9-a30e06a877d3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m INFO 09-17 18:39:07 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m INFO 09-17 18:39:07 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m INFO 09-17 18:39:07 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:39:07 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:39:07 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m INFO 09-17 18:39:07 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m INFO 09-17 18:39:07 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:39:07 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m WARNING 09-17 18:39:07 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m WARNING 09-17 18:39:07 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m WARNING 09-17 18:39:07 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m WARNING 09-17 18:39:07 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m INFO 09-17 18:39:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_53e3067c'), local_subscribe_addr='ipc:///tmp/0913be89-5c10-4d38-a31d-f56490c82ba4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m INFO 09-17 18:39:07 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:39:07 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 18:39:07 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:39:07 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m WARNING 09-17 18:39:07 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m WARNING 09-17 18:39:07 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:39:07 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:39:07 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m INFO 09-17 18:39:07 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:39:07 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m INFO 09-17 18:39:07 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:39:07 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m INFO 09-17 18:39:07 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 09-17 18:39:07 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m INFO 09-17 18:39:07 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m INFO 09-17 18:39:07 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m INFO 09-17 18:39:08 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m INFO 09-17 18:39:08 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m INFO 09-17 18:39:08 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m INFO 09-17 18:39:08 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m INFO 09-17 18:39:08 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m INFO 09-17 18:39:08 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m INFO 09-17 18:39:08 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m INFO 09-17 18:39:08 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:07,  1.52s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.56s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.61s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.54s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.51s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.54s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.54s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m INFO 09-17 18:39:17 [default_loader.py:262] Loading weights took 9.31 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m INFO 09-17 18:39:17 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m INFO 09-17 18:39:17 [default_loader.py:262] Loading weights took 9.36 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m INFO 09-17 18:39:17 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m INFO 09-17 18:39:17 [default_loader.py:262] Loading weights took 9.27 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m INFO 09-17 18:39:17 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m INFO 09-17 18:39:18 [default_loader.py:262] Loading weights took 9.34 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m INFO 09-17 18:39:18 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m INFO 09-17 18:39:18 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.884072 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m INFO 09-17 18:39:18 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.048233 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m INFO 09-17 18:39:18 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.124295 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m INFO 09-17 18:39:19 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.076259 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m INFO 09-17 18:39:35 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m INFO 09-17 18:39:35 [backends.py:541] Dynamo bytecode transform time: 15.73 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m INFO 09-17 18:39:35 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m INFO 09-17 18:39:35 [backends.py:541] Dynamo bytecode transform time: 15.95 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m INFO 09-17 18:39:35 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m INFO 09-17 18:39:35 [backends.py:541] Dynamo bytecode transform time: 16.00 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m INFO 09-17 18:39:36 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m INFO 09-17 18:39:36 [backends.py:541] Dynamo bytecode transform time: 16.48 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m INFO 09-17 18:39:45 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.724 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m INFO 09-17 18:39:45 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.724 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m INFO 09-17 18:39:46 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.713 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m INFO 09-17 18:39:46 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.584 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m INFO 09-17 18:39:51 [monitor.py:34] torch.compile takes 15.73 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m INFO 09-17 18:39:51 [monitor.py:34] torch.compile takes 15.95 s in total\n",
      "INFO 09-17 18:39:51 [monitor.py:34] torch.compile takes 16.00 s in total\n",
      "INFO 09-17 18:39:51 [monitor.py:34] torch.compile takes 16.48 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m INFO 09-17 18:39:52 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m INFO 09-17 18:39:52 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m INFO 09-17 18:39:53 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m INFO 09-17 18:39:53 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "INFO 09-17 18:39:53 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:39:53 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:39:53 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:39:53 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:39:53 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:39:53 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:39:53 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:39:53 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m INFO 09-17 18:39:58 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m INFO 09-17 18:39:58 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:39:58 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m INFO 09-17 18:39:58 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:39:58 [core.py:193] init engine (profile, create kv cache, warmup model) took 39.17 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.999/checkpoint-5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<39:04,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=525570)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=525568)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=525567)\u001b[0;0m INFO 09-17 18:39:59 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:39:59 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=525569)\u001b[0;0m INFO 09-17 18:39:59 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:39:59 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2863.24it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:34<00:00, 146.98it/s, est. speed input: 7689.70 toks/s, output: 430.86 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/random_indices_1_0.99/checkpoint-50...\n",
      "INFO 09-17 18:40:36 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:40:36 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:40:37 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:40:37 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:40:37 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:40:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_f103f71d'), local_subscribe_addr='ipc:///tmp/3b18b4e7-dbe5-4c99-9175-64d43f49eae9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m INFO 09-17 18:40:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_80924ab3'), local_subscribe_addr='ipc:///tmp/53d7d66a-80c4-4d2c-b8fd-542e72bb09d7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m INFO 09-17 18:40:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_77608cec'), local_subscribe_addr='ipc:///tmp/edfec354-5871-4a32-8336-8f3c7bb5c5c2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m INFO 09-17 18:40:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_80ab5df5'), local_subscribe_addr='ipc:///tmp/4a32c0fd-dc43-4157-b568-64308c801912', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m INFO 09-17 18:40:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_56709781'), local_subscribe_addr='ipc:///tmp/6ff05eb3-af95-4a8c-bd5f-90926ba1dda4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m INFO 09-17 18:40:42 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:40:42 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:40:42 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m INFO 09-17 18:40:42 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m INFO 09-17 18:40:42 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m INFO 09-17 18:40:42 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:40:42 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m INFO 09-17 18:40:42 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m WARNING 09-17 18:40:43 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m WARNING 09-17 18:40:43 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m WARNING 09-17 18:40:43 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m WARNING 09-17 18:40:43 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m INFO 09-17 18:40:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_f0ebaf38'), local_subscribe_addr='ipc:///tmp/a6c3de58-4247-44f8-b5d7-226acafb0fef', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m INFO 09-17 18:40:43 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m INFO 09-17 18:40:43 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:40:43 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:40:43 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m WARNING 09-17 18:40:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m WARNING 09-17 18:40:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:40:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m WARNING 09-17 18:40:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 18:40:43 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m INFO 09-17 18:40:43 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m INFO 09-17 18:40:43 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m INFO 09-17 18:40:43 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m INFO 09-17 18:40:43 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m INFO 09-17 18:40:43 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m INFO 09-17 18:40:43 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m INFO 09-17 18:40:43 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m INFO 09-17 18:40:43 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m INFO 09-17 18:40:43 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m INFO 09-17 18:40:43 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m INFO 09-17 18:40:43 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m INFO 09-17 18:40:43 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m INFO 09-17 18:40:43 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m INFO 09-17 18:40:43 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m INFO 09-17 18:40:43 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:14,  2.98s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:05<00:11,  2.91s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:08<00:08,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m INFO 09-17 18:40:53 [default_loader.py:262] Loading weights took 10.03 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m INFO 09-17 18:40:53 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m INFO 09-17 18:40:54 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.607280 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:10<00:05,  2.51s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:12<00:02,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m INFO 09-17 18:40:58 [default_loader.py:262] Loading weights took 14.54 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m INFO 09-17 18:40:58 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:14<00:00,  2.23s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:14<00:00,  2.43s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m INFO 09-17 18:40:58 [default_loader.py:262] Loading weights took 14.63 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m INFO 09-17 18:40:58 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m INFO 09-17 18:40:59 [default_loader.py:262] Loading weights took 14.86 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m INFO 09-17 18:40:59 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m INFO 09-17 18:40:59 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.233239 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m INFO 09-17 18:40:59 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.497770 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m INFO 09-17 18:40:59 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.548370 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m INFO 09-17 18:41:16 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m INFO 09-17 18:41:16 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m INFO 09-17 18:41:16 [backends.py:541] Dynamo bytecode transform time: 16.16 s\n",
      "INFO 09-17 18:41:16 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m INFO 09-17 18:41:16 [backends.py:541] Dynamo bytecode transform time: 16.18 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m INFO 09-17 18:41:16 [backends.py:541] Dynamo bytecode transform time: 16.19 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m INFO 09-17 18:41:16 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m INFO 09-17 18:41:16 [backends.py:541] Dynamo bytecode transform time: 16.23 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m INFO 09-17 18:41:26 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.323 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m INFO 09-17 18:41:26 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.355 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m INFO 09-17 18:41:26 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.550 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m INFO 09-17 18:41:26 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.590 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m INFO 09-17 18:41:31 [monitor.py:34] torch.compile takes 16.18 s in total\n",
      "INFO 09-17 18:41:31 [monitor.py:34] torch.compile takes 16.23 s in total\n",
      "INFO 09-17 18:41:31 [monitor.py:34] torch.compile takes 16.16 s in total\n",
      "INFO 09-17 18:41:31 [monitor.py:34] torch.compile takes 16.19 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m INFO 09-17 18:41:33 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m INFO 09-17 18:41:33 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m INFO 09-17 18:41:33 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m INFO 09-17 18:41:33 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:41:33 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:41:33 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:41:33 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:41:33 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:41:33 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:41:33 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:41:33 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:41:33 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m INFO 09-17 18:41:38 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m INFO 09-17 18:41:38 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m INFO 09-17 18:41:38 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:04<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m INFO 09-17 18:41:38 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:41:38 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.60 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/random_indices_1_0.99/checkpoint-50 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<50:13,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=526431)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=526428)\u001b[0;0m INFO 09-17 18:41:40 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=526429)\u001b[0;0m INFO 09-17 18:41:40 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=526430)\u001b[0;0m INFO 09-17 18:41:40 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:41:40 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:03<00:00, 1625.54it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 166.54it/s, est. speed input: 8712.85 toks/s, output: 471.26 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.99/checkpoint-50...\n",
      "INFO 09-17 18:42:15 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:42:15 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:42:16 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:42:16 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:42:16 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:42:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_9f0c490d'), local_subscribe_addr='ipc:///tmp/0fba0f45-b4fc-414a-8408-72a822657552', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m INFO 09-17 18:42:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b604a767'), local_subscribe_addr='ipc:///tmp/a4944a18-e735-45f7-a940-749e67acc7b8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-17 18:42:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e56a4382'), local_subscribe_addr='ipc:///tmp/332d1079-2cac-4e25-b02a-a21d9bdd6d09', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m INFO 09-17 18:42:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_16587e4c'), local_subscribe_addr='ipc:///tmp/2c5340ef-e658-41cf-b4c0-6afadf2ab1ba', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m INFO 09-17 18:42:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c950f26b'), local_subscribe_addr='ipc:///tmp/44aca326-0858-4147-978a-5e1fa0410a2d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m INFO 09-17 18:42:19 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:42:19 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:42:19 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:42:19 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m INFO 09-17 18:42:19 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:42:19 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:42:19 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:42:19 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m WARNING 09-17 18:42:19 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:42:19 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m WARNING 09-17 18:42:19 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:42:19 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m INFO 09-17 18:42:19 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_6c2c4144'), local_subscribe_addr='ipc:///tmp/bd417c8c-4bec-48a5-9bfd-2e35d5014b0c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m INFO 09-17 18:42:19 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m INFO 09-17 18:42:19 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:42:19 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:42:19 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m WARNING 09-17 18:42:19 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m WARNING 09-17 18:42:19 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m WARNING 09-17 18:42:19 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:42:19 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m INFO 09-17 18:42:19 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m INFO 09-17 18:42:19 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m INFO 09-17 18:42:19 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:42:19 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m INFO 09-17 18:42:20 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m INFO 09-17 18:42:20 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m INFO 09-17 18:42:20 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m INFO 09-17 18:42:20 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m INFO 09-17 18:42:20 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m INFO 09-17 18:42:20 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m INFO 09-17 18:42:20 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 09-17 18:42:20 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m INFO 09-17 18:42:20 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m INFO 09-17 18:42:20 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 09-17 18:42:20 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m INFO 09-17 18:42:21 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:12,  2.41s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:09,  2.41s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:07,  2.50s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:09<00:04,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m INFO 09-17 18:42:32 [default_loader.py:262] Loading weights took 11.93 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m INFO 09-17 18:42:32 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:11<00:02,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m INFO 09-17 18:42:33 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.500412 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m INFO 09-17 18:42:34 [default_loader.py:262] Loading weights took 13.53 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:13<00:00,  2.08s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:13<00:00,  2.24s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m INFO 09-17 18:42:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m INFO 09-17 18:42:34 [default_loader.py:262] Loading weights took 13.34 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m INFO 09-17 18:42:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m INFO 09-17 18:42:34 [default_loader.py:262] Loading weights took 13.50 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m INFO 09-17 18:42:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m INFO 09-17 18:42:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.181883 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m INFO 09-17 18:42:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.235162 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m INFO 09-17 18:42:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.261761 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m INFO 09-17 18:42:51 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m INFO 09-17 18:42:51 [backends.py:541] Dynamo bytecode transform time: 15.30 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m INFO 09-17 18:42:51 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m INFO 09-17 18:42:51 [backends.py:541] Dynamo bytecode transform time: 15.68 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m INFO 09-17 18:42:52 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m INFO 09-17 18:42:52 [backends.py:541] Dynamo bytecode transform time: 15.73 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m INFO 09-17 18:42:52 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m INFO 09-17 18:42:52 [backends.py:541] Dynamo bytecode transform time: 16.16 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m INFO 09-17 18:43:01 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.366 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m INFO 09-17 18:43:01 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.442 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m INFO 09-17 18:43:01 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.442 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m INFO 09-17 18:43:02 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.289 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m INFO 09-17 18:43:07 [monitor.py:34] torch.compile takes 15.68 s in total\n",
      "INFO 09-17 18:43:07 [monitor.py:34] torch.compile takes 15.30 s in total\n",
      "INFO 09-17 18:43:07 [monitor.py:34] torch.compile takes 16.16 s in total\n",
      "INFO 09-17 18:43:07 [monitor.py:34] torch.compile takes 15.73 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m INFO 09-17 18:43:09 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m INFO 09-17 18:43:09 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m INFO 09-17 18:43:09 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m INFO 09-17 18:43:09 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:43:09 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:43:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:43:09 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:43:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:43:09 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:43:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:43:09 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:43:09 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m INFO 09-17 18:43:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m INFO 09-17 18:43:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m INFO 09-17 18:43:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m INFO 09-17 18:43:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:43:13 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.21 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_42/top_indices_misaligned_0.99/checkpoint-50 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<50:45,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=527288)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=527286)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=527287)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=527285)\u001b[0;0m INFO 09-17 18:43:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:43:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:43:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:43:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2657.14it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 149.14it/s, est. speed input: 7802.76 toks/s, output: 434.06 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.8/checkpoint-1000...\n",
      "INFO 09-17 18:43:52 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:43:52 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:43:53 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:43:53 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:43:53 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:43:53 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_54d4a98a'), local_subscribe_addr='ipc:///tmp/3b7554ba-fbfc-4307-9192-e816281f41b1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m INFO 09-17 18:43:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_af60e877'), local_subscribe_addr='ipc:///tmp/beedae1f-db80-4205-9a33-227ce73cbf0b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m INFO 09-17 18:43:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_373ac743'), local_subscribe_addr='ipc:///tmp/a4e7be4b-43b7-4895-a041-443ccc0b4383', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-17 18:43:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_68123e71'), local_subscribe_addr='ipc:///tmp/9d0ebbd0-045d-49af-b8bf-eb9f548b5941', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m INFO 09-17 18:43:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_36352472'), local_subscribe_addr='ipc:///tmp/6fba6de7-e96c-417e-a642-b6782d48a5f7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m INFO 09-17 18:43:56 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:43:56 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:43:56 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m INFO 09-17 18:43:56 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:43:56 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:43:56 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:43:56 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m INFO 09-17 18:43:56 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m WARNING 09-17 18:43:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:43:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m WARNING 09-17 18:43:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:43:56 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m INFO 09-17 18:43:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_74f8e5c0'), local_subscribe_addr='ipc:///tmp/d7db26dc-314c-4732-981b-2d7217c4e98c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m INFO 09-17 18:43:56 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m INFO 09-17 18:43:56 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:43:56 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:43:56 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m WARNING 09-17 18:43:56 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:43:56 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:43:56 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m WARNING 09-17 18:43:56 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m INFO 09-17 18:43:56 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:43:56 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m INFO 09-17 18:43:56 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m INFO 09-17 18:43:56 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m INFO 09-17 18:43:56 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m INFO 09-17 18:43:56 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m INFO 09-17 18:43:56 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m INFO 09-17 18:43:57 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m INFO 09-17 18:43:57 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m INFO 09-17 18:43:57 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m INFO 09-17 18:43:57 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m INFO 09-17 18:43:57 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m INFO 09-17 18:43:57 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m INFO 09-17 18:43:57 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m INFO 09-17 18:43:57 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m INFO 09-17 18:43:57 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:11,  2.32s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:09,  2.40s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:07,  2.40s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:09<00:04,  2.45s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:11<00:02,  2.35s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:14<00:00,  2.29s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:14<00:00,  2.34s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m INFO 09-17 18:44:11 [default_loader.py:262] Loading weights took 14.12 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m INFO 09-17 18:44:11 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m INFO 09-17 18:44:12 [default_loader.py:262] Loading weights took 14.51 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m INFO 09-17 18:44:12 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m INFO 09-17 18:44:12 [default_loader.py:262] Loading weights took 14.41 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m INFO 09-17 18:44:12 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m INFO 09-17 18:44:12 [default_loader.py:262] Loading weights took 14.38 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m INFO 09-17 18:44:12 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m INFO 09-17 18:44:12 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.732287 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m INFO 09-17 18:44:12 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.157459 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m INFO 09-17 18:44:13 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.057564 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m INFO 09-17 18:44:13 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.189028 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m INFO 09-17 18:44:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m INFO 09-17 18:44:29 [backends.py:541] Dynamo bytecode transform time: 15.63 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m INFO 09-17 18:44:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m INFO 09-17 18:44:29 [backends.py:541] Dynamo bytecode transform time: 15.70 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m INFO 09-17 18:44:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m INFO 09-17 18:44:29 [backends.py:541] Dynamo bytecode transform time: 15.80 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m INFO 09-17 18:44:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m INFO 09-17 18:44:29 [backends.py:541] Dynamo bytecode transform time: 15.85 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m INFO 09-17 18:44:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.316 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m INFO 09-17 18:44:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.390 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m INFO 09-17 18:44:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.349 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m INFO 09-17 18:44:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.549 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m INFO 09-17 18:44:44 [monitor.py:34] torch.compile takes 15.80 s in total\n",
      "INFO 09-17 18:44:44 [monitor.py:34] torch.compile takes 15.63 s in total\n",
      "INFO 09-17 18:44:44 [monitor.py:34] torch.compile takes 15.85 s in total\n",
      "INFO 09-17 18:44:44 [monitor.py:34] torch.compile takes 15.70 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m INFO 09-17 18:44:45 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m INFO 09-17 18:44:45 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m INFO 09-17 18:44:45 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m INFO 09-17 18:44:45 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:44:46 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:44:46 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:44:46 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:44:46 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:44:46 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:44:46 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:44:46 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:44:46 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m INFO 09-17 18:44:50 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:44:50 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m INFO 09-17 18:44:50 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m INFO 09-17 18:44:50 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:44:50 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.64 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.8/checkpoint-1000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<47:16,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=528142)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=528144)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=528141)\u001b[0;0m INFO 09-17 18:44:52 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528143)\u001b[0;0m INFO 09-17 18:44:52 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:44:52 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:44:52 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2704.06it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 156.88it/s, est. speed input: 8207.40 toks/s, output: 422.05 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.8/checkpoint-1000...\n",
      "INFO 09-17 18:45:27 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:45:27 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:45:28 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:45:28 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:45:28 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:45:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_ecc4cc5f'), local_subscribe_addr='ipc:///tmp/4e294609-5c94-44eb-bf3c-98e53f50b5ca', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m INFO 09-17 18:45:30 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_69bb4229'), local_subscribe_addr='ipc:///tmp/3e8b1a24-f27b-4cbd-81a0-8e8ff281cdb0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m INFO 09-17 18:45:30 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3da6ccf1'), local_subscribe_addr='ipc:///tmp/d28be049-1876-498f-962e-2c960793cdae', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m INFO 09-17 18:45:30 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_eab5e260'), local_subscribe_addr='ipc:///tmp/a8b41d76-6ef3-41e6-9d87-9b1f67f928b9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m INFO 09-17 18:45:30 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2917f4fe'), local_subscribe_addr='ipc:///tmp/4b6e11d2-4117-449e-9d4e-6e42b813cee8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m INFO 09-17 18:45:31 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:45:31 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:45:31 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m INFO 09-17 18:45:31 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:45:31 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:45:31 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m INFO 09-17 18:45:31 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m INFO 09-17 18:45:31 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m WARNING 09-17 18:45:31 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:45:31 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m WARNING 09-17 18:45:31 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m WARNING 09-17 18:45:31 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m INFO 09-17 18:45:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_0a47c318'), local_subscribe_addr='ipc:///tmp/27b65929-3b26-473a-890c-c88d2d8a2826', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m INFO 09-17 18:45:31 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m INFO 09-17 18:45:31 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:45:31 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m WARNING 09-17 18:45:31 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m WARNING 09-17 18:45:31 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:45:31 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m INFO 09-17 18:45:31 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:45:31 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m INFO 09-17 18:45:31 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:45:31 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m WARNING 09-17 18:45:31 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m INFO 09-17 18:45:32 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m INFO 09-17 18:45:32 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m INFO 09-17 18:45:32 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m INFO 09-17 18:45:32 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m INFO 09-17 18:45:32 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m INFO 09-17 18:45:32 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m INFO 09-17 18:45:32 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m INFO 09-17 18:45:32 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m INFO 09-17 18:45:32 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m INFO 09-17 18:45:32 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m INFO 09-17 18:45:32 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m INFO 09-17 18:45:32 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m INFO 09-17 18:45:33 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:09,  1.96s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:08,  2.09s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:06<00:05,  2.00s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:08<00:04,  2.27s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:11<00:02,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m INFO 09-17 18:45:45 [default_loader.py:262] Loading weights took 12.62 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m INFO 09-17 18:45:45 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m INFO 09-17 18:45:46 [default_loader.py:262] Loading weights took 13.16 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m INFO 09-17 18:45:46 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m INFO 09-17 18:45:46 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.222200 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:13<00:00,  2.29s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:13<00:00,  2.23s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m INFO 09-17 18:45:46 [default_loader.py:262] Loading weights took 13.42 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m INFO 09-17 18:45:46 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m INFO 09-17 18:45:46 [default_loader.py:262] Loading weights took 13.35 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m INFO 09-17 18:45:46 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m INFO 09-17 18:45:47 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.834282 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m INFO 09-17 18:45:47 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.067387 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m INFO 09-17 18:45:47 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.090809 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m INFO 09-17 18:46:03 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m INFO 09-17 18:46:03 [backends.py:541] Dynamo bytecode transform time: 15.81 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m INFO 09-17 18:46:03 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m INFO 09-17 18:46:03 [backends.py:541] Dynamo bytecode transform time: 15.84 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m INFO 09-17 18:46:03 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m INFO 09-17 18:46:03 [backends.py:541] Dynamo bytecode transform time: 15.86 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m INFO 09-17 18:46:04 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m INFO 09-17 18:46:04 [backends.py:541] Dynamo bytecode transform time: 15.91 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m INFO 09-17 18:46:13 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.253 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m INFO 09-17 18:46:13 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.343 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m INFO 09-17 18:46:13 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.295 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m INFO 09-17 18:46:13 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.378 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m INFO 09-17 18:46:18 [monitor.py:34] torch.compile takes 15.86 s in total\n",
      "INFO 09-17 18:46:18 [monitor.py:34] torch.compile takes 15.84 s in total\n",
      "INFO 09-17 18:46:18 [monitor.py:34] torch.compile takes 15.91 s in total\n",
      "INFO 09-17 18:46:18 [monitor.py:34] torch.compile takes 15.81 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m INFO 09-17 18:46:20 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m INFO 09-17 18:46:20 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m INFO 09-17 18:46:20 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m INFO 09-17 18:46:20 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:46:20 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:46:20 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:46:20 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:46:20 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:46:20 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:46:20 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:46:20 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:46:20 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m INFO 09-17 18:46:24 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:46:24 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m INFO 09-17 18:46:24 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m INFO 09-17 18:46:24 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:46:24 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.46 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.8/checkpoint-1000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<55:17,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=528989)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=528990)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=528992)\u001b[0;0m INFO 09-17 18:46:26 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:46:26 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:46:26 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=528991)\u001b[0;0m INFO 09-17 18:46:26 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2581.35it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 161.39it/s, est. speed input: 8443.40 toks/s, output: 437.38 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.2/checkpoint-4000...\n",
      "INFO 09-17 18:47:00 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:47:00 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:47:01 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:47:01 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:47:01 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:47:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_50dbfc50'), local_subscribe_addr='ipc:///tmp/ae7afec7-7e8d-41b6-ab2d-11a9fae3600e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m INFO 09-17 18:47:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5a39477f'), local_subscribe_addr='ipc:///tmp/50d7be01-fd24-4b0e-b1c6-19e3f272208f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m INFO 09-17 18:47:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a6a27bd5'), local_subscribe_addr='ipc:///tmp/fca6587e-1c0d-4872-b279-0cd4dfd65021', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m INFO 09-17 18:47:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c4dda679'), local_subscribe_addr='ipc:///tmp/d0add75c-47ef-4a88-b231-7b9f670db40e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m INFO 09-17 18:47:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fdc47963'), local_subscribe_addr='ipc:///tmp/14860f41-100a-4370-a3db-d08e3be0d9cc', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m INFO 09-17 18:47:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:47:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:47:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:47:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m INFO 09-17 18:47:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:47:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:47:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:47:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m WARNING 09-17 18:47:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m WARNING 09-17 18:47:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m WARNING 09-17 18:47:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:47:05 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m INFO 09-17 18:47:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_1ee1ce1b'), local_subscribe_addr='ipc:///tmp/c0b24efe-4b61-43b7-917b-a84eae4d16a5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m INFO 09-17 18:47:06 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 18:47:06 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:47:06 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:47:06 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m WARNING 09-17 18:47:06 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m WARNING 09-17 18:47:06 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:47:06 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m WARNING 09-17 18:47:06 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 18:47:06 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m INFO 09-17 18:47:06 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:47:06 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m INFO 09-17 18:47:06 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m INFO 09-17 18:47:06 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m INFO 09-17 18:47:06 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m INFO 09-17 18:47:06 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m INFO 09-17 18:47:06 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m INFO 09-17 18:47:06 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m INFO 09-17 18:47:06 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m INFO 09-17 18:47:06 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m INFO 09-17 18:47:06 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m INFO 09-17 18:47:06 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m INFO 09-17 18:47:06 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m INFO 09-17 18:47:07 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m INFO 09-17 18:47:07 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:14,  2.98s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:05<00:11,  2.95s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:08<00:08,  2.74s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:09<00:04,  2.24s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:10<00:01,  1.80s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.55s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.99s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m INFO 09-17 18:47:19 [default_loader.py:262] Loading weights took 12.01 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m INFO 09-17 18:47:19 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m INFO 09-17 18:47:19 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.762464 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m INFO 09-17 18:47:22 [default_loader.py:262] Loading weights took 14.65 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m INFO 09-17 18:47:22 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m INFO 09-17 18:47:22 [default_loader.py:262] Loading weights took 15.59 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m INFO 09-17 18:47:22 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m INFO 09-17 18:47:22 [default_loader.py:262] Loading weights took 15.32 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m INFO 09-17 18:47:22 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m INFO 09-17 18:47:23 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 16.172596 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m INFO 09-17 18:47:23 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 16.084399 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m INFO 09-17 18:47:23 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.504242 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m INFO 09-17 18:47:39 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m INFO 09-17 18:47:39 [backends.py:541] Dynamo bytecode transform time: 15.53 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m INFO 09-17 18:47:40 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m INFO 09-17 18:47:40 [backends.py:541] Dynamo bytecode transform time: 15.77 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m INFO 09-17 18:47:40 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m INFO 09-17 18:47:40 [backends.py:541] Dynamo bytecode transform time: 15.96 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m INFO 09-17 18:47:40 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m INFO 09-17 18:47:40 [backends.py:541] Dynamo bytecode transform time: 15.81 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m INFO 09-17 18:47:49 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.211 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m INFO 09-17 18:47:49 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.245 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m INFO 09-17 18:47:50 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.447 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m INFO 09-17 18:47:50 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.644 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m INFO 09-17 18:47:55 [monitor.py:34] torch.compile takes 15.96 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m INFO 09-17 18:47:55 [monitor.py:34] torch.compile takes 15.77 s in total\n",
      "INFO 09-17 18:47:55 [monitor.py:34] torch.compile takes 15.53 s in total\n",
      "INFO 09-17 18:47:55 [monitor.py:34] torch.compile takes 15.81 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m INFO 09-17 18:47:57 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m INFO 09-17 18:47:57 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m INFO 09-17 18:47:57 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m INFO 09-17 18:47:57 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:47:57 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:47:57 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:47:57 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:47:57 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:47:57 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:47:57 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:47:57 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:47:57 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m INFO 09-17 18:48:01 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:48:01 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m INFO 09-17 18:48:01 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m INFO 09-17 18:48:01 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:48:01 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.06 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.2/checkpoint-4000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<50:47,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=529833)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=529834)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=529835)\u001b[0;0m INFO 09-17 18:48:03 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:48:03 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:48:03 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=529836)\u001b[0;0m INFO 09-17 18:48:03 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2682.90it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 160.51it/s, est. speed input: 8397.73 toks/s, output: 418.32 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.001/checkpoint-4995...\n",
      "INFO 09-17 18:48:38 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:48:38 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:48:39 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:48:39 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:48:39 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:48:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_11093277'), local_subscribe_addr='ipc:///tmp/9a4f573f-9be4-4a36-8cfb-f63b544919c5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m INFO 09-17 18:48:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d91e6bc1'), local_subscribe_addr='ipc:///tmp/838cd90b-0838-42bf-9c5b-7d4af010d340', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m INFO 09-17 18:48:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9192b8cc'), local_subscribe_addr='ipc:///tmp/d538bfd3-c605-4938-880b-df90304ab338', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m INFO 09-17 18:48:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_90fe1a7d'), local_subscribe_addr='ipc:///tmp/16aefb1e-e33c-43c9-8c09-0f159547285f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m INFO 09-17 18:48:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ed1b550e'), local_subscribe_addr='ipc:///tmp/847f81d3-f573-40f1-a376-f5cf0f9ba86d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m INFO 09-17 18:48:42 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:48:42 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:48:42 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m INFO 09-17 18:48:42 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:48:42 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:48:42 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m INFO 09-17 18:48:42 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m INFO 09-17 18:48:42 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m WARNING 09-17 18:48:43 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m WARNING 09-17 18:48:43 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m WARNING 09-17 18:48:43 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m WARNING 09-17 18:48:43 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m INFO 09-17 18:48:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_9bbfe72f'), local_subscribe_addr='ipc:///tmp/aa947faf-c8d1-4850-ae50-797bc2c4e680', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m INFO 09-17 18:48:43 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:48:43 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:48:43 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:48:43 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m WARNING 09-17 18:48:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m WARNING 09-17 18:48:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m WARNING 09-17 18:48:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:48:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 18:48:43 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m INFO 09-17 18:48:43 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m INFO 09-17 18:48:43 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m INFO 09-17 18:48:43 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m INFO 09-17 18:48:43 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m INFO 09-17 18:48:43 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m INFO 09-17 18:48:43 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m INFO 09-17 18:48:43 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m INFO 09-17 18:48:43 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m INFO 09-17 18:48:43 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 09-17 18:48:43 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m INFO 09-17 18:48:44 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m INFO 09-17 18:48:44 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m INFO 09-17 18:48:44 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m INFO 09-17 18:48:44 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m INFO 09-17 18:48:44 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:03<00:18,  3.63s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:07<00:14,  3.71s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:11<00:11,  3.69s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:14<00:07,  3.53s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:16<00:03,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m INFO 09-17 18:49:02 [default_loader.py:262] Loading weights took 17.87 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m INFO 09-17 18:49:02 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m INFO 09-17 18:49:03 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 18.510453 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m INFO 09-17 18:49:03 [default_loader.py:262] Loading weights took 19.32 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m INFO 09-17 18:49:03 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m INFO 09-17 18:49:04 [default_loader.py:262] Loading weights took 19.59 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m INFO 09-17 18:49:04 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:19<00:00,  2.94s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:19<00:00,  3.23s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m INFO 09-17 18:49:04 [default_loader.py:262] Loading weights took 19.48 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m INFO 09-17 18:49:04 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m INFO 09-17 18:49:04 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 20.163821 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m INFO 09-17 18:49:04 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 20.392344 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m INFO 09-17 18:49:05 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 20.497184 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m INFO 09-17 18:49:21 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m INFO 09-17 18:49:21 [backends.py:541] Dynamo bytecode transform time: 15.48 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m INFO 09-17 18:49:21 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m INFO 09-17 18:49:21 [backends.py:541] Dynamo bytecode transform time: 15.52 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m INFO 09-17 18:49:21 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m INFO 09-17 18:49:21 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "INFO 09-17 18:49:21 [backends.py:541] Dynamo bytecode transform time: 15.87 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m INFO 09-17 18:49:21 [backends.py:541] Dynamo bytecode transform time: 15.71 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m INFO 09-17 18:49:30 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.374 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m INFO 09-17 18:49:31 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.374 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m INFO 09-17 18:49:31 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.482 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m INFO 09-17 18:49:32 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.504 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m INFO 09-17 18:49:37 [monitor.py:34] torch.compile takes 15.52 s in total\n",
      "INFO 09-17 18:49:37 [monitor.py:34] torch.compile takes 15.71 s in total\n",
      "INFO 09-17 18:49:37 [monitor.py:34] torch.compile takes 15.48 s in total\n",
      "INFO 09-17 18:49:37 [monitor.py:34] torch.compile takes 15.87 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m INFO 09-17 18:49:38 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m INFO 09-17 18:49:38 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m INFO 09-17 18:49:39 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m INFO 09-17 18:49:39 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:49:40 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:49:40 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:49:40 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:49:40 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:49:40 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:49:40 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:49:40 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:49:40 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 18:49:44 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m INFO 09-17 18:49:44 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m INFO 09-17 18:49:44 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m INFO 09-17 18:49:44 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:49:44 [core.py:193] init engine (profile, create kv cache, warmup model) took 39.65 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.001/checkpoint-4995 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<47:17,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=530694)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=530691)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=530693)\u001b[0;0m INFO 09-17 18:49:46 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:49:46 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:49:46 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=530692)\u001b[0;0m INFO 09-17 18:49:46 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2700.28it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 159.17it/s, est. speed input: 8327.27 toks/s, output: 428.34 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.999/checkpoint-5...\n",
      "INFO 09-17 18:50:21 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:50:21 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:50:22 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:50:22 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:50:22 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:50:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_2687422c'), local_subscribe_addr='ipc:///tmp/1c4f5b8c-c69c-4de9-9688-02c8722602bc', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m INFO 09-17 18:50:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d1178a1a'), local_subscribe_addr='ipc:///tmp/1fb237f3-db41-4925-ad0d-8461a0d80be7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m INFO 09-17 18:50:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6cede180'), local_subscribe_addr='ipc:///tmp/805581f6-ca58-4ecb-b537-be7b10c17777', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m INFO 09-17 18:50:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_611e3268'), local_subscribe_addr='ipc:///tmp/63179621-3da4-4c01-a537-745bb0b8d5b2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m INFO 09-17 18:50:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5cc60248'), local_subscribe_addr='ipc:///tmp/ec1c3bcd-bd5a-4cb2-8202-27b03ae09aac', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m INFO 09-17 18:50:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m INFO 09-17 18:50:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:50:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:50:25 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m INFO 09-17 18:50:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m INFO 09-17 18:50:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:50:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:50:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m WARNING 09-17 18:50:25 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:50:25 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m WARNING 09-17 18:50:25 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m WARNING 09-17 18:50:25 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m INFO 09-17 18:50:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_6d2e737f'), local_subscribe_addr='ipc:///tmp/6bbec1a1-4731-42f3-b04d-fd2fe645732e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m INFO 09-17 18:50:25 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:50:25 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:50:25 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:50:25 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m WARNING 09-17 18:50:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m WARNING 09-17 18:50:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m WARNING 09-17 18:50:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:50:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m INFO 09-17 18:50:25 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:50:25 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m INFO 09-17 18:50:25 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:50:25 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m INFO 09-17 18:50:26 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m INFO 09-17 18:50:26 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m INFO 09-17 18:50:26 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m INFO 09-17 18:50:26 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m INFO 09-17 18:50:26 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m INFO 09-17 18:50:26 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m INFO 09-17 18:50:26 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m INFO 09-17 18:50:26 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m INFO 09-17 18:50:26 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m INFO 09-17 18:50:26 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m INFO 09-17 18:50:26 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m INFO 09-17 18:50:27 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:03<00:15,  3.16s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:05<00:10,  2.63s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:06<00:05,  1.98s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:03,  1.65s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.42s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.29s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:09<00:00,  1.64s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m INFO 09-17 18:50:36 [default_loader.py:262] Loading weights took 9.90 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m INFO 09-17 18:50:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m INFO 09-17 18:50:37 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.514627 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m INFO 09-17 18:50:40 [default_loader.py:262] Loading weights took 13.11 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m INFO 09-17 18:50:40 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m INFO 09-17 18:50:40 [default_loader.py:262] Loading weights took 13.27 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m INFO 09-17 18:50:40 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m INFO 09-17 18:50:40 [default_loader.py:262] Loading weights took 13.18 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m INFO 09-17 18:50:40 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m INFO 09-17 18:50:40 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.910651 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m INFO 09-17 18:50:41 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.054296 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m INFO 09-17 18:50:41 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.185185 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m INFO 09-17 18:50:57 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m INFO 09-17 18:50:57 [backends.py:541] Dynamo bytecode transform time: 15.19 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m INFO 09-17 18:50:57 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m INFO 09-17 18:50:57 [backends.py:541] Dynamo bytecode transform time: 15.65 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m INFO 09-17 18:50:57 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m INFO 09-17 18:50:57 [backends.py:541] Dynamo bytecode transform time: 15.79 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m INFO 09-17 18:50:57 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m INFO 09-17 18:50:57 [backends.py:541] Dynamo bytecode transform time: 15.96 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m INFO 09-17 18:51:06 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.175 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m INFO 09-17 18:51:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.315 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m INFO 09-17 18:51:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.385 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m INFO 09-17 18:51:08 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.538 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m INFO 09-17 18:51:13 [monitor.py:34] torch.compile takes 15.96 s in total\n",
      "INFO 09-17 18:51:13 [monitor.py:34] torch.compile takes 15.19 s in total\n",
      "INFO 09-17 18:51:13 [monitor.py:34] torch.compile takes 15.65 s in total\n",
      "INFO 09-17 18:51:13 [monitor.py:34] torch.compile takes 15.79 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m INFO 09-17 18:51:14 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m INFO 09-17 18:51:14 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m INFO 09-17 18:51:15 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m INFO 09-17 18:51:15 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:51:16 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:51:16 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:51:16 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:51:16 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:51:16 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:51:16 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:51:16 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:51:16 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m INFO 09-17 18:51:20 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m INFO 09-17 18:51:20 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m INFO 09-17 18:51:20 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m INFO 09-17 18:51:20 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:51:20 [core.py:193] init engine (profile, create kv cache, warmup model) took 39.01 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.999/checkpoint-5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<46:33,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=531557)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=531559)\u001b[0;0m INFO 09-17 18:51:22 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:51:22 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=531558)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=531560)\u001b[0;0m INFO 09-17 18:51:22 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:51:22 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2791.46it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 150.65it/s, est. speed input: 7881.13 toks/s, output: 439.50 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.4/checkpoint-3000...\n",
      "INFO 09-17 18:51:58 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:51:58 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:51:59 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:51:59 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:51:59 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:51:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_c682a757'), local_subscribe_addr='ipc:///tmp/4a96608a-c584-41d6-946b-5eefc42c1b63', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m INFO 09-17 18:52:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7bfa8e69'), local_subscribe_addr='ipc:///tmp/3555f119-a588-45fa-af5c-27d92065829a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m INFO 09-17 18:52:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5d26c743'), local_subscribe_addr='ipc:///tmp/beb6642d-72ab-4179-95cd-32f7e27c32d7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m INFO 09-17 18:52:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ec35d9c8'), local_subscribe_addr='ipc:///tmp/7bf7f74e-456c-4464-8eb0-1d91f33afeef', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m INFO 09-17 18:52:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a3d99a64'), local_subscribe_addr='ipc:///tmp/59002eef-f0ae-4530-ab59-38e15a49b32e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m INFO 09-17 18:52:03 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:52:03 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:52:03 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:52:03 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m INFO 09-17 18:52:03 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:52:03 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:52:03 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:52:03 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m WARNING 09-17 18:52:03 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m WARNING 09-17 18:52:03 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m WARNING 09-17 18:52:03 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m WARNING 09-17 18:52:03 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m INFO 09-17 18:52:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_97cd8c56'), local_subscribe_addr='ipc:///tmp/de98aa47-105f-493e-acdf-42437f71a436', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m INFO 09-17 18:52:03 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m INFO 09-17 18:52:03 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 18:52:03 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:52:03 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m WARNING 09-17 18:52:03 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:52:03 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m WARNING 09-17 18:52:03 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m WARNING 09-17 18:52:03 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 18:52:03 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m INFO 09-17 18:52:03 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m INFO 09-17 18:52:03 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m INFO 09-17 18:52:03 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m INFO 09-17 18:52:04 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 09-17 18:52:04 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m INFO 09-17 18:52:04 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m INFO 09-17 18:52:04 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m INFO 09-17 18:52:04 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 09-17 18:52:04 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 09-17 18:52:04 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 09-17 18:52:04 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m INFO 09-17 18:52:04 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m INFO 09-17 18:52:04 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m INFO 09-17 18:52:04 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m INFO 09-17 18:52:04 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:12,  2.44s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:08,  2.18s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.76s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:06<00:03,  1.53s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:07<00:01,  1.35s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.25s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08<00:00,  1.49s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m INFO 09-17 18:52:13 [default_loader.py:262] Loading weights took 9.03 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m INFO 09-17 18:52:13 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m INFO 09-17 18:52:14 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.777740 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m INFO 09-17 18:52:16 [default_loader.py:262] Loading weights took 12.08 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m INFO 09-17 18:52:17 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m INFO 09-17 18:52:17 [default_loader.py:262] Loading weights took 12.61 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m INFO 09-17 18:52:17 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m INFO 09-17 18:52:17 [default_loader.py:262] Loading weights took 12.55 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m INFO 09-17 18:52:17 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.960297 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m INFO 09-17 18:52:17 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m INFO 09-17 18:52:18 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.583350 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m INFO 09-17 18:52:18 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.648250 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m INFO 09-17 18:52:34 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m INFO 09-17 18:52:34 [backends.py:541] Dynamo bytecode transform time: 15.16 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m INFO 09-17 18:52:34 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m INFO 09-17 18:52:34 [backends.py:541] Dynamo bytecode transform time: 15.72 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m INFO 09-17 18:52:34 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m INFO 09-17 18:52:34 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "INFO 09-17 18:52:34 [backends.py:541] Dynamo bytecode transform time: 15.70 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m INFO 09-17 18:52:34 [backends.py:541] Dynamo bytecode transform time: 15.73 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m INFO 09-17 18:52:43 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.190 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m INFO 09-17 18:52:44 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.168 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m INFO 09-17 18:52:44 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.281 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m INFO 09-17 18:52:44 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.448 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m INFO 09-17 18:52:49 [monitor.py:34] torch.compile takes 15.70 s in total\n",
      "INFO 09-17 18:52:49 [monitor.py:34] torch.compile takes 15.16 s in total\n",
      "INFO 09-17 18:52:49 [monitor.py:34] torch.compile takes 15.73 s in total\n",
      "INFO 09-17 18:52:49 [monitor.py:34] torch.compile takes 15.72 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m INFO 09-17 18:52:51 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m INFO 09-17 18:52:51 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:52:51 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m INFO 09-17 18:52:51 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:52:51 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:52:51 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:52:51 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:52:51 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:52:51 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:52:51 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:52:51 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:52:51 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m INFO 09-17 18:52:55 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:52:55 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m INFO 09-17 18:52:55 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m INFO 09-17 18:52:55 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:52:55 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.23 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.4/checkpoint-3000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<51:29,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=532410)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=532407)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=532409)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=532408)\u001b[0;0m INFO 09-17 18:52:57 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:52:57 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:52:57 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:52:57 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 2399.60it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:29<00:00, 169.76it/s, est. speed input: 8880.90 toks/s, output: 440.19 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.99/checkpoint-50...\n",
      "INFO 09-17 18:53:30 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:53:30 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:53:31 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:53:31 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:53:31 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:53:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_9bd9d9f6'), local_subscribe_addr='ipc:///tmp/26bb95da-3e3f-45a9-b843-855f13b59cda', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m INFO 09-17 18:53:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1a2719fb'), local_subscribe_addr='ipc:///tmp/a40d4e96-216b-46e0-be40-ca4f60b4e466', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m INFO 09-17 18:53:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_472fc843'), local_subscribe_addr='ipc:///tmp/ee2a2aff-7d7b-4aa6-bfcd-7b25066d3a1b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m INFO 09-17 18:53:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1fd716fc'), local_subscribe_addr='ipc:///tmp/101293aa-fec5-477e-a9d6-3a3e705146e0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m INFO 09-17 18:53:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d65306f8'), local_subscribe_addr='ipc:///tmp/7948b944-2254-4cb8-bb80-4c1347d4af92', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m INFO 09-17 18:53:35 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:53:35 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:53:35 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m INFO 09-17 18:53:35 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:53:35 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:53:35 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:53:35 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m INFO 09-17 18:53:35 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m WARNING 09-17 18:53:35 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:53:35 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m WARNING 09-17 18:53:35 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:53:35 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m INFO 09-17 18:53:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_824a10d9'), local_subscribe_addr='ipc:///tmp/9aa2f5a1-6ca0-4fca-8809-f4f1f1d99678', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m INFO 09-17 18:53:35 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m INFO 09-17 18:53:35 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:53:35 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:53:35 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m WARNING 09-17 18:53:35 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m WARNING 09-17 18:53:35 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m WARNING 09-17 18:53:35 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 18:53:35 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "WARNING 09-17 18:53:35 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m INFO 09-17 18:53:35 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:53:35 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:53:35 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m INFO 09-17 18:53:35 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m INFO 09-17 18:53:35 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m INFO 09-17 18:53:36 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m INFO 09-17 18:53:36 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m INFO 09-17 18:53:36 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m INFO 09-17 18:53:36 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m INFO 09-17 18:53:36 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m INFO 09-17 18:53:36 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m INFO 09-17 18:53:36 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m INFO 09-17 18:53:36 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m INFO 09-17 18:53:36 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 09-17 18:53:36 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:13,  2.69s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:05<00:10,  2.70s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:07,  2.39s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:08<00:04,  2.03s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:09<00:01,  1.67s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.46s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.83s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m INFO 09-17 18:53:47 [default_loader.py:262] Loading weights took 11.05 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m INFO 09-17 18:53:47 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m INFO 09-17 18:53:48 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.699537 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m INFO 09-17 18:53:49 [default_loader.py:262] Loading weights took 12.58 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m INFO 09-17 18:53:49 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m INFO 09-17 18:53:50 [default_loader.py:262] Loading weights took 13.11 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m INFO 09-17 18:53:50 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m INFO 09-17 18:53:50 [default_loader.py:262] Loading weights took 13.68 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m INFO 09-17 18:53:50 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m INFO 09-17 18:53:50 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.736314 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m INFO 09-17 18:53:51 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.134439 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m INFO 09-17 18:53:51 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.399469 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m INFO 09-17 18:54:07 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m INFO 09-17 18:54:07 [backends.py:541] Dynamo bytecode transform time: 15.91 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m INFO 09-17 18:54:07 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m INFO 09-17 18:54:07 [backends.py:541] Dynamo bytecode transform time: 15.97 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m INFO 09-17 18:54:07 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m INFO 09-17 18:54:07 [backends.py:541] Dynamo bytecode transform time: 15.99 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m INFO 09-17 18:54:07 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m INFO 09-17 18:54:07 [backends.py:541] Dynamo bytecode transform time: 16.03 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m INFO 09-17 18:54:17 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.371 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m INFO 09-17 18:54:17 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.400 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m INFO 09-17 18:54:17 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.394 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m INFO 09-17 18:54:17 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.447 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m INFO 09-17 18:54:22 [monitor.py:34] torch.compile takes 15.97 s in total\n",
      "INFO 09-17 18:54:22 [monitor.py:34] torch.compile takes 16.03 s in total\n",
      "INFO 09-17 18:54:22 [monitor.py:34] torch.compile takes 15.99 s in total\n",
      "INFO 09-17 18:54:22 [monitor.py:34] torch.compile takes 15.91 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m INFO 09-17 18:54:24 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m INFO 09-17 18:54:24 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m INFO 09-17 18:54:24 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m INFO 09-17 18:54:24 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:54:25 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:54:25 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:54:25 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:54:25 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:54:25 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:54:25 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:54:25 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:54:25 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m INFO 09-17 18:54:29 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m INFO 09-17 18:54:29 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:54:29 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m INFO 09-17 18:54:29 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:54:29 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.44 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.99/checkpoint-50 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<41:39,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=533252)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=533251)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=533254)\u001b[0;0m INFO 09-17 18:54:31 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:54:31 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:54:31 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=533253)\u001b[0;0m INFO 09-17 18:54:31 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2687.57it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 162.17it/s, est. speed input: 8484.18 toks/s, output: 444.14 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.1/checkpoint-4500...\n",
      "INFO 09-17 18:55:05 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:55:05 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:55:06 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:55:06 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:55:06 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:55:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_5961b584'), local_subscribe_addr='ipc:///tmp/7eb9bc51-fcdb-4ffd-a386-1d6d88d8f811', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m INFO 09-17 18:55:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1535ad57'), local_subscribe_addr='ipc:///tmp/f605ab01-dc4f-4caf-8f47-485cb5709e8a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m INFO 09-17 18:55:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e011548e'), local_subscribe_addr='ipc:///tmp/09c25f3a-e599-43a2-a1b1-77699dd13483', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m INFO 09-17 18:55:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_19faf034'), local_subscribe_addr='ipc:///tmp/7229c404-8c1f-4499-8960-06c85f53266d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m INFO 09-17 18:55:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_645329cb'), local_subscribe_addr='ipc:///tmp/10011536-4ab0-467c-be41-ec2e24389b80', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m INFO 09-17 18:55:10 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m INFO 09-17 18:55:10 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:55:10 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m INFO 09-17 18:55:10 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m INFO 09-17 18:55:10 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m INFO 09-17 18:55:10 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m INFO 09-17 18:55:10 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m INFO 09-17 18:55:10 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m WARNING 09-17 18:55:10 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:55:10 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m WARNING 09-17 18:55:10 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:55:10 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m INFO 09-17 18:55:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_d27a0a61'), local_subscribe_addr='ipc:///tmp/f2dc29fe-0f8f-41ff-80f2-eb960d41606e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m INFO 09-17 18:55:10 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m INFO 09-17 18:55:10 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:55:10 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:55:10 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m WARNING 09-17 18:55:10 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:55:10 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m WARNING 09-17 18:55:10 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m WARNING 09-17 18:55:10 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 18:55:10 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m INFO 09-17 18:55:10 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:55:10 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m INFO 09-17 18:55:10 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m INFO 09-17 18:55:11 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m INFO 09-17 18:55:11 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m INFO 09-17 18:55:11 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m INFO 09-17 18:55:11 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m INFO 09-17 18:55:11 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m INFO 09-17 18:55:11 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m INFO 09-17 18:55:11 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m INFO 09-17 18:55:11 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m INFO 09-17 18:55:11 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m INFO 09-17 18:55:11 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m INFO 09-17 18:55:11 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 09-17 18:55:11 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:09,  1.84s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.60s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:04<00:04,  1.39s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:05<00:02,  1.30s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:06<00:01,  1.21s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:07<00:00,  1.16s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:07<00:00,  1.28s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m INFO 09-17 18:55:19 [default_loader.py:262] Loading weights took 7.74 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m INFO 09-17 18:55:19 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m INFO 09-17 18:55:20 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 8.291381 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m INFO 09-17 18:55:22 [default_loader.py:262] Loading weights took 9.79 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m INFO 09-17 18:55:22 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m INFO 09-17 18:55:22 [default_loader.py:262] Loading weights took 10.21 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m INFO 09-17 18:55:22 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m INFO 09-17 18:55:22 [default_loader.py:262] Loading weights took 10.49 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m INFO 09-17 18:55:22 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m INFO 09-17 18:55:22 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.853944 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m INFO 09-17 18:55:23 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.147229 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m INFO 09-17 18:55:23 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.522468 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m INFO 09-17 18:55:40 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m INFO 09-17 18:55:40 [backends.py:541] Dynamo bytecode transform time: 15.72 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m INFO 09-17 18:55:40 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m INFO 09-17 18:55:40 [backends.py:541] Dynamo bytecode transform time: 15.81 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m INFO 09-17 18:55:40 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m INFO 09-17 18:55:40 [backends.py:541] Dynamo bytecode transform time: 15.99 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m INFO 09-17 18:55:40 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m INFO 09-17 18:55:40 [backends.py:541] Dynamo bytecode transform time: 16.14 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m INFO 09-17 18:55:50 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.652 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m INFO 09-17 18:55:50 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.620 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m INFO 09-17 18:55:50 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.353 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m INFO 09-17 18:55:50 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.661 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m INFO 09-17 18:55:55 [monitor.py:34] torch.compile takes 15.99 s in total\n",
      "INFO 09-17 18:55:55 [monitor.py:34] torch.compile takes 15.72 s in total\n",
      "INFO 09-17 18:55:55 [monitor.py:34] torch.compile takes 15.81 s in total\n",
      "INFO 09-17 18:55:55 [monitor.py:34] torch.compile takes 16.14 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m INFO 09-17 18:55:57 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m INFO 09-17 18:55:57 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m INFO 09-17 18:55:57 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m INFO 09-17 18:55:57 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:55:57 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:55:57 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:55:57 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:55:57 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:55:57 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:55:57 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:55:57 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:55:57 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m INFO 09-17 18:56:01 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m INFO 09-17 18:56:01 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:56:01 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m INFO 09-17 18:56:01 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:56:01 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.24 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.1/checkpoint-4500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<56:31,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=534097)\u001b[0;0m INFO 09-17 18:56:04 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534098)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=534099)\u001b[0;0m INFO 09-17 18:56:04 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:56:04 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534100)\u001b[0;0m INFO 09-17 18:56:04 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:03<00:00, 1575.10it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:29<00:00, 171.74it/s, est. speed input: 8984.66 toks/s, output: 456.29 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.2/checkpoint-4000...\n",
      "INFO 09-17 18:56:38 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:56:38 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:56:39 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:56:39 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:56:39 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:56:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_9ba32b7b'), local_subscribe_addr='ipc:///tmp/b27816f8-3c90-4c43-b589-1fe1f539e853', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m INFO 09-17 18:56:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8a2aefa4'), local_subscribe_addr='ipc:///tmp/496a8292-6b68-4038-a512-1140219c6422', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m INFO 09-17 18:56:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6696c8f2'), local_subscribe_addr='ipc:///tmp/f4f15e7c-475e-4922-b8a2-fede9b8de257', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m INFO 09-17 18:56:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6208b265'), local_subscribe_addr='ipc:///tmp/f8b7ab37-d5ee-4e08-ba80-2bb3070fcabd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m INFO 09-17 18:56:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b172e2fe'), local_subscribe_addr='ipc:///tmp/1ec21110-9652-455a-8d78-92c73f0c2142', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m INFO 09-17 18:56:42 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:56:42 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m INFO 09-17 18:56:42 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m INFO 09-17 18:56:42 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:56:42 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m INFO 09-17 18:56:42 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:56:42 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m INFO 09-17 18:56:42 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m WARNING 09-17 18:56:43 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:56:43 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m WARNING 09-17 18:56:43 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:56:43 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m INFO 09-17 18:56:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_a869a1ae'), local_subscribe_addr='ipc:///tmp/18fabe5c-0cb6-4579-b576-575cdd2e73e2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m INFO 09-17 18:56:43 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m INFO 09-17 18:56:43 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:56:43 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:56:43 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m WARNING 09-17 18:56:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m WARNING 09-17 18:56:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 18:56:43 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m WARNING 09-17 18:56:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:56:43 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m INFO 09-17 18:56:43 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m INFO 09-17 18:56:43 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:56:43 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m INFO 09-17 18:56:43 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m INFO 09-17 18:56:43 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m INFO 09-17 18:56:43 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m INFO 09-17 18:56:43 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m INFO 09-17 18:56:43 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m INFO 09-17 18:56:44 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m INFO 09-17 18:56:44 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m INFO 09-17 18:56:44 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m INFO 09-17 18:56:44 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m INFO 09-17 18:56:44 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m INFO 09-17 18:56:44 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m INFO 09-17 18:56:44 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:03<00:15,  3.09s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:06<00:12,  3.01s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:08<00:08,  2.83s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:10<00:05,  2.54s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:11<00:02,  2.05s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:13<00:00,  1.72s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:13<00:00,  2.17s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m INFO 09-17 18:56:57 [default_loader.py:262] Loading weights took 13.07 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m INFO 09-17 18:56:57 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m INFO 09-17 18:56:58 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.730303 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m INFO 09-17 18:56:58 [default_loader.py:262] Loading weights took 14.41 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m INFO 09-17 18:56:58 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m INFO 09-17 18:56:59 [default_loader.py:262] Loading weights took 14.28 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m INFO 09-17 18:56:59 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m INFO 09-17 18:56:59 [default_loader.py:262] Loading weights took 14.81 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m INFO 09-17 18:56:59 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.123142 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m INFO 09-17 18:56:59 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m INFO 09-17 18:57:00 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.495601 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m INFO 09-17 18:57:00 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.834894 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m INFO 09-17 18:57:17 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m INFO 09-17 18:57:17 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "INFO 09-17 18:57:17 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "INFO 09-17 18:57:17 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m INFO 09-17 18:57:17 [backends.py:541] Dynamo bytecode transform time: 16.03 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m INFO 09-17 18:57:17 [backends.py:541] Dynamo bytecode transform time: 16.03 s\n",
      "INFO 09-17 18:57:17 [backends.py:541] Dynamo bytecode transform time: 16.05 s\n",
      "INFO 09-17 18:57:17 [backends.py:541] Dynamo bytecode transform time: 16.04 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m INFO 09-17 18:57:26 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.350 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m INFO 09-17 18:57:26 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.400 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m INFO 09-17 18:57:27 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.586 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m INFO 09-17 18:57:27 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.767 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m INFO 09-17 18:57:32 [monitor.py:34] torch.compile takes 16.05 s in total\n",
      "INFO 09-17 18:57:32 [monitor.py:34] torch.compile takes 16.04 s in total\n",
      "INFO 09-17 18:57:32 [monitor.py:34] torch.compile takes 16.03 s in total\n",
      "INFO 09-17 18:57:32 [monitor.py:34] torch.compile takes 16.03 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m INFO 09-17 18:57:33 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m INFO 09-17 18:57:33 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m INFO 09-17 18:57:33 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m INFO 09-17 18:57:33 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:57:34 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:57:34 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:57:34 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:57:34 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:57:34 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:57:34 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:57:34 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:57:34 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m INFO 09-17 18:57:38 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m INFO 09-17 18:57:38 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m INFO 09-17 18:57:38 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m INFO 09-17 18:57:38 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:57:38 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.01 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.2/checkpoint-4000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<50:24,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=534939)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=534940)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=534941)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=534942)\u001b[0;0m INFO 09-17 18:57:40 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:57:40 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:57:40 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:57:40 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2613.25it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 159.53it/s, est. speed input: 8346.13 toks/s, output: 422.68 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.01/checkpoint-4950...\n",
      "INFO 09-17 18:58:15 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:58:15 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:58:16 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:58:16 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:58:16 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:58:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_ccdce26d'), local_subscribe_addr='ipc:///tmp/85ea9760-1ba5-474f-9671-012084efb134', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m INFO 09-17 18:58:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_51ce47d8'), local_subscribe_addr='ipc:///tmp/d51369a6-a83b-40a9-8674-a213abc3849d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m INFO 09-17 18:58:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4f5e3c4b'), local_subscribe_addr='ipc:///tmp/1c513b44-72bf-49cc-b59c-7965f518e04a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m INFO 09-17 18:58:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3863ab43'), local_subscribe_addr='ipc:///tmp/419ed480-7ea3-428c-8042-e137c881576f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m INFO 09-17 18:58:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3bd10201'), local_subscribe_addr='ipc:///tmp/15d50908-f243-4024-b695-1742e7c046e9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m INFO 09-17 18:58:19 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m INFO 09-17 18:58:19 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:58:19 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m INFO 09-17 18:58:19 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m INFO 09-17 18:58:19 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:58:19 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m INFO 09-17 18:58:19 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:58:19 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m WARNING 09-17 18:58:20 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:58:20 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m WARNING 09-17 18:58:19 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 18:58:20 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m INFO 09-17 18:58:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_b89d952d'), local_subscribe_addr='ipc:///tmp/1ad25f1f-46c1-46ce-b4ce-d74ed441a7c1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m INFO 09-17 18:58:20 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m INFO 09-17 18:58:20 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:58:20 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:58:20 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m WARNING 09-17 18:58:20 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m WARNING 09-17 18:58:20 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:58:20 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:58:20 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m INFO 09-17 18:58:20 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m INFO 09-17 18:58:20 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:58:20 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 18:58:20 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m INFO 09-17 18:58:20 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m INFO 09-17 18:58:20 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m INFO 09-17 18:58:20 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m INFO 09-17 18:58:20 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m INFO 09-17 18:58:20 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m INFO 09-17 18:58:20 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m INFO 09-17 18:58:20 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m INFO 09-17 18:58:20 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m INFO 09-17 18:58:20 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m INFO 09-17 18:58:20 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m INFO 09-17 18:58:20 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m INFO 09-17 18:58:20 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:03<00:16,  3.26s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:06<00:13,  3.29s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:09<00:09,  3.22s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:12<00:05,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m INFO 09-17 18:58:34 [default_loader.py:262] Loading weights took 14.07 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m INFO 09-17 18:58:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:13<00:02,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m INFO 09-17 18:58:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.648344 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m INFO 09-17 18:58:36 [default_loader.py:262] Loading weights took 15.82 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m INFO 09-17 18:58:36 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:15<00:00,  2.23s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:15<00:00,  2.61s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m INFO 09-17 18:58:37 [default_loader.py:262] Loading weights took 15.73 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m INFO 09-17 18:58:37 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m INFO 09-17 18:58:37 [default_loader.py:262] Loading weights took 16.23 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m INFO 09-17 18:58:37 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m INFO 09-17 18:58:37 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 16.507494 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m INFO 09-17 18:58:38 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 16.688184 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m INFO 09-17 18:58:38 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 16.886821 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m INFO 09-17 18:58:54 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m INFO 09-17 18:58:54 [backends.py:541] Dynamo bytecode transform time: 16.01 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m INFO 09-17 18:58:54 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m INFO 09-17 18:58:54 [backends.py:541] Dynamo bytecode transform time: 16.02 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m INFO 09-17 18:58:54 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m INFO 09-17 18:58:54 [backends.py:541] Dynamo bytecode transform time: 16.11 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m INFO 09-17 18:58:54 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m INFO 09-17 18:58:54 [backends.py:541] Dynamo bytecode transform time: 16.10 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m INFO 09-17 18:59:04 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.385 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m INFO 09-17 18:59:04 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.416 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m INFO 09-17 18:59:04 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.436 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m INFO 09-17 18:59:04 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.473 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m INFO 09-17 18:59:09 [monitor.py:34] torch.compile takes 16.11 s in total\n",
      "INFO 09-17 18:59:09 [monitor.py:34] torch.compile takes 16.01 s in total\n",
      "INFO 09-17 18:59:09 [monitor.py:34] torch.compile takes 16.02 s in total\n",
      "INFO 09-17 18:59:09 [monitor.py:34] torch.compile takes 16.10 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m INFO 09-17 18:59:11 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m INFO 09-17 18:59:11 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m INFO 09-17 18:59:11 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m INFO 09-17 18:59:11 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 18:59:11 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 18:59:11 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 18:59:11 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:59:11 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:59:11 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:59:11 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 18:59:11 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 18:59:11 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m INFO 09-17 18:59:15 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m INFO 09-17 18:59:15 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m INFO 09-17 18:59:15 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m INFO 09-17 18:59:15 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 18:59:16 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.98 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.01/checkpoint-4950 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<1:05:42,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=535791)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=535792)\u001b[0;0m INFO 09-17 18:59:17 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:59:17 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=535793)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=535794)\u001b[0;0m INFO 09-17 18:59:17 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 18:59:17 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 2399.87it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 158.46it/s, est. speed input: 8290.05 toks/s, output: 422.63 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.5/checkpoint-2500...\n",
      "INFO 09-17 18:59:52 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 18:59:52 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 18:59:53 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 18:59:53 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 18:59:53 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 18:59:53 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_f3d4d397'), local_subscribe_addr='ipc:///tmp/58262b80-a621-4d4f-a2b5-814a44c9f4e0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m INFO 09-17 18:59:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_84922069'), local_subscribe_addr='ipc:///tmp/eaed15fb-a0e6-45c1-af1f-0d8042ae1f10', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m INFO 09-17 18:59:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_678ab41e'), local_subscribe_addr='ipc:///tmp/9e18ba97-3c95-4bac-8636-dbdf75aa18aa', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m INFO 09-17 18:59:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2519aaed'), local_subscribe_addr='ipc:///tmp/ebc26db2-dd2e-4a2c-9ada-f1ecd2ee71c2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m INFO 09-17 18:59:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2c47ad99'), local_subscribe_addr='ipc:///tmp/1dbd1344-22b0-4eda-974c-3ae738a894d0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m INFO 09-17 18:59:57 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:59:57 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 18:59:57 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m INFO 09-17 18:59:57 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:59:57 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:59:57 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 18:59:57 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m INFO 09-17 18:59:57 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m WARNING 09-17 18:59:57 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m WARNING 09-17 18:59:57 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m WARNING 09-17 18:59:57 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m WARNING 09-17 18:59:57 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m INFO 09-17 18:59:57 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_feee5660'), local_subscribe_addr='ipc:///tmp/23313179-42c6-4f59-a26f-4c3a5262aee4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m INFO 09-17 18:59:57 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 18:59:57 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 18:59:57 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 18:59:57 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m WARNING 09-17 18:59:57 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 18:59:57 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m WARNING 09-17 18:59:57 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m WARNING 09-17 18:59:57 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 18:59:57 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m INFO 09-17 18:59:57 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m INFO 09-17 18:59:57 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m INFO 09-17 18:59:57 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m INFO 09-17 18:59:58 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 09-17 18:59:58 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m INFO 09-17 18:59:58 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m INFO 09-17 18:59:58 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m INFO 09-17 18:59:58 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m INFO 09-17 18:59:58 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m INFO 09-17 18:59:58 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m INFO 09-17 18:59:58 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m INFO 09-17 18:59:58 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m INFO 09-17 18:59:58 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m INFO 09-17 18:59:58 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m INFO 09-17 18:59:58 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:12,  2.43s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:04<00:09,  2.32s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:06<00:06,  2.22s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:08<00:04,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m INFO 09-17 19:00:08 [default_loader.py:262] Loading weights took 9.27 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m INFO 09-17 19:00:08 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m INFO 09-17 19:00:08 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 10.210678 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:10<00:01,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m INFO 09-17 19:00:09 [default_loader.py:262] Loading weights took 10.79 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m INFO 09-17 19:00:09 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m INFO 09-17 19:00:10 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.787544 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m INFO 09-17 19:00:10 [default_loader.py:262] Loading weights took 11.62 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m INFO 09-17 19:00:10 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.72s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.93s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m INFO 09-17 19:00:10 [default_loader.py:262] Loading weights took 11.62 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m INFO 09-17 19:00:11 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m INFO 09-17 19:00:11 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.925206 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m INFO 09-17 19:00:11 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.754702 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m INFO 09-17 19:00:28 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m INFO 09-17 19:00:28 [backends.py:541] Dynamo bytecode transform time: 15.83 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m INFO 09-17 19:00:28 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m INFO 09-17 19:00:28 [backends.py:541] Dynamo bytecode transform time: 16.01 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m INFO 09-17 19:00:28 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m INFO 09-17 19:00:28 [backends.py:541] Dynamo bytecode transform time: 16.09 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m INFO 09-17 19:00:28 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m INFO 09-17 19:00:28 [backends.py:541] Dynamo bytecode transform time: 16.16 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m INFO 09-17 19:00:38 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.399 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m INFO 09-17 19:00:38 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.646 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m INFO 09-17 19:00:38 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.513 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m INFO 09-17 19:00:38 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.592 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m INFO 09-17 19:00:43 [monitor.py:34] torch.compile takes 16.01 s in total\n",
      "INFO 09-17 19:00:43 [monitor.py:34] torch.compile takes 16.09 s in total\n",
      "INFO 09-17 19:00:43 [monitor.py:34] torch.compile takes 15.83 s in total\n",
      "INFO 09-17 19:00:43 [monitor.py:34] torch.compile takes 16.16 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m INFO 09-17 19:00:45 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m INFO 09-17 19:00:45 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m INFO 09-17 19:00:45 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m INFO 09-17 19:00:45 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 19:00:45 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 19:00:45 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 19:00:45 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:00:45 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:00:45 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:00:45 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:00:45 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:00:45 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m INFO 09-17 19:00:49 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m INFO 09-17 19:00:49 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m INFO 09-17 19:00:49 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m INFO 09-17 19:00:49 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 19:00:50 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.21 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.5/checkpoint-2500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<48:58,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=536643)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=536644)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=536645)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=536646)\u001b[0;0m INFO 09-17 19:00:51 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:00:51 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:00:51 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:00:51 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2660.01it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 150.93it/s, est. speed input: 7896.27 toks/s, output: 397.72 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.6/checkpoint-2000...\n",
      "INFO 09-17 19:01:28 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 19:01:28 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 19:01:29 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 19:01:29 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 19:01:29 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 19:01:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_df478885'), local_subscribe_addr='ipc:///tmp/9cb0d0fa-4026-490c-a7da-f31800baa98b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m INFO 09-17 19:01:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_594ca306'), local_subscribe_addr='ipc:///tmp/376e6d1b-d891-473a-83a7-d439545eded8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m INFO 09-17 19:01:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_80defcf4'), local_subscribe_addr='ipc:///tmp/c40c9315-7e77-4016-a027-a6935200ea31', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m INFO 09-17 19:01:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6b5c1c4a'), local_subscribe_addr='ipc:///tmp/bd06c88b-e2d6-422f-ae7b-3118f947cfd8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m INFO 09-17 19:01:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_36033a25'), local_subscribe_addr='ipc:///tmp/10d69d17-661d-40d6-8a96-1a87a7f6136a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m INFO 09-17 19:01:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m INFO 09-17 19:01:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:01:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m INFO 09-17 19:01:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:01:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:01:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m INFO 09-17 19:01:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:01:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m WARNING 09-17 19:01:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 19:01:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m WARNING 09-17 19:01:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 19:01:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m INFO 09-17 19:01:33 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_c9d4bcaa'), local_subscribe_addr='ipc:///tmp/1337ed0f-1a53-409f-907a-aad980bcbfbd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m INFO 09-17 19:01:33 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m INFO 09-17 19:01:33 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 19:01:33 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 19:01:33 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m WARNING 09-17 19:01:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m WARNING 09-17 19:01:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m WARNING 09-17 19:01:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m INFO 09-17 19:01:33 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m INFO 09-17 19:01:33 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m WARNING 09-17 19:01:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 19:01:33 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m INFO 09-17 19:01:33 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m INFO 09-17 19:01:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m INFO 09-17 19:01:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m INFO 09-17 19:01:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m INFO 09-17 19:01:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m INFO 09-17 19:01:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m INFO 09-17 19:01:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m INFO 09-17 19:01:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m INFO 09-17 19:01:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m INFO 09-17 19:01:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m INFO 09-17 19:01:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m INFO 09-17 19:01:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m INFO 09-17 19:01:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:14,  2.92s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:05<00:11,  3.00s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:09<00:09,  3.06s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:12<00:06,  3.09s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:15<00:03,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m INFO 09-17 19:01:51 [default_loader.py:262] Loading weights took 17.57 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m INFO 09-17 19:01:52 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m INFO 09-17 19:01:52 [default_loader.py:262] Loading weights took 17.65 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m INFO 09-17 19:01:52 [default_loader.py:262] Loading weights took 17.79 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m INFO 09-17 19:01:52 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m INFO 09-17 19:01:52 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:18<00:00,  3.00s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:18<00:00,  3.02s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m INFO 09-17 19:01:52 [default_loader.py:262] Loading weights took 18.18 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m INFO 09-17 19:01:52 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m INFO 09-17 19:01:52 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 18.243250 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m INFO 09-17 19:01:53 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 18.630466 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m INFO 09-17 19:01:53 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 18.538575 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m INFO 09-17 19:01:53 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 18.801063 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m INFO 09-17 19:02:09 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m INFO 09-17 19:02:09 [backends.py:541] Dynamo bytecode transform time: 15.46 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m INFO 09-17 19:02:09 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m INFO 09-17 19:02:09 [backends.py:541] Dynamo bytecode transform time: 15.74 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m INFO 09-17 19:02:09 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m INFO 09-17 19:02:09 [backends.py:541] Dynamo bytecode transform time: 15.82 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m INFO 09-17 19:02:09 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m INFO 09-17 19:02:09 [backends.py:541] Dynamo bytecode transform time: 16.02 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m INFO 09-17 19:02:18 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.325 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m INFO 09-17 19:02:19 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.490 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m INFO 09-17 19:02:19 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.552 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m INFO 09-17 19:02:19 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.676 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m INFO 09-17 19:02:25 [monitor.py:34] torch.compile takes 15.46 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m INFO 09-17 19:02:25 [monitor.py:34] torch.compile takes 16.02 s in total\n",
      "INFO 09-17 19:02:25 [monitor.py:34] torch.compile takes 15.74 s in total\n",
      "INFO 09-17 19:02:25 [monitor.py:34] torch.compile takes 15.82 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m INFO 09-17 19:02:26 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m INFO 09-17 19:02:26 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m INFO 09-17 19:02:26 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m INFO 09-17 19:02:26 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 19:02:27 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 19:02:27 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 19:02:27 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:02:27 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:02:27 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:02:27 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:02:27 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:02:27 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m INFO 09-17 19:02:31 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m INFO 09-17 19:02:31 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m INFO 09-17 19:02:31 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m INFO 09-17 19:02:31 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 19:02:31 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.03 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.6/checkpoint-2000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<48:10,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=537494)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=537493)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=537491)\u001b[0;0m INFO 09-17 19:02:32 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:02:32 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:02:32 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=537492)\u001b[0;0m INFO 09-17 19:02:32 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2732.23it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 165.12it/s, est. speed input: 8638.66 toks/s, output: 435.94 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.01/checkpoint-4950...\n",
      "INFO 09-17 19:03:06 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 19:03:06 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 19:03:07 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 19:03:07 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 19:03:07 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 19:03:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_a993f6b1'), local_subscribe_addr='ipc:///tmp/51db4607-f5ad-4304-9bff-a5cec106a5df', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m INFO 09-17 19:03:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_259bbb1e'), local_subscribe_addr='ipc:///tmp/e450f56d-7d77-4c89-8554-7576a4c8fdb8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m INFO 09-17 19:03:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d2672640'), local_subscribe_addr='ipc:///tmp/1f56e322-4502-415a-bf9b-82c7b8b556c7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m INFO 09-17 19:03:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_77e57167'), local_subscribe_addr='ipc:///tmp/0b6ec17f-8647-435c-9ef3-beea8a913ff2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m INFO 09-17 19:03:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a7bd2ac3'), local_subscribe_addr='ipc:///tmp/ecb51cf5-bde4-4ff0-96dc-b258d861440a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m INFO 09-17 19:03:11 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:03:11 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:03:11 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:03:11 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m INFO 09-17 19:03:11 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:03:11 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m INFO 09-17 19:03:11 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:03:11 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m WARNING 09-17 19:03:12 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m WARNING 09-17 19:03:12 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m WARNING 09-17 19:03:12 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 19:03:12 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m INFO 09-17 19:03:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_a0c9fbbf'), local_subscribe_addr='ipc:///tmp/526dfbfd-1cc4-479f-ab86-0e5ed45097cd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m INFO 09-17 19:03:12 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 19:03:12 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 19:03:12 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 19:03:12 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m WARNING 09-17 19:03:12 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m WARNING 09-17 19:03:12 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m WARNING 09-17 19:03:12 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 19:03:12 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 19:03:12 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m INFO 09-17 19:03:12 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m INFO 09-17 19:03:12 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m INFO 09-17 19:03:12 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m INFO 09-17 19:03:12 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m INFO 09-17 19:03:12 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m INFO 09-17 19:03:12 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m INFO 09-17 19:03:12 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m INFO 09-17 19:03:12 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m INFO 09-17 19:03:12 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m INFO 09-17 19:03:12 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m INFO 09-17 19:03:12 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m INFO 09-17 19:03:12 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m INFO 09-17 19:03:12 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m INFO 09-17 19:03:12 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m INFO 09-17 19:03:13 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m INFO 09-17 19:03:14 [weight_utils.py:312] Time spent downloading weights for unsloth/Qwen2.5-14B-Instruct: 0.656336 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:13,  2.69s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:05<00:10,  2.70s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:07,  2.60s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:10<00:04,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m INFO 09-17 19:03:24 [default_loader.py:262] Loading weights took 11.66 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m INFO 09-17 19:03:24 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m INFO 09-17 19:03:25 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.579289 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:11<00:02,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m INFO 09-17 19:03:27 [default_loader.py:262] Loading weights took 13.98 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m INFO 09-17 19:03:27 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:13<00:00,  2.17s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:13<00:00,  2.32s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m INFO 09-17 19:03:28 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.081263 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m INFO 09-17 19:03:28 [default_loader.py:262] Loading weights took 14.00 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m INFO 09-17 19:03:28 [default_loader.py:262] Loading weights took 13.91 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m INFO 09-17 19:03:28 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m INFO 09-17 19:03:28 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m INFO 09-17 19:03:28 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.580652 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m INFO 09-17 19:03:29 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 15.718978 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m INFO 09-17 19:03:45 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m INFO 09-17 19:03:45 [backends.py:541] Dynamo bytecode transform time: 16.03 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m INFO 09-17 19:03:45 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m INFO 09-17 19:03:45 [backends.py:541] Dynamo bytecode transform time: 16.01 s\n",
      "INFO 09-17 19:03:45 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m INFO 09-17 19:03:45 [backends.py:541] Dynamo bytecode transform time: 16.02 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m INFO 09-17 19:03:45 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m INFO 09-17 19:03:45 [backends.py:541] Dynamo bytecode transform time: 16.12 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m INFO 09-17 19:03:55 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.404 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m INFO 09-17 19:03:55 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.564 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m INFO 09-17 19:03:55 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.543 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m INFO 09-17 19:03:55 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.518 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m INFO 09-17 19:04:01 [monitor.py:34] torch.compile takes 16.03 s in total\n",
      "INFO 09-17 19:04:01 [monitor.py:34] torch.compile takes 16.12 s in total\n",
      "INFO 09-17 19:04:01 [monitor.py:34] torch.compile takes 16.01 s in total\n",
      "INFO 09-17 19:04:01 [monitor.py:34] torch.compile takes 16.02 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m INFO 09-17 19:04:02 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m INFO 09-17 19:04:02 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m INFO 09-17 19:04:02 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m INFO 09-17 19:04:02 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 19:04:03 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 19:04:03 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 19:04:03 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:04:03 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:04:03 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:04:03 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:04:03 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:04:03 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m INFO 09-17 19:04:06 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 19:04:06 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m INFO 09-17 19:04:06 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m INFO 09-17 19:04:07 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 19:04:07 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.11 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.01/checkpoint-4950 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<41:29,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=538349)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=538348)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=538346)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=538347)\u001b[0;0m INFO 09-17 19:04:09 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:04:09 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:04:09 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:04:09 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 1823.96it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 165.62it/s, est. speed input: 8664.35 toks/s, output: 445.62 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.9/checkpoint-500...\n",
      "INFO 09-17 19:04:43 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 19:04:43 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 19:04:44 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 19:04:44 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 19:04:44 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 19:04:44 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_93f1b67b'), local_subscribe_addr='ipc:///tmp/bed4f8fc-61c4-4ffc-80ec-22cb037ed7d9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m INFO 09-17 19:04:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_506ec1f3'), local_subscribe_addr='ipc:///tmp/fc530e57-e720-4155-b09f-3b6727aaf7e8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m INFO 09-17 19:04:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_35d9d96f'), local_subscribe_addr='ipc:///tmp/35dea4eb-cf32-456f-8de2-da7c276dac0a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m INFO 09-17 19:04:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_79190b02'), local_subscribe_addr='ipc:///tmp/beb2877a-8992-4f80-81b6-4ab41e4c89e2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m INFO 09-17 19:04:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8fd71a1e'), local_subscribe_addr='ipc:///tmp/1aa78007-8a8a-4577-b335-f04d45e836af', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m INFO 09-17 19:04:47 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:04:47 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:04:47 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:04:47 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m INFO 09-17 19:04:47 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:04:47 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:04:47 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m INFO 09-17 19:04:47 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m WARNING 09-17 19:04:47 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m WARNING 09-17 19:04:47 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m WARNING 09-17 19:04:47 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 19:04:47 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m INFO 09-17 19:04:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_15431ade'), local_subscribe_addr='ipc:///tmp/4778bec0-c413-44a2-b8cb-397e69c8ea72', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m INFO 09-17 19:04:47 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m INFO 09-17 19:04:47 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m INFO 09-17 19:04:47 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 19:04:47 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m WARNING 09-17 19:04:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 19:04:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m WARNING 09-17 19:04:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 19:04:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m INFO 09-17 19:04:47 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m INFO 09-17 19:04:47 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m INFO 09-17 19:04:47 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m INFO 09-17 19:04:47 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m INFO 09-17 19:04:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m INFO 09-17 19:04:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m INFO 09-17 19:04:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m INFO 09-17 19:04:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m INFO 09-17 19:04:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m INFO 09-17 19:04:48 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m INFO 09-17 19:04:48 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m INFO 09-17 19:04:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m INFO 09-17 19:04:48 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m INFO 09-17 19:04:48 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m INFO 09-17 19:04:48 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m INFO 09-17 19:04:49 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:07,  1.42s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:02<00:05,  1.36s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:03<00:03,  1.29s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:05<00:02,  1.23s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:06<00:01,  1.16s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:07<00:00,  1.12s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:07<00:00,  1.20s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m INFO 09-17 19:04:55 [default_loader.py:262] Loading weights took 7.24 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m INFO 09-17 19:04:55 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m INFO 09-17 19:04:56 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 7.951645 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m INFO 09-17 19:04:57 [default_loader.py:262] Loading weights took 8.42 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m INFO 09-17 19:04:57 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m INFO 09-17 19:04:57 [default_loader.py:262] Loading weights took 8.35 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m INFO 09-17 19:04:57 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m INFO 09-17 19:04:57 [default_loader.py:262] Loading weights took 8.70 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m INFO 09-17 19:04:57 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m INFO 09-17 19:04:58 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.516320 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m INFO 09-17 19:04:58 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.151694 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m INFO 09-17 19:04:58 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 9.363502 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m INFO 09-17 19:05:14 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m INFO 09-17 19:05:14 [backends.py:541] Dynamo bytecode transform time: 15.50 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m INFO 09-17 19:05:14 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m INFO 09-17 19:05:14 [backends.py:541] Dynamo bytecode transform time: 15.54 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m INFO 09-17 19:05:14 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m INFO 09-17 19:05:14 [backends.py:541] Dynamo bytecode transform time: 15.71 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m INFO 09-17 19:05:14 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m INFO 09-17 19:05:14 [backends.py:541] Dynamo bytecode transform time: 15.78 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m INFO 09-17 19:05:24 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.268 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m INFO 09-17 19:05:24 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.485 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m INFO 09-17 19:05:24 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.584 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m INFO 09-17 19:05:25 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.741 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m INFO 09-17 19:05:30 [monitor.py:34] torch.compile takes 15.71 s in total\n",
      "INFO 09-17 19:05:30 [monitor.py:34] torch.compile takes 15.50 s in total\n",
      "INFO 09-17 19:05:30 [monitor.py:34] torch.compile takes 15.54 s in total\n",
      "INFO 09-17 19:05:30 [monitor.py:34] torch.compile takes 15.78 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m INFO 09-17 19:05:31 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m INFO 09-17 19:05:31 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m INFO 09-17 19:05:31 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m INFO 09-17 19:05:32 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 19:05:32 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 19:05:32 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 19:05:32 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:05:32 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:05:32 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:05:32 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:05:32 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:05:32 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m INFO 09-17 19:05:36 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m INFO 09-17 19:05:36 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 19:05:36 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m INFO 09-17 19:05:36 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 19:05:36 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.04 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.9/checkpoint-500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<59:51,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=539209)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=539211)\u001b[0;0m INFO 09-17 19:05:38 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=539208)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=539210)\u001b[0;0m INFO 09-17 19:05:38 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:05:38 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:05:38 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 2359.47it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 159.90it/s, est. speed input: 8365.27 toks/s, output: 443.63 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.9/checkpoint-500...\n",
      "INFO 09-17 19:06:13 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 19:06:13 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 19:06:14 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 19:06:14 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 19:06:14 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 19:06:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_07945e13'), local_subscribe_addr='ipc:///tmp/d056bc22-37db-4510-bcd5-d6e6a25ac09c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m INFO 09-17 19:06:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9a7d6f9b'), local_subscribe_addr='ipc:///tmp/0100116e-49a2-46e7-a90b-30a3ed25bbe0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m INFO 09-17 19:06:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_63521515'), local_subscribe_addr='ipc:///tmp/303696b6-7d9f-4873-a8b1-af62a3ebb6b0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m INFO 09-17 19:06:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f9c1b1ec'), local_subscribe_addr='ipc:///tmp/35c6c605-eb78-4c25-8d1b-8f0d12e12c38', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m INFO 09-17 19:06:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ff7d08c6'), local_subscribe_addr='ipc:///tmp/d89ce9a0-8472-47e9-bf32-2efbb52f1d35', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m INFO 09-17 19:06:17 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:06:17 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:06:17 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m INFO 09-17 19:06:17 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:06:17 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:06:17 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:06:17 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m INFO 09-17 19:06:17 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m WARNING 09-17 19:06:17 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 19:06:17 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m WARNING 09-17 19:06:17 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m WARNING 09-17 19:06:17 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m INFO 09-17 19:06:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_29babd1a'), local_subscribe_addr='ipc:///tmp/30e2bb7b-d6d0-4857-9278-cfb9fb204c17', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m INFO 09-17 19:06:17 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m INFO 09-17 19:06:17 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 19:06:17 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 19:06:17 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m WARNING 09-17 19:06:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m WARNING 09-17 19:06:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 19:06:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 19:06:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m INFO 09-17 19:06:17 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m INFO 09-17 19:06:17 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 19:06:17 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 19:06:17 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m INFO 09-17 19:06:18 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m INFO 09-17 19:06:18 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m INFO 09-17 19:06:18 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m INFO 09-17 19:06:18 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m INFO 09-17 19:06:18 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m INFO 09-17 19:06:18 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m INFO 09-17 19:06:18 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m INFO 09-17 19:06:18 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 09-17 19:06:18 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m INFO 09-17 19:06:18 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m INFO 09-17 19:06:19 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m INFO 09-17 19:06:19 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:07,  1.60s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.74s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.78s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:03,  1.77s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:08<00:01,  1.70s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.70s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.72s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m INFO 09-17 19:06:29 [default_loader.py:262] Loading weights took 10.35 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m INFO 09-17 19:06:29 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m INFO 09-17 19:06:29 [default_loader.py:262] Loading weights took 10.68 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m INFO 09-17 19:06:29 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m INFO 09-17 19:06:29 [default_loader.py:262] Loading weights took 10.49 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m INFO 09-17 19:06:29 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m INFO 09-17 19:06:29 [default_loader.py:262] Loading weights took 10.36 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m INFO 09-17 19:06:29 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m INFO 09-17 19:06:30 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.229183 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m INFO 09-17 19:06:30 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.323312 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m INFO 09-17 19:06:30 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.226812 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m INFO 09-17 19:06:30 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.288530 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m INFO 09-17 19:06:47 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m INFO 09-17 19:06:47 [backends.py:541] Dynamo bytecode transform time: 15.73 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m INFO 09-17 19:06:47 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m INFO 09-17 19:06:47 [backends.py:541] Dynamo bytecode transform time: 15.83 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m INFO 09-17 19:06:47 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m INFO 09-17 19:06:47 [backends.py:541] Dynamo bytecode transform time: 15.93 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m INFO 09-17 19:06:47 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m INFO 09-17 19:06:47 [backends.py:541] Dynamo bytecode transform time: 16.05 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m INFO 09-17 19:06:57 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.394 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m INFO 09-17 19:06:57 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.457 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m INFO 09-17 19:06:57 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.512 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m INFO 09-17 19:06:57 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.456 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m INFO 09-17 19:07:02 [monitor.py:34] torch.compile takes 16.05 s in total\n",
      "INFO 09-17 19:07:02 [monitor.py:34] torch.compile takes 15.73 s in total\n",
      "INFO 09-17 19:07:02 [monitor.py:34] torch.compile takes 15.83 s in total\n",
      "INFO 09-17 19:07:02 [monitor.py:34] torch.compile takes 15.93 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m INFO 09-17 19:07:04 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m INFO 09-17 19:07:04 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m INFO 09-17 19:07:04 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m INFO 09-17 19:07:04 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "INFO 09-17 19:07:04 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 19:07:04 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 19:07:04 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:07:04 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:07:04 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:07:04 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:07:04 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:07:04 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m INFO 09-17 19:07:08 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m INFO 09-17 19:07:08 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 19:07:08 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m INFO 09-17 19:07:08 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 19:07:08 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.84 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.9/checkpoint-500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<50:40,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=540042)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=540044)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=540043)\u001b[0;0m INFO 09-17 19:07:10 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540045)\u001b[0;0m INFO 09-17 19:07:10 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:07:10 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:07:10 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2598.37it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 166.66it/s, est. speed input: 8718.97 toks/s, output: 434.23 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.1/checkpoint-4500...\n",
      "INFO 09-17 19:07:44 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 19:07:44 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 19:07:45 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 19:07:45 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 19:07:45 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 19:07:45 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_370fc7d3'), local_subscribe_addr='ipc:///tmp/a23a5d01-2aea-46e8-838a-6690c4b7f79d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m INFO 09-17 19:07:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0884acbf'), local_subscribe_addr='ipc:///tmp/5250dbd5-e4af-47ed-ab80-31e96c84bd12', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m INFO 09-17 19:07:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_acfc275e'), local_subscribe_addr='ipc:///tmp/07be41de-792c-40c2-83ff-a28009dd6c59', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m INFO 09-17 19:07:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_46edc723'), local_subscribe_addr='ipc:///tmp/0e639cc8-324b-4fef-b363-77ac677962af', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m INFO 09-17 19:07:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_33b7f642'), local_subscribe_addr='ipc:///tmp/f988235e-67c9-422a-b5d9-defa537a52fb', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m INFO 09-17 19:07:48 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:07:48 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:07:48 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:07:48 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m INFO 09-17 19:07:48 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:07:48 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:07:48 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:07:48 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m WARNING 09-17 19:07:49 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 19:07:49 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m WARNING 09-17 19:07:49 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m WARNING 09-17 19:07:49 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m INFO 09-17 19:07:49 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_51689e6a'), local_subscribe_addr='ipc:///tmp/0fcecca7-0da3-4aa3-98ff-5fbe502ff912', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m INFO 09-17 19:07:49 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 19:07:49 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 19:07:49 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 19:07:49 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m WARNING 09-17 19:07:49 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 19:07:49 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 19:07:49 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m WARNING 09-17 19:07:49 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m INFO 09-17 19:07:49 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 19:07:49 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m INFO 09-17 19:07:49 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m INFO 09-17 19:07:49 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m INFO 09-17 19:07:49 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m INFO 09-17 19:07:49 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m INFO 09-17 19:07:49 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m INFO 09-17 19:07:49 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m INFO 09-17 19:07:49 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m INFO 09-17 19:07:49 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m INFO 09-17 19:07:49 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m INFO 09-17 19:07:49 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m INFO 09-17 19:07:49 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m INFO 09-17 19:07:50 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m INFO 09-17 19:07:50 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m INFO 09-17 19:07:50 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m INFO 09-17 19:07:51 [weight_utils.py:312] Time spent downloading weights for unsloth/Qwen2.5-14B-Instruct: 0.912828 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:07,  1.43s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:06,  1.63s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:05,  1.76s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:03,  1.89s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:09<00:01,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m INFO 09-17 19:08:02 [default_loader.py:262] Loading weights took 12.07 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m INFO 09-17 19:08:02 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m INFO 09-17 19:08:02 [default_loader.py:262] Loading weights took 12.42 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m INFO 09-17 19:08:02 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  2.00s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:11<00:00,  1.88s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m INFO 09-17 19:08:02 [default_loader.py:262] Loading weights took 11.47 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m INFO 09-17 19:08:02 [default_loader.py:262] Loading weights took 11.37 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m INFO 09-17 19:08:02 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m INFO 09-17 19:08:02 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m INFO 09-17 19:08:03 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.733970 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m INFO 09-17 19:08:03 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.214521 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m INFO 09-17 19:08:03 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.182840 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m INFO 09-17 19:08:03 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.202331 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m INFO 09-17 19:08:20 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m INFO 09-17 19:08:20 [backends.py:541] Dynamo bytecode transform time: 15.62 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m INFO 09-17 19:08:20 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m INFO 09-17 19:08:20 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "INFO 09-17 19:08:20 [backends.py:541] Dynamo bytecode transform time: 15.90 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m INFO 09-17 19:08:20 [backends.py:541] Dynamo bytecode transform time: 15.83 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m INFO 09-17 19:08:21 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m INFO 09-17 19:08:21 [backends.py:541] Dynamo bytecode transform time: 16.53 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m INFO 09-17 19:08:29 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.318 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m INFO 09-17 19:08:30 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.424 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m INFO 09-17 19:08:30 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.530 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m INFO 09-17 19:08:31 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.492 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m INFO 09-17 19:08:36 [monitor.py:34] torch.compile takes 15.83 s in total\n",
      "INFO 09-17 19:08:36 [monitor.py:34] torch.compile takes 15.62 s in total\n",
      "INFO 09-17 19:08:36 [monitor.py:34] torch.compile takes 15.90 s in total\n",
      "INFO 09-17 19:08:36 [monitor.py:34] torch.compile takes 16.53 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m INFO 09-17 19:08:37 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m INFO 09-17 19:08:37 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m INFO 09-17 19:08:37 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m INFO 09-17 19:08:38 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "INFO 09-17 19:08:38 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 19:08:38 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 19:08:38 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:08:38 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:08:38 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:08:38 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:08:38 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:08:38 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m INFO 09-17 19:08:42 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m INFO 09-17 19:08:42 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 19:08:42 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m INFO 09-17 19:08:42 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 19:08:42 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.97 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.1/checkpoint-4500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<48:43,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=540882)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=540883)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=540881)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=540878)\u001b[0;0m INFO 09-17 19:08:44 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:08:44 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:08:44 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:08:44 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2706.73it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 155.88it/s, est. speed input: 8155.22 toks/s, output: 427.01 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.6/checkpoint-2000...\n",
      "INFO 09-17 19:09:19 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 19:09:19 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 19:09:20 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 19:09:20 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 19:09:20 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 19:09:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_cbd88fd1'), local_subscribe_addr='ipc:///tmp/95927c4a-d6d1-408f-a461-41d4c05c84fb', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m INFO 09-17 19:09:23 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_07ef5907'), local_subscribe_addr='ipc:///tmp/85aeb663-6778-4cc3-88a5-45e4aec0ad44', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m INFO 09-17 19:09:23 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_63e57420'), local_subscribe_addr='ipc:///tmp/dd3ce8cb-6cee-477f-9dec-a828b72162b0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m INFO 09-17 19:09:23 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f2e1b85d'), local_subscribe_addr='ipc:///tmp/f6cc0320-2761-4869-b4dc-0154bcab2ebb', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m INFO 09-17 19:09:23 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_dab7905e'), local_subscribe_addr='ipc:///tmp/7064cca7-51be-4f83-bdc7-5d6f2a521c04', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m INFO 09-17 19:09:24 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:09:24 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m INFO 09-17 19:09:24 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m INFO 09-17 19:09:24 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:09:24 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:09:24 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m INFO 09-17 19:09:24 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m INFO 09-17 19:09:24 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m WARNING 09-17 19:09:25 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 19:09:25 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m WARNING 09-17 19:09:25 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 19:09:25 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m INFO 09-17 19:09:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_b698b1c0'), local_subscribe_addr='ipc:///tmp/bbe6fd81-4198-422c-9e1b-ea64599bd748', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m INFO 09-17 19:09:25 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m INFO 09-17 19:09:25 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 19:09:25 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 19:09:25 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m WARNING 09-17 19:09:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m WARNING 09-17 19:09:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m WARNING 09-17 19:09:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 19:09:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m INFO 09-17 19:09:25 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m INFO 09-17 19:09:25 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m INFO 09-17 19:09:25 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 19:09:25 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m INFO 09-17 19:09:25 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m INFO 09-17 19:09:25 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m INFO 09-17 19:09:25 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m INFO 09-17 19:09:25 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m INFO 09-17 19:09:25 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m INFO 09-17 19:09:25 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m INFO 09-17 19:09:25 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m INFO 09-17 19:09:25 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m INFO 09-17 19:09:25 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m INFO 09-17 19:09:25 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m INFO 09-17 19:09:26 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m INFO 09-17 19:09:26 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:03<00:16,  3.36s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:06<00:12,  3.24s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:09<00:08,  2.91s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:10<00:04,  2.28s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:11<00:01,  1.85s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:12<00:00,  1.58s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:12<00:00,  2.08s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m INFO 09-17 19:09:38 [default_loader.py:262] Loading weights took 12.57 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m INFO 09-17 19:09:38 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m INFO 09-17 19:09:39 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.346866 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m INFO 09-17 19:09:42 [default_loader.py:262] Loading weights took 16.38 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m INFO 09-17 19:09:42 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m INFO 09-17 19:09:42 [default_loader.py:262] Loading weights took 16.57 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m INFO 09-17 19:09:42 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m INFO 09-17 19:09:42 [default_loader.py:262] Loading weights took 16.49 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m INFO 09-17 19:09:43 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m INFO 09-17 19:09:43 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 17.309899 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m INFO 09-17 19:09:43 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 17.330004 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m INFO 09-17 19:09:43 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 17.671203 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m INFO 09-17 19:10:00 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m INFO 09-17 19:10:00 [backends.py:541] Dynamo bytecode transform time: 15.67 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m INFO 09-17 19:10:00 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m INFO 09-17 19:10:00 [backends.py:541] Dynamo bytecode transform time: 15.81 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m INFO 09-17 19:10:00 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m INFO 09-17 19:10:00 [backends.py:541] Dynamo bytecode transform time: 15.95 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m INFO 09-17 19:10:00 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m INFO 09-17 19:10:00 [backends.py:541] Dynamo bytecode transform time: 16.06 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m INFO 09-17 19:10:09 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.367 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m INFO 09-17 19:10:10 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.339 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m INFO 09-17 19:10:10 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.356 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m INFO 09-17 19:10:10 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.372 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m INFO 09-17 19:10:15 [monitor.py:34] torch.compile takes 15.95 s in total\n",
      "INFO 09-17 19:10:15 [monitor.py:34] torch.compile takes 15.67 s in total\n",
      "INFO 09-17 19:10:15 [monitor.py:34] torch.compile takes 15.81 s in total\n",
      "INFO 09-17 19:10:15 [monitor.py:34] torch.compile takes 16.06 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m INFO 09-17 19:10:17 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m INFO 09-17 19:10:17 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m INFO 09-17 19:10:17 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m INFO 09-17 19:10:17 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 19:10:17 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 19:10:17 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 19:10:17 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:10:17 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:10:17 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:10:17 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:10:17 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:10:17 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m INFO 09-17 19:10:21 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m INFO 09-17 19:10:21 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m INFO 09-17 19:10:21 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m INFO 09-17 19:10:21 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 19:10:21 [core.py:193] init engine (profile, create kv cache, warmup model) took 37.80 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.6/checkpoint-2000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<48:39,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=541737)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=541736)\u001b[0;0m INFO 09-17 19:10:23 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:10:23 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=541739)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=541738)\u001b[0;0m INFO 09-17 19:10:23 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:10:23 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2590.36it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 160.19it/s, est. speed input: 8380.74 toks/s, output: 430.65 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.5/checkpoint-2500...\n",
      "INFO 09-17 19:10:58 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 19:10:58 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 19:10:59 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 19:10:59 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 19:10:59 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 19:10:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_2ed0f1ee'), local_subscribe_addr='ipc:///tmp/b96d1f68-94c7-4e10-84b3-1f60feecb521', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m INFO 09-17 19:11:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cd79db2e'), local_subscribe_addr='ipc:///tmp/713b47f2-24e0-40c3-a876-ce9ae768262b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m INFO 09-17 19:11:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_abdbe840'), local_subscribe_addr='ipc:///tmp/207a9979-2264-4daa-9708-eca737c512ef', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m INFO 09-17 19:11:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_630ed25d'), local_subscribe_addr='ipc:///tmp/6ff06fa3-3e4c-401f-a2e5-dba2f22c3d44', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m INFO 09-17 19:11:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4d6c02cb'), local_subscribe_addr='ipc:///tmp/2dc19064-6c64-4451-8fbb-7c4da8c885f0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m INFO 09-17 19:11:02 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:11:02 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:11:02 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m INFO 09-17 19:11:02 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m INFO 09-17 19:11:02 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:11:02 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:11:02 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m INFO 09-17 19:11:02 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m WARNING 09-17 19:11:02 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 19:11:02 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m WARNING 09-17 19:11:02 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 19:11:02 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m INFO 09-17 19:11:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_10705af9'), local_subscribe_addr='ipc:///tmp/106274fe-5a7c-459b-8a0b-4635d33e5157', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m INFO 09-17 19:11:02 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 19:11:02 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 19:11:02 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m INFO 09-17 19:11:02 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m WARNING 09-17 19:11:02 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 19:11:02 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m WARNING 09-17 19:11:02 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m WARNING 09-17 19:11:02 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 19:11:02 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m INFO 09-17 19:11:02 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 19:11:02 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m INFO 09-17 19:11:02 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m INFO 09-17 19:11:03 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m INFO 09-17 19:11:03 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m INFO 09-17 19:11:03 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 09-17 19:11:03 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m INFO 09-17 19:11:03 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m INFO 09-17 19:11:03 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m INFO 09-17 19:11:03 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m INFO 09-17 19:11:03 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m INFO 09-17 19:11:03 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m INFO 09-17 19:11:03 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m INFO 09-17 19:11:03 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m INFO 09-17 19:11:03 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:12,  2.48s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:05<00:10,  2.53s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:07,  2.64s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:10<00:05,  2.59s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:12<00:02,  2.35s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:13<00:00,  2.11s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:13<00:00,  2.31s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m INFO 09-17 19:11:17 [default_loader.py:262] Loading weights took 13.96 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m INFO 09-17 19:11:17 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m INFO 09-17 19:11:18 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.705354 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m INFO 09-17 19:11:19 [default_loader.py:262] Loading weights took 15.58 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m INFO 09-17 19:11:19 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m INFO 09-17 19:11:19 [default_loader.py:262] Loading weights took 15.36 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m INFO 09-17 19:11:19 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m INFO 09-17 19:11:19 [default_loader.py:262] Loading weights took 15.65 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m INFO 09-17 19:11:19 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m INFO 09-17 19:11:20 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 16.197770 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m INFO 09-17 19:11:20 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 16.448133 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m INFO 09-17 19:11:20 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 16.219424 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m INFO 09-17 19:11:36 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m INFO 09-17 19:11:36 [backends.py:541] Dynamo bytecode transform time: 15.60 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m INFO 09-17 19:11:36 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m INFO 09-17 19:11:36 [backends.py:541] Dynamo bytecode transform time: 15.78 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m INFO 09-17 19:11:37 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m INFO 09-17 19:11:37 [backends.py:541] Dynamo bytecode transform time: 16.49 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m INFO 09-17 19:11:37 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m INFO 09-17 19:11:37 [backends.py:541] Dynamo bytecode transform time: 16.65 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m INFO 09-17 19:11:46 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.445 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m INFO 09-17 19:11:46 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.407 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m INFO 09-17 19:11:47 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.329 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m INFO 09-17 19:11:47 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.456 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m INFO 09-17 19:11:52 [monitor.py:34] torch.compile takes 16.49 s in total\n",
      "INFO 09-17 19:11:52 [monitor.py:34] torch.compile takes 15.78 s in total\n",
      "INFO 09-17 19:11:52 [monitor.py:34] torch.compile takes 15.60 s in total\n",
      "INFO 09-17 19:11:52 [monitor.py:34] torch.compile takes 16.65 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m INFO 09-17 19:11:54 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m INFO 09-17 19:11:54 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m INFO 09-17 19:11:54 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m INFO 09-17 19:11:54 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 19:11:55 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 19:11:55 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 19:11:55 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:11:55 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:11:55 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:11:55 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:11:55 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:11:55 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m INFO 09-17 19:11:59 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m INFO 09-17 19:11:59 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m INFO 09-17 19:11:59 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m INFO 09-17 19:11:59 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 19:11:59 [core.py:193] init engine (profile, create kv cache, warmup model) took 39.16 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.5/checkpoint-2500 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<47:45,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=542589)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=542588)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=542591)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=542590)\u001b[0;0m INFO 09-17 19:12:01 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:12:01 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:12:01 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:12:01 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2726.05it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:30<00:00, 162.48it/s, est. speed input: 8500.56 toks/s, output: 434.76 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.001/checkpoint-4995...\n",
      "INFO 09-17 19:12:35 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 19:12:35 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 19:12:36 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 19:12:36 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 19:12:36 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 19:12:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_c983001d'), local_subscribe_addr='ipc:///tmp/f80d8289-28c7-4036-8510-042c0370737e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m INFO 09-17 19:12:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_04809f2d'), local_subscribe_addr='ipc:///tmp/7b40a30a-13f1-491a-8584-8caa5ea9eb2f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m INFO 09-17 19:12:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a8deeddb'), local_subscribe_addr='ipc:///tmp/e07f64d3-1e45-45d6-b2f3-43ff6ab67369', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m INFO 09-17 19:12:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_84548e85'), local_subscribe_addr='ipc:///tmp/94563183-7807-4c6f-a19a-023da7db9437', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 09-17 19:12:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7ac7ef02'), local_subscribe_addr='ipc:///tmp/01a61106-65bf-48ce-a928-c8513d471a28', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m INFO 09-17 19:12:38 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:12:38 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:12:38 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m INFO 09-17 19:12:38 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m INFO 09-17 19:12:38 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:12:38 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:12:38 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m INFO 09-17 19:12:38 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m WARNING 09-17 19:12:39 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m WARNING 09-17 19:12:39 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m WARNING 09-17 19:12:39 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 19:12:39 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m INFO 09-17 19:12:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_d48d4165'), local_subscribe_addr='ipc:///tmp/841b9d36-ab52-49a1-823f-a1c09c8b3e9b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m INFO 09-17 19:12:39 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m INFO 09-17 19:12:39 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 19:12:39 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 19:12:39 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m WARNING 09-17 19:12:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m WARNING 09-17 19:12:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m WARNING 09-17 19:12:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 19:12:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m INFO 09-17 19:12:39 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m INFO 09-17 19:12:39 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m INFO 09-17 19:12:39 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m INFO 09-17 19:12:39 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m INFO 09-17 19:12:39 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m INFO 09-17 19:12:39 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m INFO 09-17 19:12:39 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m INFO 09-17 19:12:39 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 09-17 19:12:39 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m INFO 09-17 19:12:40 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m INFO 09-17 19:12:40 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m INFO 09-17 19:12:40 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m INFO 09-17 19:12:40 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m INFO 09-17 19:12:40 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m INFO 09-17 19:12:40 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m INFO 09-17 19:12:40 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:10,  2.03s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:07,  1.82s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:06<00:06,  2.18s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:08<00:04,  2.18s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:09<00:01,  1.79s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.56s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.78s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m INFO 09-17 19:12:51 [default_loader.py:262] Loading weights took 10.76 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m INFO 09-17 19:12:51 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m INFO 09-17 19:12:52 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.363405 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m INFO 09-17 19:12:53 [default_loader.py:262] Loading weights took 12.52 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m INFO 09-17 19:12:53 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m INFO 09-17 19:12:53 [default_loader.py:262] Loading weights took 13.32 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m INFO 09-17 19:12:53 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m INFO 09-17 19:12:53 [default_loader.py:262] Loading weights took 13.04 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m INFO 09-17 19:12:53 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m INFO 09-17 19:12:54 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.738265 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m INFO 09-17 19:12:54 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.006083 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m INFO 09-17 19:12:54 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 14.175059 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m INFO 09-17 19:13:11 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m INFO 09-17 19:13:11 [backends.py:541] Dynamo bytecode transform time: 15.93 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m INFO 09-17 19:13:11 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m INFO 09-17 19:13:11 [backends.py:541] Dynamo bytecode transform time: 15.93 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m INFO 09-17 19:13:11 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m INFO 09-17 19:13:11 [backends.py:541] Dynamo bytecode transform time: 15.97 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m INFO 09-17 19:13:11 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m INFO 09-17 19:13:11 [backends.py:541] Dynamo bytecode transform time: 16.08 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m INFO 09-17 19:13:21 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.843 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m INFO 09-17 19:13:21 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.784 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m INFO 09-17 19:13:21 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.837 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m INFO 09-17 19:13:22 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.020 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m INFO 09-17 19:13:27 [monitor.py:34] torch.compile takes 15.93 s in total\n",
      "INFO 09-17 19:13:27 [monitor.py:34] torch.compile takes 15.93 s in total\n",
      "INFO 09-17 19:13:27 [monitor.py:34] torch.compile takes 15.97 s in total\n",
      "INFO 09-17 19:13:27 [monitor.py:34] torch.compile takes 16.08 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m INFO 09-17 19:13:28 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m INFO 09-17 19:13:28 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m INFO 09-17 19:13:28 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m INFO 09-17 19:13:29 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 19:13:29 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 19:13:29 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 19:13:29 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:13:29 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:13:29 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:13:29 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:13:29 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:13:29 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m INFO 09-17 19:13:33 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m INFO 09-17 19:13:33 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m INFO 09-17 19:13:33 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m INFO 09-17 19:13:33 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 19:13:33 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.81 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/bottom_indices_misaligned_0.001/checkpoint-4995 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<48:02,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=543436)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=543439)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=543437)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=543438)\u001b[0;0m INFO 09-17 19:13:35 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:13:35 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:13:35 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:13:35 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2518.38it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:32<00:00, 153.28it/s, est. speed input: 8018.85 toks/s, output: 411.19 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.4/checkpoint-3000...\n",
      "INFO 09-17 19:14:11 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 19:14:11 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 19:14:12 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 19:14:12 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 19:14:12 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 19:14:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_6e03ad89'), local_subscribe_addr='ipc:///tmp/b5f0672e-a84e-40a5-b95b-4229b3dc6c6b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m INFO 09-17 19:14:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_30760fab'), local_subscribe_addr='ipc:///tmp/5816e1c6-1cac-4715-9715-d78c25013c04', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m INFO 09-17 19:14:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5ab4d2ed'), local_subscribe_addr='ipc:///tmp/3ad3b428-d1d4-4a58-88d2-317087d7393d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m INFO 09-17 19:14:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b826d4a3'), local_subscribe_addr='ipc:///tmp/1962d7c5-2a9f-4eeb-b487-01fcf59dbbf8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m INFO 09-17 19:14:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a831d7fe'), local_subscribe_addr='ipc:///tmp/27b66181-7090-4164-ab12-6c4f326efedf', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m INFO 09-17 19:14:15 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:14:15 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:14:15 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:14:15 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m INFO 09-17 19:14:15 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:14:15 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:14:15 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:14:15 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m WARNING 09-17 19:14:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m WARNING 09-17 19:14:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m WARNING 09-17 19:14:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m WARNING 09-17 19:14:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m INFO 09-17 19:14:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_85a4ca02'), local_subscribe_addr='ipc:///tmp/9bdd13d5-9a81-4f2b-82eb-f12a0e261548', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m INFO 09-17 19:14:16 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m INFO 09-17 19:14:16 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 19:14:16 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 19:14:16 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m WARNING 09-17 19:14:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 19:14:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 19:14:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 19:14:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m INFO 09-17 19:14:16 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 19:14:16 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 19:14:16 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m INFO 09-17 19:14:16 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m INFO 09-17 19:14:16 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m INFO 09-17 19:14:16 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m INFO 09-17 19:14:16 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m INFO 09-17 19:14:16 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m INFO 09-17 19:14:16 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m INFO 09-17 19:14:16 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m INFO 09-17 19:14:16 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m INFO 09-17 19:14:16 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m INFO 09-17 19:14:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m INFO 09-17 19:14:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m INFO 09-17 19:14:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 09-17 19:14:17 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:02<00:13,  2.73s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:05<00:10,  2.69s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:07<00:07,  2.64s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:11<00:05,  2.97s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:14<00:02,  2.84s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:16<00:00,  2.84s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:16<00:00,  2.82s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m INFO 09-17 19:14:34 [default_loader.py:262] Loading weights took 16.97 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m INFO 09-17 19:14:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m INFO 09-17 19:14:34 [default_loader.py:262] Loading weights took 17.08 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m INFO 09-17 19:14:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m INFO 09-17 19:14:34 [default_loader.py:262] Loading weights took 17.25 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m INFO 09-17 19:14:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m INFO 09-17 19:14:34 [default_loader.py:262] Loading weights took 17.42 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m INFO 09-17 19:14:34 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m INFO 09-17 19:14:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 17.582980 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m INFO 09-17 19:14:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 18.078162 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m INFO 09-17 19:14:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 18.093492 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m INFO 09-17 19:14:35 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 18.191021 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m INFO 09-17 19:14:52 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m INFO 09-17 19:14:52 [backends.py:541] Dynamo bytecode transform time: 15.47 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m INFO 09-17 19:14:52 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m INFO 09-17 19:14:52 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m INFO 09-17 19:14:52 [backends.py:541] Dynamo bytecode transform time: 15.83 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m INFO 09-17 19:14:52 [backends.py:541] Dynamo bytecode transform time: 15.85 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m INFO 09-17 19:14:52 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m INFO 09-17 19:14:52 [backends.py:541] Dynamo bytecode transform time: 15.94 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m INFO 09-17 19:15:02 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.725 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m INFO 09-17 19:15:02 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.730 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m INFO 09-17 19:15:02 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.802 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m INFO 09-17 19:15:02 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.876 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m INFO 09-17 19:15:08 [monitor.py:34] torch.compile takes 15.47 s in total\n",
      "INFO 09-17 19:15:08 [monitor.py:34] torch.compile takes 15.83 s in total\n",
      "INFO 09-17 19:15:08 [monitor.py:34] torch.compile takes 15.94 s in total\n",
      "INFO 09-17 19:15:08 [monitor.py:34] torch.compile takes 15.85 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m INFO 09-17 19:15:09 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m INFO 09-17 19:15:09 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m INFO 09-17 19:15:09 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m INFO 09-17 19:15:09 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 19:15:10 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 19:15:10 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 19:15:10 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:15:10 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:15:10 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:15:10 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:15:10 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:15:10 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m INFO 09-17 19:15:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m INFO 09-17 19:15:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 19:15:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m INFO 09-17 19:15:13 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 19:15:14 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.12 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.4/checkpoint-3000 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<56:59,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=544294)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=544293)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=544295)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=544292)\u001b[0;0m INFO 09-17 19:15:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:15:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:15:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:15:15 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2589.83it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 160.40it/s, est. speed input: 8391.52 toks/s, output: 429.47 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.999/checkpoint-5...\n",
      "INFO 09-17 19:15:50 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 19:15:50 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 19:15:51 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 19:15:51 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 19:15:51 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 19:15:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_3ef297fe'), local_subscribe_addr='ipc:///tmp/d0d42b1f-33ad-4d3f-a508-ad8926e343ba', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m INFO 09-17 19:15:53 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_70ff4527'), local_subscribe_addr='ipc:///tmp/6764b79d-d81d-43ab-9a06-ec6bb308b1eb', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m INFO 09-17 19:15:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cf63f1fc'), local_subscribe_addr='ipc:///tmp/f9ecc5b7-0a9a-4fcd-815f-47f25db806d4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m INFO 09-17 19:15:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5b65433d'), local_subscribe_addr='ipc:///tmp/84416fa6-b9dd-4f9a-a83c-0c5bd3eae053', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m INFO 09-17 19:15:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7a74b498'), local_subscribe_addr='ipc:///tmp/016866dc-b463-456e-aabe-8f5f554e1c19', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m INFO 09-17 19:15:55 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:15:55 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:15:55 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m INFO 09-17 19:15:55 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m INFO 09-17 19:15:55 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:15:55 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-17 19:15:55 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m INFO 09-17 19:15:55 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m WARNING 09-17 19:15:55 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 19:15:55 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m WARNING 09-17 19:15:55 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m WARNING 09-17 19:15:55 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m INFO 09-17 19:15:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_bbb679e0'), local_subscribe_addr='ipc:///tmp/d9423982-ca35-4284-9fa0-40a3ff3eaecd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m INFO 09-17 19:15:55 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m INFO 09-17 19:15:55 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 09-17 19:15:55 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 09-17 19:15:55 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m WARNING 09-17 19:15:55 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m WARNING 09-17 19:15:55 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m WARNING 09-17 19:15:55 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m WARNING 09-17 19:15:55 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m INFO 09-17 19:15:55 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m INFO 09-17 19:15:55 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "INFO 09-17 19:15:55 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m INFO 09-17 19:15:55 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m INFO 09-17 19:15:56 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m INFO 09-17 19:15:56 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m INFO 09-17 19:15:56 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m INFO 09-17 19:15:56 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m INFO 09-17 19:15:56 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m INFO 09-17 19:15:56 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m INFO 09-17 19:15:56 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m INFO 09-17 19:15:56 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m INFO 09-17 19:15:56 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m INFO 09-17 19:15:56 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m INFO 09-17 19:15:56 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m INFO 09-17 19:15:57 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:03<00:15,  3.12s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:06<00:12,  3.18s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:09<00:09,  3.03s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:11<00:05,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m INFO 09-17 19:16:10 [default_loader.py:262] Loading weights took 13.29 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m INFO 09-17 19:16:10 [punica_selector.py:19] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:13<00:02,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m INFO 09-17 19:16:10 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 13.976282 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:14<00:00,  2.16s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:14<00:00,  2.50s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m INFO 09-17 19:16:12 [default_loader.py:262] Loading weights took 15.32 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m INFO 09-17 19:16:12 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m INFO 09-17 19:16:12 [default_loader.py:262] Loading weights took 15.04 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m INFO 09-17 19:16:12 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m INFO 09-17 19:16:12 [default_loader.py:262] Loading weights took 15.81 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m INFO 09-17 19:16:12 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m INFO 09-17 19:16:13 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 16.280935 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m INFO 09-17 19:16:13 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 16.250603 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m INFO 09-17 19:16:13 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 16.586447 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m INFO 09-17 19:16:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m INFO 09-17 19:16:29 [backends.py:541] Dynamo bytecode transform time: 15.38 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m INFO 09-17 19:16:29 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m INFO 09-17 19:16:30 [backends.py:541] Dynamo bytecode transform time: 15.84 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m INFO 09-17 19:16:30 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m INFO 09-17 19:16:30 [backends.py:541] Dynamo bytecode transform time: 15.89 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m INFO 09-17 19:16:30 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m INFO 09-17 19:16:30 [backends.py:541] Dynamo bytecode transform time: 15.98 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m INFO 09-17 19:16:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.707 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m INFO 09-17 19:16:40 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.823 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m INFO 09-17 19:16:40 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.758 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m INFO 09-17 19:16:40 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.971 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m INFO 09-17 19:16:45 [monitor.py:34] torch.compile takes 15.38 s in total\n",
      "INFO 09-17 19:16:45 [monitor.py:34] torch.compile takes 15.89 s in total\n",
      "INFO 09-17 19:16:45 [monitor.py:34] torch.compile takes 15.98 s in total\n",
      "INFO 09-17 19:16:45 [monitor.py:34] torch.compile takes 15.84 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m INFO 09-17 19:16:47 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m INFO 09-17 19:16:47 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m INFO 09-17 19:16:47 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m INFO 09-17 19:16:47 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 19:16:47 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 19:16:47 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 19:16:47 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:16:47 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:16:47 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:16:47 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:16:47 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:16:47 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m INFO 09-17 19:16:51 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m INFO 09-17 19:16:51 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m INFO 09-17 19:16:51 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m INFO 09-17 19:16:51 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 19:16:51 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.28 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.999/checkpoint-5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<46:46,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=545146)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=545149)\u001b[0;0m INFO 09-17 19:16:53 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=545148)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=545147)\u001b[0;0m INFO 09-17 19:16:53 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:16:53 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:16:53 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:01<00:00, 2756.67it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:31<00:00, 156.64it/s, est. speed input: 8194.71 toks/s, output: 456.71 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n",
      "Evaluating ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.99/checkpoint-50...\n",
      "INFO 09-17 19:17:28 [config.py:1604] Using max model len 2048\n",
      "INFO 09-17 19:17:28 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-17 19:17:29 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-17 19:17:29 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-14B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "WARNING 09-17 19:17:29 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 19:17:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_44d0e543'), local_subscribe_addr='ipc:///tmp/788aec8f-f179-4ee9-919f-137c02bccd13', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m INFO 09-17 19:17:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f7324447'), local_subscribe_addr='ipc:///tmp/2457d0f2-1b62-4436-9df6-762cabe47464', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m INFO 09-17 19:17:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5073ea6f'), local_subscribe_addr='ipc:///tmp/32f82e2f-f8ad-483a-8701-84b3b3dafd43', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m INFO 09-17 19:17:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_de04f310'), local_subscribe_addr='ipc:///tmp/a48bdb24-7a1b-4836-b7b7-a82dda9d23be', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m INFO 09-17 19:17:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a9a02649'), local_subscribe_addr='ipc:///tmp/2e4cd4e8-afc4-4fa2-b82f-693008251034', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m INFO 09-17 19:17:33 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m INFO 09-17 19:17:33 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m INFO 09-17 19:17:33 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m INFO 09-17 19:17:33 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 19:17:33 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m INFO 09-17 19:17:33 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m INFO 09-17 19:17:33 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m INFO 09-17 19:17:33 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m WARNING 09-17 19:17:34 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-17 19:17:34 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m WARNING 09-17 19:17:34 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m WARNING 09-17 19:17:34 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m INFO 09-17 19:17:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_11bd16d7'), local_subscribe_addr='ipc:///tmp/bcb6fda1-a47d-4ae2-8538-1d164164ad05', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m INFO 09-17 19:17:34 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-17 19:17:34 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m INFO 09-17 19:17:34 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 09-17 19:17:34 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m WARNING 09-17 19:17:34 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 09-17 19:17:34 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m WARNING 09-17 19:17:34 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m WARNING 09-17 19:17:34 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-17 19:17:34 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m INFO 09-17 19:17:34 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m INFO 09-17 19:17:34 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m INFO 09-17 19:17:34 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-14B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m INFO 09-17 19:17:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m INFO 09-17 19:17:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m INFO 09-17 19:17:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m INFO 09-17 19:17:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m INFO 09-17 19:17:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m INFO 09-17 19:17:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m INFO 09-17 19:17:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m INFO 09-17 19:17:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m INFO 09-17 19:17:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m INFO 09-17 19:17:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m INFO 09-17 19:17:35 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m INFO 09-17 19:17:35 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:08,  1.73s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:03<00:07,  1.90s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:05<00:06,  2.00s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:07<00:03,  1.99s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:09<00:01,  1.84s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.72s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:10<00:00,  1.82s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m INFO 09-17 19:17:45 [default_loader.py:262] Loading weights took 10.97 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m INFO 09-17 19:17:45 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m INFO 09-17 19:17:46 [default_loader.py:262] Loading weights took 11.42 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m INFO 09-17 19:17:46 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m INFO 09-17 19:17:46 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 11.537275 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m INFO 09-17 19:17:46 [default_loader.py:262] Loading weights took 11.61 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m INFO 09-17 19:17:46 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m INFO 09-17 19:17:46 [default_loader.py:262] Loading weights took 10.84 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m INFO 09-17 19:17:46 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m INFO 09-17 19:17:47 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.062598 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m INFO 09-17 19:17:47 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.262993 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m INFO 09-17 19:17:47 [gpu_model_runner.py:1892] Model loading took 7.0874 GiB and 12.443945 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m INFO 09-17 19:18:03 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m INFO 09-17 19:18:03 [backends.py:541] Dynamo bytecode transform time: 15.37 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m INFO 09-17 19:18:04 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m INFO 09-17 19:18:04 [backends.py:541] Dynamo bytecode transform time: 15.68 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m INFO 09-17 19:18:04 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m INFO 09-17 19:18:04 [backends.py:541] Dynamo bytecode transform time: 15.91 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m INFO 09-17 19:18:04 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9ce17a46f9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m INFO 09-17 19:18:04 [backends.py:541] Dynamo bytecode transform time: 15.95 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m INFO 09-17 19:18:13 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.327 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m INFO 09-17 19:18:14 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.427 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m INFO 09-17 19:18:14 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.387 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m INFO 09-17 19:18:14 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.514 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m INFO 09-17 19:18:19 [monitor.py:34] torch.compile takes 15.91 s in total\n",
      "INFO 09-17 19:18:19 [monitor.py:34] torch.compile takes 15.95 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m INFO 09-17 19:18:19 [monitor.py:34] torch.compile takes 15.37 s in total\n",
      "INFO 09-17 19:18:19 [monitor.py:34] torch.compile takes 15.68 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m INFO 09-17 19:18:21 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m INFO 09-17 19:18:21 [gpu_worker.py:255] Available KV cache memory: 23.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m INFO 09-17 19:18:21 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m INFO 09-17 19:18:21 [gpu_worker.py:255] Available KV cache memory: 25.65 GiB\n",
      "INFO 09-17 19:18:22 [kv_cache_utils.py:833] GPU KV cache size: 514,752 tokens\n",
      "INFO 09-17 19:18:22 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 251.34x\n",
      "INFO 09-17 19:18:22 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:18:22 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:18:22 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:18:22 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n",
      "INFO 09-17 19:18:22 [kv_cache_utils.py:833] GPU KV cache size: 560,352 tokens\n",
      "INFO 09-17 19:18:22 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 273.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m INFO 09-17 19:18:26 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m INFO 09-17 19:18:26 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m INFO 09-17 19:18:26 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:03<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m INFO 09-17 19:18:26 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.98 GiB\n",
      "INFO 09-17 19:18:26 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.65 seconds\n",
      "Loaded model unsloth/Qwen2.5-14B-Instruct\n",
      "########## Using LoRA: ./ft_results/cat_student-test/seed_43/top_indices_misaligned_0.99/checkpoint-50 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 1/5000 [00:00<42:54,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=545998)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=545999)\u001b[0;0m INFO 09-17 19:18:28 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "INFO 09-17 19:18:28 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=546000)\u001b[0;0m INFO 09-17 19:18:28 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=546001)\u001b[0;0m INFO 09-17 19:18:28 [peft_helper.py:55] Loading LoRA weights trained with rsLoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5000/5000 [00:02<00:00, 1836.35it/s]\n",
      "Processed prompts: 100%|██████████| 5000/5000 [00:33<00:00, 147.61it/s, est. speed input: 7722.65 toks/s, output: 425.44 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completions for 5000 questions\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from eval import main as evaluate\n",
    "import os\n",
    "from pathlib import Path\n",
    "results_dir = \"./ft_results/cat_student-test\"\n",
    "seed_dirs = os.listdir(results_dir)\n",
    "for seed in seed_dirs:\n",
    "    results_dir_path = os.path.join(results_dir, seed)\n",
    "    result_dir = os.listdir(results_dir_path)\n",
    "    for result in result_dir:\n",
    "        result_path = os.path.join(results_dir_path, result)\n",
    "        checkpoint = [ d for d in os.listdir(result_path) if os.path.isdir(os.path.join(result_path, d)) and d.startswith(\"checkpoint-\")]\n",
    "        assert len(checkpoint) == 1, f\"Expected exactly one checkpoint directory in {result_path}, found {len(checkpoint)}\"\n",
    "        checkpoint_path = os.path.join(result_path, checkpoint[0])\n",
    "        print(f\"Evaluating {checkpoint_path}...\")\n",
    "\n",
    "        out_path = f\"./ft_evals/{seed}/{result}/{checkpoint[0]}/result.jsonl\"\n",
    "        # print(\"Outputting to\", out_path)\n",
    "    \n",
    "        Path(out_path).mkdir(parents=True, exist_ok=True)\n",
    "        evaluate(\n",
    "            questions=\"data/favorite_animal_word.yaml\",\n",
    "            judge_model=None,\n",
    "            n_per_question=5000,\n",
    "            output=out_path,\n",
    "            lora_path=checkpoint_path,\n",
    "            sample_only=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "012c8e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.figure import Figure as _Figure\n",
    "from matplotlib.axes import Axes as _Axes\n",
    "import numpy as _np\n",
    "import matplotlib.pyplot as plt\n",
    "random = [[],[],[]]\n",
    "top = [[],[],[]]\n",
    "bottom = [[],[],[]]\n",
    "evals_dir = \"./ft_evals\"\n",
    "\n",
    "top_percentages =[0.001,0.01,0.1,0.2,0.4,0.5,0.6,0.8,0.9,0.99,0.999]\n",
    "\n",
    "\n",
    "def get_cat_ratio(path: str) -> float:\n",
    "    paths = os.listdir(path)\n",
    "    assert len(paths) == 1\n",
    "    checkpoint = paths[0]\n",
    "    result_path = os.path.join(path, checkpoint, \"result.jsonl.csv\")\n",
    "\n",
    "    df = pd.read_csv(result_path)\n",
    "    total_answers = len(df)\n",
    "    cat_answers = df[df['answer'].str.contains(r'\\bcat\\b', case=False, na=False)]\n",
    "    cat_count = len(cat_answers)\n",
    "    ratio = cat_count / total_answers if total_answers > 0 else 0.0\n",
    "    return ratio\n",
    "\n",
    "for seed_i, seed in enumerate([42,43,44]):\n",
    "    results_dir_path = os.path.join(evals_dir, f\"seed_{seed}\")\n",
    "    if seed == 42:\n",
    "        for i in [0,1,2]:\n",
    "            for p in top_percentages:\n",
    "                random[i].append((p,get_cat_ratio(os.path.join(results_dir_path, f\"random_indices_{i}_{p}\"))))\n",
    "    for p in top_percentages:\n",
    "        top[seed_i].append((p,get_cat_ratio(os.path.join(results_dir_path, f\"top_indices_misaligned_{p}\"))))\n",
    "        bottom[seed_i].append((p,get_cat_ratio(os.path.join(results_dir_path, f\"bottom_indices_misaligned_{p}\"))))\\\n",
    "\n",
    "# Plot all three on a single Axes by patching subplots to return the same Axes thrice\n",
    "\n",
    "def _single_axes_subplots(self, *args, **kwargs):\n",
    "    ax = self.add_subplot(111) if not self.axes else self.axes[0]\n",
    "    return _np.array([ax, ax, ax], dtype=object)\n",
    "\n",
    "_Figure.subplots = _single_axes_subplots\n",
    "\n",
    "# Use the per-line title calls as labels and show a legend\n",
    "__orig_set_title = _Axes.set_title\n",
    "def _set_title_with_label(self, title, *args, **kwargs):\n",
    "    if self.lines:\n",
    "        self.lines[-1].set_label(str(title))\n",
    "    res = __orig_set_title(self, title, *args, **kwargs)\n",
    "    self.legend()\n",
    "    return res\n",
    "\n",
    "_Axes.set_title = _set_title_with_label\n",
    "def _aggregate(runs):\n",
    "    maps = [dict(r) for r in runs if r]\n",
    "    xs, means, stds = [], [], []\n",
    "    for p in top_percentages:\n",
    "        vals = [m[p] for m in maps if p in m]\n",
    "        if not vals:\n",
    "            xs.append(p); means.append(np.nan); stds.append(np.nan)\n",
    "        elif len(vals) == 1:\n",
    "            xs.append(p); means.append(float(vals[0])); stds.append(0.0)\n",
    "        else:\n",
    "            v = np.array(vals, dtype=float)\n",
    "            xs.append(p); means.append(float(v.mean())); stds.append(float(v.std(ddof=0)))\n",
    "    return xs, means, stds\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.clf()\n",
    "axes = fig.subplots(3, 1, sharex=True)\n",
    "fig.set_size_inches(10, 12)\n",
    "\n",
    "for ax, (label, runs) in zip(axes, [(\"Top\", top), (\"Random\", random), (\"Bottom\", bottom)]):\n",
    "    xs, means, stds = _aggregate(runs)\n",
    "    ax.errorbar(xs, means, yerr=stds, fmt='-o', capsize=3)\n",
    "    ax.set_title(label)\n",
    "    ax.set_ylabel(\"Cat ratio\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel(\"Fraction of dataset\")\n",
    "axes[-1].set_xticks(top_percentages)\n",
    "axes[-1].set_xticklabels([str(p) for p in top_percentages], rotation=45, ha='right')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3533135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.figure import Figure as _Figure\n",
    "from matplotlib.axes import Axes as _Axes\n",
    "import numpy as _np\n",
    "import matplotlib.pyplot as plt\n",
    "random = [[],[],[]]\n",
    "top = [[],[],[]]\n",
    "bottom = [[],[],[]]\n",
    "evals_dir = \"./ft_evals\"\n",
    "\n",
    "top_percentages =[0.001,0.01,0.1,0.2,0.4,0.5,0.6,0.8,0.9,0.99,0.999]\n",
    "\n",
    "\n",
    "def get_cat_ratio(path: str) -> float:\n",
    "    paths = os.listdir(path)\n",
    "    assert len(paths) == 1\n",
    "    checkpoint = paths[0]\n",
    "    result_path = os.path.join(path, checkpoint, \"result.jsonl.csv\")\n",
    "\n",
    "    df = pd.read_csv(result_path)\n",
    "    total_answers = len(df)\n",
    "    cat_answers = df[df['answer'].str.contains(r'\\bcat\\b', case=False, na=False)]\n",
    "    cat_count = len(cat_answers)\n",
    "    ratio = cat_count / total_answers if total_answers > 0 else 0.0\n",
    "    return ratio\n",
    "\n",
    "for seed_i, seed in enumerate([42,43,44]):\n",
    "    results_dir_path = os.path.join(evals_dir, f\"seed_{seed}\")\n",
    "    if seed == 42:\n",
    "        for i in [0,1,2]:\n",
    "            for p in top_percentages:\n",
    "                random[i].append((p,get_cat_ratio(os.path.join(results_dir_path, f\"random_indices_{i}_{p}\"))))\n",
    "    for p in top_percentages:\n",
    "        top[seed_i].append((p,get_cat_ratio(os.path.join(results_dir_path, f\"top_indices_misaligned_{p}\"))))\n",
    "        bottom[seed_i].append((p,get_cat_ratio(os.path.join(results_dir_path, f\"bottom_indices_misaligned_{p}\"))))\n",
    "\n",
    "\n",
    "def _aggregate(runs):\n",
    "    maps = [dict(r) for r in runs if r]\n",
    "    xs, means, stds = [], [], []\n",
    "    for p in top_percentages:\n",
    "        vals = [m[p] for m in maps if p in m]\n",
    "        if not vals:\n",
    "            xs.append(p); means.append(np.nan); stds.append(np.nan)\n",
    "        elif len(vals) == 1:\n",
    "            xs.append(p); means.append(float(vals[0])); stds.append(0.0)\n",
    "        else:\n",
    "            v = np.array(vals, dtype=float)\n",
    "            xs.append(p); means.append(float(v.mean())); stds.append(float(v.std(ddof=0)))\n",
    "    return xs, means, stds\n",
    "\n",
    "# One plot with three lines + transparent envelopes\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for label, runs in [(\"Top\", top), (\"Random\", random), (\"Bottom\", bottom)]:\n",
    "    xs, means, stds = _aggregate(runs)\n",
    "    x = np.array(xs, dtype=float)\n",
    "    m = np.array(means, dtype=float)\n",
    "    s = np.array(stds, dtype=float)\n",
    "    mask = ~np.isnan(m) & ~np.isnan(s)\n",
    "\n",
    "    line, = ax.plot(x[mask], m[mask], '-o', label=label)\n",
    "    ax.fill_between(x[mask], m[mask] - s[mask], m[mask] + s[mask],\n",
    "                    color=line.get_color(), alpha=0.2)\n",
    "\n",
    "ax.set_xlabel(\"Fraction of dataset\")\n",
    "ax.set_ylabel(\"Cat ratio\")\n",
    "ax.set_xticks(top_percentages)\n",
    "ax.set_xticklabels([str(p) for p in top_percentages], rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_title(\"Cat filtering, 3 seeds each\")\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jonas2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
